{
  "video_id": "aircAruvnKk",
  "created_at": "2025-07-26T18:58:55.543304",
  "segment_count": 255,
  "metadata": {
    "video_id": "aircAruvnKk",
    "language": "en",
    "segment_count": 255
  },
  "transcript": [
    {
      "text": "This is a 3. It's sloppily written and rendered at an extremely low resolution of 28x28 pixels,",
      "start": 4.22,
      "duration": 6.492999999999999,
      "language": "en"
    },
    {
      "text": "but your brain has no trouble recognizing it as a 3.",
      "start": 10.713,
      "duration": 3.007,
      "language": "en"
    },
    {
      "text": "And I want you to take a moment to appreciate how",
      "start": 14.34,
      "duration": 2.219,
      "language": "en"
    },
    {
      "text": "crazy it is that brains can do this so effortlessly.",
      "start": 16.559,
      "duration": 2.401,
      "language": "en"
    },
    {
      "text": "I mean, this, this and this are also recognizable as 3s,",
      "start": 19.7,
      "duration": 3.262,
      "language": "en"
    },
    {
      "text": "even though the specific values of each pixel is very different from one",
      "start": 22.962,
      "duration": 4.251,
      "language": "en"
    },
    {
      "text": "image to the next. The particular light-sensitive cells in your eye that are firing when you",
      "start": 27.213,
      "duration": 5.734999999999999,
      "language": "en"
    },
    {
      "text": "see this 3 are very different from the ones firing when you see this 3.",
      "start": 32.948,
      "duration": 3.992,
      "language": "en"
    },
    {
      "text": "But something in that crazy-smart visual cortex of yours resolves these as representing",
      "start": 37.52,
      "duration": 5.22,
      "language": "en"
    },
    {
      "text": "the same idea, while at the same time recognizing other images as their own distinct",
      "start": 42.74,
      "duration": 5.1,
      "language": "en"
    },
    {
      "text": "ideas. But if I told you, hey, sit down and write for me a program that takes in a grid of",
      "start": 47.84,
      "duration": 6.793999999999997,
      "language": "en"
    },
    {
      "text": "28x28 pixels like this and outputs a single number between 0 and 10,",
      "start": 54.634,
      "duration": 4.501,
      "language": "en"
    },
    {
      "text": "telling you what it thinks the digit is, well the task goes from comically trivial to",
      "start": 59.135,
      "duration": 5.61,
      "language": "en"
    },
    {
      "text": "dauntingly difficult. Unless you've been living under a rock, I think I hardly need to motivate the relevance",
      "start": 64.745,
      "duration": 6.111999999999995,
      "language": "en"
    },
    {
      "text": "and importance of machine learning and neural networks to the present and to the future.",
      "start": 70.857,
      "duration": 3.783,
      "language": "en"
    },
    {
      "text": "But what I want to do here is show you what a neural network actually is,",
      "start": 75.12,
      "duration": 3.83,
      "language": "en"
    },
    {
      "text": "assuming no background, and to help visualize what it's doing,",
      "start": 78.95,
      "duration": 3.306,
      "language": "en"
    },
    {
      "text": "not as a buzzword but as a piece of math.",
      "start": 82.256,
      "duration": 2.204,
      "language": "en"
    },
    {
      "text": "My hope is that you come away feeling like the structure itself is motivated,",
      "start": 85.02,
      "duration": 3.757,
      "language": "en"
    },
    {
      "text": "and to feel like you know what it means when you read,",
      "start": 88.777,
      "duration": 2.684,
      "language": "en"
    },
    {
      "text": "or you hear about a neural network quote-unquote learning.",
      "start": 91.461,
      "duration": 2.879,
      "language": "en"
    },
    {
      "text": "This video is just going to be devoted to the structure component of that,",
      "start": 95.36,
      "duration": 2.901,
      "language": "en"
    },
    {
      "text": "and the following one is going to tackle learning. What we're going to do is put together a neural",
      "start": 98.261,
      "duration": 5.016999999999996,
      "language": "en"
    },
    {
      "text": "network that can learn to recognize handwritten digits.",
      "start": 103.278,
      "duration": 2.762,
      "language": "en"
    },
    {
      "text": "This is a somewhat classic example for introducing the topic,",
      "start": 109.36,
      "duration": 2.7,
      "language": "en"
    },
    {
      "text": "and I'm happy to stick with the status quo here,",
      "start": 112.06,
      "duration": 2.168,
      "language": "en"
    },
    {
      "text": "because at the end of the two videos I want to point you to a couple good",
      "start": 114.228,
      "duration": 3.275,
      "language": "en"
    },
    {
      "text": "resources where you can learn more, and where you can download the code that",
      "start": 117.503,
      "duration": 3.408,
      "language": "en"
    },
    {
      "text": "does this and play with it on your own computer.",
      "start": 120.911,
      "duration": 2.169,
      "language": "en"
    },
    {
      "text": "There are many many variants of neural networks,",
      "start": 125.04,
      "duration": 2.621,
      "language": "en"
    },
    {
      "text": "and in recent years there's been sort of a boom in research towards these variants,",
      "start": 127.661,
      "duration": 4.585,
      "language": "en"
    },
    {
      "text": "but in these two introductory videos you and I are just going to look at the simplest",
      "start": 132.246,
      "duration": 4.696,
      "language": "en"
    },
    {
      "text": "plain vanilla form with no added frills.",
      "start": 136.942,
      "duration": 2.238,
      "language": "en"
    },
    {
      "text": "This is kind of a necessary prerequisite for understanding any of the more powerful",
      "start": 139.86,
      "duration": 4.03,
      "language": "en"
    },
    {
      "text": "modern variants, and trust me it still has plenty of complexity for us to wrap our minds",
      "start": 143.89,
      "duration": 4.322,
      "language": "en"
    },
    {
      "text": "around. But even in this simplest form it can learn to recognize handwritten digits,",
      "start": 148.212,
      "duration": 4.983000000000004,
      "language": "en"
    },
    {
      "text": "which is a pretty cool thing for a computer to be able to do.",
      "start": 153.195,
      "duration": 3.325,
      "language": "en"
    },
    {
      "text": "And at the same time you'll see how it does fall",
      "start": 157.48,
      "duration": 2.327,
      "language": "en"
    },
    {
      "text": "short of a couple hopes that we might have for it.",
      "start": 159.807,
      "duration": 2.473,
      "language": "en"
    },
    {
      "text": "As the name suggests neural networks are inspired by the brain, but let's break that down.",
      "start": 163.38,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "What are the neurons, and in what sense are they linked together?",
      "start": 168.52,
      "duration": 3.14,
      "language": "en"
    },
    {
      "text": "Right now when I say neuron all I want you to think about is a thing that holds a number,",
      "start": 172.5,
      "duration": 5.521,
      "language": "en"
    },
    {
      "text": "specifically a number between 0 and 1.",
      "start": 178.021,
      "duration": 2.419,
      "language": "en"
    },
    {
      "text": "It's really not more than that. For example the network starts with a bunch of neurons corresponding to",
      "start": 180.68,
      "duration": 8.141999999999996,
      "language": "en"
    },
    {
      "text": "each of the 28x28 pixels of the input image, which is 784 neurons in total.",
      "start": 188.822,
      "duration": 5.398,
      "language": "en"
    },
    {
      "text": "Each one of these holds a number that represents the grayscale value of the",
      "start": 194.7,
      "duration": 4.714,
      "language": "en"
    },
    {
      "text": "corresponding pixel, ranging from 0 for black pixels up to 1 for white pixels.",
      "start": 199.414,
      "duration": 4.966,
      "language": "en"
    },
    {
      "text": "This number inside the neuron is called its activation,",
      "start": 205.3,
      "duration": 2.953,
      "language": "en"
    },
    {
      "text": "and the image you might have in mind here is that each neuron is lit up when its",
      "start": 208.253,
      "duration": 4.35,
      "language": "en"
    },
    {
      "text": "activation is a high number. So all of these 784 neurons make up the first layer of our network.",
      "start": 212.603,
      "duration": 9.256999999999977,
      "language": "en"
    },
    {
      "text": "Now jumping over to the last layer, this has 10 neurons,",
      "start": 226.5,
      "duration": 2.926,
      "language": "en"
    },
    {
      "text": "each representing one of the digits. The activation in these neurons, again some number that's between 0 and 1,",
      "start": 229.426,
      "duration": 7.189999999999998,
      "language": "en"
    },
    {
      "text": "represents how much the system thinks that a given image corresponds with a given digit.",
      "start": 236.616,
      "duration": 5.504,
      "language": "en"
    },
    {
      "text": "There's also a couple layers in between called the hidden layers,",
      "start": 243.04,
      "duration": 3.381,
      "language": "en"
    },
    {
      "text": "which for the time being should just be a giant question mark for",
      "start": 246.421,
      "duration": 3.434,
      "language": "en"
    },
    {
      "text": "how on earth this process of recognizing digits is going to be handled.",
      "start": 249.855,
      "duration": 3.745,
      "language": "en"
    },
    {
      "text": "In this network I chose two hidden layers, each one with 16 neurons,",
      "start": 254.26,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "and admittedly that's kind of an arbitrary choice.",
      "start": 257.86,
      "duration": 2.7,
      "language": "en"
    },
    {
      "text": "To be honest I chose two layers based on how I want to motivate the structure in",
      "start": 261.02,
      "duration": 3.635,
      "language": "en"
    },
    {
      "text": "just a moment, and 16, well that was just a nice number to fit on the screen.",
      "start": 264.655,
      "duration": 3.545,
      "language": "en"
    },
    {
      "text": "In practice there is a lot of room for experiment with a specific structure here.",
      "start": 268.78,
      "duration": 3.56,
      "language": "en"
    },
    {
      "text": "The way the network operates, activations in one",
      "start": 273.02,
      "duration": 2.647,
      "language": "en"
    },
    {
      "text": "layer determine the activations of the next layer.",
      "start": 275.667,
      "duration": 2.813,
      "language": "en"
    },
    {
      "text": "And of course the heart of the network as an information processing mechanism comes down",
      "start": 279.2,
      "duration": 4.611,
      "language": "en"
    },
    {
      "text": "to exactly how those activations from one layer bring about activations in the next",
      "start": 283.811,
      "duration": 4.402,
      "language": "en"
    },
    {
      "text": "layer. It's meant to be loosely analogous to how in biological networks of neurons,",
      "start": 288.213,
      "duration": 5.419999999999959,
      "language": "en"
    },
    {
      "text": "some groups of neurons firing cause certain others to fire.",
      "start": 293.633,
      "duration": 3.547,
      "language": "en"
    },
    {
      "text": "Now the network I'm showing here has already been trained to recognize digits,",
      "start": 298.12,
      "duration": 3.461,
      "language": "en"
    },
    {
      "text": "and let me show you what I mean by that. It means if you feed in an image, lighting up all 784 neurons of the input layer",
      "start": 301.581,
      "duration": 6.712999999999965,
      "language": "en"
    },
    {
      "text": "according to the brightness of each pixel in the image,",
      "start": 308.294,
      "duration": 3.257,
      "language": "en"
    },
    {
      "text": "that pattern of activations causes some very specific pattern in the next layer",
      "start": 311.551,
      "duration": 4.654,
      "language": "en"
    },
    {
      "text": "which causes some pattern in the one after it,",
      "start": 316.205,
      "duration": 2.734,
      "language": "en"
    },
    {
      "text": "which finally gives some pattern in the output layer.",
      "start": 318.939,
      "duration": 3.141,
      "language": "en"
    },
    {
      "text": "And the brightest neuron of that output layer is the network's choice,",
      "start": 322.56,
      "duration": 3.957,
      "language": "en"
    },
    {
      "text": "so to speak, for what digit this image represents.",
      "start": 326.517,
      "duration": 2.883,
      "language": "en"
    },
    {
      "text": "And before jumping into the math for how one layer influences the next,",
      "start": 332.56,
      "duration": 3.777,
      "language": "en"
    },
    {
      "text": "or how training works, let's just talk about why it's even reasonable",
      "start": 336.337,
      "duration": 3.725,
      "language": "en"
    },
    {
      "text": "to expect a layered structure like this to behave intelligently.",
      "start": 340.062,
      "duration": 3.458,
      "language": "en"
    },
    {
      "text": "What are we expecting here? What is the best hope for what those middle layers might be doing?",
      "start": 344.06,
      "duration": 3.5399999999999636,
      "language": "en"
    },
    {
      "text": "Well, when you or I recognize digits, we piece together various components.",
      "start": 348.92,
      "duration": 4.6,
      "language": "en"
    },
    {
      "text": "A 9 has a loop up top and a line on the right.",
      "start": 354.2,
      "duration": 2.62,
      "language": "en"
    },
    {
      "text": "An 8 also has a loop up top, but it's paired with another loop down low.",
      "start": 357.38,
      "duration": 3.8,
      "language": "en"
    },
    {
      "text": "A 4 basically breaks down into three specific lines, and things like that.",
      "start": 361.98,
      "duration": 4.84,
      "language": "en"
    },
    {
      "text": "Now in a perfect world, we might hope that each neuron in the second",
      "start": 367.6,
      "duration": 3.958,
      "language": "en"
    },
    {
      "text": "to last layer corresponds with one of these subcomponents,",
      "start": 371.558,
      "duration": 3.434,
      "language": "en"
    },
    {
      "text": "that anytime you feed in an image with, say, a loop up top,",
      "start": 374.992,
      "duration": 3.492,
      "language": "en"
    },
    {
      "text": "like a 9 or an 8, there's some specific neuron whose activation is",
      "start": 378.484,
      "duration": 3.899,
      "language": "en"
    },
    {
      "text": "going to be close to 1. And I don't mean this specific loop of pixels,",
      "start": 382.383,
      "duration": 4.523000000000025,
      "language": "en"
    },
    {
      "text": "the hope would be that any generally loopy pattern towards the top sets off this neuron.",
      "start": 386.906,
      "duration": 4.654,
      "language": "en"
    },
    {
      "text": "That way, going from the third layer to the last one just requires",
      "start": 392.44,
      "duration": 3.609,
      "language": "en"
    },
    {
      "text": "learning which combination of subcomponents corresponds to which digits.",
      "start": 396.049,
      "duration": 3.991,
      "language": "en"
    },
    {
      "text": "Of course, that just kicks the problem down the road,",
      "start": 401.0,
      "duration": 2.199,
      "language": "en"
    },
    {
      "text": "because how would you recognize these subcomponents,",
      "start": 403.199,
      "duration": 2.2,
      "language": "en"
    },
    {
      "text": "or even learn what the right subcomponents should be?",
      "start": 405.399,
      "duration": 2.241,
      "language": "en"
    },
    {
      "text": "And I still haven't even talked about how one layer influences the next,",
      "start": 408.06,
      "duration": 3.158,
      "language": "en"
    },
    {
      "text": "but run with me on this one for a moment. Recognizing a loop can also break down into subproblems.",
      "start": 411.218,
      "duration": 5.461999999999989,
      "language": "en"
    },
    {
      "text": "One reasonable way to do this would be to first",
      "start": 417.28,
      "duration": 2.611,
      "language": "en"
    },
    {
      "text": "recognize the various little edges that make it up.",
      "start": 419.891,
      "duration": 2.889,
      "language": "en"
    },
    {
      "text": "Similarly, a long line, like the kind you might see in the digits 1 or 4 or 7,",
      "start": 423.78,
      "duration": 4.619,
      "language": "en"
    },
    {
      "text": "is really just a long edge, or maybe you think of it as a certain pattern of several",
      "start": 428.399,
      "duration": 5.033,
      "language": "en"
    },
    {
      "text": "smaller edges. So maybe our hope is that each neuron in the second layer of",
      "start": 433.432,
      "duration": 5.375999999999976,
      "language": "en"
    },
    {
      "text": "the network corresponds with the various relevant little edges.",
      "start": 438.808,
      "duration": 3.912,
      "language": "en"
    },
    {
      "text": "Maybe when an image like this one comes in, it lights up all of the",
      "start": 443.54,
      "duration": 3.971,
      "language": "en"
    },
    {
      "text": "neurons associated with around 8 to 10 specific little edges,",
      "start": 447.511,
      "duration": 3.674,
      "language": "en"
    },
    {
      "text": "which in turn lights up the neurons associated with the upper loop",
      "start": 451.185,
      "duration": 3.971,
      "language": "en"
    },
    {
      "text": "and a long vertical line, and those light up the neuron associated with a 9.",
      "start": 455.156,
      "duration": 4.564,
      "language": "en"
    },
    {
      "text": "Whether or not this is what our final network actually does is another question,",
      "start": 460.68,
      "duration": 4.003,
      "language": "en"
    },
    {
      "text": "one that I'll come back to once we see how to train the network,",
      "start": 464.683,
      "duration": 3.253,
      "language": "en"
    },
    {
      "text": "but this is a hope that we might have, a sort of goal with the layered structure",
      "start": 467.936,
      "duration": 4.054,
      "language": "en"
    },
    {
      "text": "like this. Moreover, you can imagine how being able to detect edges and patterns",
      "start": 471.99,
      "duration": 4.7660000000000196,
      "language": "en"
    },
    {
      "text": "like this would be really useful for other image recognition tasks.",
      "start": 476.756,
      "duration": 3.544,
      "language": "en"
    },
    {
      "text": "And even beyond image recognition, there are all sorts of intelligent",
      "start": 480.88,
      "duration": 3.132,
      "language": "en"
    },
    {
      "text": "things you might want to do that break down into layers of abstraction.",
      "start": 484.012,
      "duration": 3.268,
      "language": "en"
    },
    {
      "text": "Parsing speech, for example, involves taking raw audio and picking out distinct sounds,",
      "start": 488.04,
      "duration": 4.689,
      "language": "en"
    },
    {
      "text": "which combine to make certain syllables, which combine to form words,",
      "start": 492.729,
      "duration": 3.774,
      "language": "en"
    },
    {
      "text": "which combine to make up phrases and more abstract thoughts, etc.",
      "start": 496.503,
      "duration": 3.557,
      "language": "en"
    },
    {
      "text": "But getting back to how any of this actually works,",
      "start": 501.1,
      "duration": 2.585,
      "language": "en"
    },
    {
      "text": "picture yourself right now designing how exactly the activations in one layer might",
      "start": 503.685,
      "duration": 4.258,
      "language": "en"
    },
    {
      "text": "determine the activations in the next. The goal is to have some mechanism that could conceivably combine pixels into edges,",
      "start": 507.943,
      "duration": 8.045000000000073,
      "language": "en"
    },
    {
      "text": "or edges into patterns, or patterns into digits.",
      "start": 515.988,
      "duration": 2.992,
      "language": "en"
    },
    {
      "text": "And to zoom in on one very specific example, let's say the hope",
      "start": 519.44,
      "duration": 3.828,
      "language": "en"
    },
    {
      "text": "is for one particular neuron in the second layer to pick up",
      "start": 523.268,
      "duration": 3.646,
      "language": "en"
    },
    {
      "text": "on whether or not the image has an edge in this region here.",
      "start": 526.914,
      "duration": 3.706,
      "language": "en"
    },
    {
      "text": "The question at hand is what parameters should the network have?",
      "start": 531.44,
      "duration": 3.66,
      "language": "en"
    },
    {
      "text": "What dials and knobs should you be able to tweak so that it's expressive",
      "start": 535.64,
      "duration": 4.01,
      "language": "en"
    },
    {
      "text": "enough to potentially capture this pattern, or any other pixel pattern,",
      "start": 539.65,
      "duration": 4.009,
      "language": "en"
    },
    {
      "text": "or the pattern that several edges can make a loop, and other such things?",
      "start": 543.659,
      "duration": 4.121,
      "language": "en"
    },
    {
      "text": "Well, what we'll do is assign a weight to each one of the",
      "start": 548.72,
      "duration": 3.094,
      "language": "en"
    },
    {
      "text": "connections between our neuron and the neurons from the first layer.",
      "start": 551.814,
      "duration": 3.746,
      "language": "en"
    },
    {
      "text": "These weights are just numbers. Then take all of those activations from the first layer",
      "start": 556.32,
      "duration": 5.577999999999861,
      "language": "en"
    },
    {
      "text": "and compute their weighted sum according to these weights.",
      "start": 561.898,
      "duration": 3.602,
      "language": "en"
    },
    {
      "text": "I find it helpful to think of these weights as being organized into a",
      "start": 567.7,
      "duration": 3.397,
      "language": "en"
    },
    {
      "text": "little grid of their own, and I'm going to use green pixels to indicate",
      "start": 571.097,
      "duration": 3.545,
      "language": "en"
    },
    {
      "text": "positive weights, and red pixels to indicate negative weights,",
      "start": 574.642,
      "duration": 3.101,
      "language": "en"
    },
    {
      "text": "where the brightness of that pixel is some loose depiction of the weight's value.",
      "start": 577.743,
      "duration": 4.037,
      "language": "en"
    },
    {
      "text": "Now if we made the weights associated with almost all of the pixels zero",
      "start": 582.78,
      "duration": 3.747,
      "language": "en"
    },
    {
      "text": "except for some positive weights in this region that we care about,",
      "start": 586.527,
      "duration": 3.539,
      "language": "en"
    },
    {
      "text": "then taking the weighted sum of all the pixel values really just amounts",
      "start": 590.066,
      "duration": 3.799,
      "language": "en"
    },
    {
      "text": "to adding up the values of the pixel just in the region that we care about.",
      "start": 593.865,
      "duration": 3.955,
      "language": "en"
    },
    {
      "text": "And if you really wanted to pick up on whether there's an edge here,",
      "start": 599.14,
      "duration": 3.252,
      "language": "en"
    },
    {
      "text": "what you might do is have some negative weights associated with the surrounding pixels.",
      "start": 602.392,
      "duration": 4.208,
      "language": "en"
    },
    {
      "text": "Then the sum is largest when those middle pixels",
      "start": 607.48,
      "duration": 2.557,
      "language": "en"
    },
    {
      "text": "are bright but the surrounding pixels are darker.",
      "start": 610.037,
      "duration": 2.663,
      "language": "en"
    },
    {
      "text": "When you compute a weighted sum like this, you might come out with any number,",
      "start": 614.26,
      "duration": 4.387,
      "language": "en"
    },
    {
      "text": "but for this network what we want is for activations to be some value between 0 and 1.",
      "start": 618.647,
      "duration": 4.893,
      "language": "en"
    },
    {
      "text": "So a common thing to do is to pump this weighted sum into some function",
      "start": 624.12,
      "duration": 4.126,
      "language": "en"
    },
    {
      "text": "that squishes the real number line into the range between 0 and 1.",
      "start": 628.246,
      "duration": 3.894,
      "language": "en"
    },
    {
      "text": "And a common function that does this is called the sigmoid function,",
      "start": 632.46,
      "duration": 3.373,
      "language": "en"
    },
    {
      "text": "also known as a logistic curve. Basically very negative inputs end up close to 0, positive inputs end up close to 1,",
      "start": 635.833,
      "duration": 7.518000000000029,
      "language": "en"
    },
    {
      "text": "and it just steadily increases around the input 0.",
      "start": 643.351,
      "duration": 3.249,
      "language": "en"
    },
    {
      "text": "So the activation of the neuron here is basically a",
      "start": 649.12,
      "duration": 3.517,
      "language": "en"
    },
    {
      "text": "measure of how positive the relevant weighted sum is.",
      "start": 652.637,
      "duration": 3.723,
      "language": "en"
    },
    {
      "text": "But maybe it's not that you want the neuron to",
      "start": 657.54,
      "duration": 2.101,
      "language": "en"
    },
    {
      "text": "light up when the weighted sum is bigger than 0.",
      "start": 659.641,
      "duration": 2.239,
      "language": "en"
    },
    {
      "text": "Maybe you only want it to be active when the sum is bigger than say 10.",
      "start": 662.28,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "That is, you want some bias for it to be inactive.",
      "start": 666.84,
      "duration": 3.42,
      "language": "en"
    },
    {
      "text": "What we'll do then is just add in some other number like negative 10 to this",
      "start": 671.38,
      "duration": 4.086,
      "language": "en"
    },
    {
      "text": "weighted sum before plugging it through the sigmoid squishification function.",
      "start": 675.466,
      "duration": 4.194,
      "language": "en"
    },
    {
      "text": "That additional number is called the bias. So the weights tell you what pixel pattern this neuron in the second",
      "start": 680.58,
      "duration": 6.730000000000018,
      "language": "en"
    },
    {
      "text": "layer is picking up on, and the bias tells you how high the weighted",
      "start": 687.31,
      "duration": 3.907,
      "language": "en"
    },
    {
      "text": "sum needs to be before the neuron starts getting meaningfully active.",
      "start": 691.217,
      "duration": 3.963,
      "language": "en"
    },
    {
      "text": "And that is just one neuron. Every other neuron in this layer is going to be connected to",
      "start": 696.12,
      "duration": 6.356999999999971,
      "language": "en"
    },
    {
      "text": "all 784 pixel neurons from the first layer, and each one of",
      "start": 702.477,
      "duration": 4.196,
      "language": "en"
    },
    {
      "text": "those 784 connections has its own weight associated with it.",
      "start": 706.673,
      "duration": 4.267,
      "language": "en"
    },
    {
      "text": "Also, each one has some bias, some other number that you add",
      "start": 711.6,
      "duration": 2.975,
      "language": "en"
    },
    {
      "text": "on to the weighted sum before squishing it with the sigmoid.",
      "start": 714.575,
      "duration": 3.025,
      "language": "en"
    },
    {
      "text": "And that's a lot to think about! With this hidden layer of 16 neurons, that's a total of 784 times 16 weights,",
      "start": 718.11,
      "duration": 8.08800000000008,
      "language": "en"
    },
    {
      "text": "along with 16 biases. And all of that is just the connections from the first layer to the second.",
      "start": 726.198,
      "duration": 5.7420000000000755,
      "language": "en"
    },
    {
      "text": "The connections between the other layers also have",
      "start": 732.52,
      "duration": 2.363,
      "language": "en"
    },
    {
      "text": "a bunch of weights and biases associated with them.",
      "start": 734.883,
      "duration": 2.457,
      "language": "en"
    },
    {
      "text": "All said and done, this network has almost exactly 13,000 total weights and biases.",
      "start": 738.34,
      "duration": 5.46,
      "language": "en"
    },
    {
      "text": "13,000 knobs and dials that can be tweaked and turned",
      "start": 743.8,
      "duration": 3.265,
      "language": "en"
    },
    {
      "text": "to make this network behave in different ways.",
      "start": 747.065,
      "duration": 2.895,
      "language": "en"
    },
    {
      "text": "So when we talk about learning, what that's referring to is",
      "start": 751.04,
      "duration": 3.222,
      "language": "en"
    },
    {
      "text": "getting the computer to find a valid setting for all of these",
      "start": 754.262,
      "duration": 3.385,
      "language": "en"
    },
    {
      "text": "many many numbers so that it'll actually solve the problem at hand.",
      "start": 757.647,
      "duration": 3.713,
      "language": "en"
    },
    {
      "text": "One thought experiment that is at once fun and kind of horrifying is to imagine sitting",
      "start": 762.62,
      "duration": 4.566,
      "language": "en"
    },
    {
      "text": "down and setting all of these weights and biases by hand,",
      "start": 767.186,
      "duration": 3.044,
      "language": "en"
    },
    {
      "text": "purposefully tweaking the numbers so that the second layer picks up on edges,",
      "start": 770.23,
      "duration": 4.093,
      "language": "en"
    },
    {
      "text": "the third layer picks up on patterns, etc.",
      "start": 774.323,
      "duration": 2.257,
      "language": "en"
    },
    {
      "text": "I personally find this satisfying rather than just treating the network as a total black",
      "start": 776.98,
      "duration": 4.362,
      "language": "en"
    },
    {
      "text": "box, because when the network doesn't perform the way you anticipate,",
      "start": 781.342,
      "duration": 3.47,
      "language": "en"
    },
    {
      "text": "if you've built up a little bit of a relationship with what those weights and biases",
      "start": 784.812,
      "duration": 4.213,
      "language": "en"
    },
    {
      "text": "actually mean, you have a starting place for experimenting with how to change the",
      "start": 789.025,
      "duration": 4.065,
      "language": "en"
    },
    {
      "text": "structure to improve. Or when the network does work but not for the reasons you might expect,",
      "start": 793.09,
      "duration": 5.342999999999961,
      "language": "en"
    },
    {
      "text": "digging into what the weights and biases are doing is a good way to challenge",
      "start": 798.433,
      "duration": 3.816,
      "language": "en"
    },
    {
      "text": "your assumptions and really expose the full space of possible solutions.",
      "start": 802.249,
      "duration": 3.571,
      "language": "en"
    },
    {
      "text": "By the way, the actual function here is a little cumbersome to write down,",
      "start": 806.84,
      "duration": 3.123,
      "language": "en"
    },
    {
      "text": "don't you think? So let me show you a more notationally compact way that these connections are represented.",
      "start": 809.963,
      "duration": 7.177000000000021,
      "language": "en"
    },
    {
      "text": "This is how you'd see it if you choose to read up more about neural networks.",
      "start": 817.66,
      "duration": 2.86,
      "language": "en"
    },
    {
      "text": "Organize all of the activations from one layer into a column as a vector.",
      "start": 820.52,
      "duration": 7.503,
      "language": "en"
    },
    {
      "text": "Then organize all of the weights as a matrix, where each row of that matrix corresponds to the connections between one layer and a particular neuron in the next layer.",
      "start": 828.357,
      "duration": 9.643000000000029,
      "language": "en"
    },
    {
      "text": "What that means is that taking the weighted sum of the activations in",
      "start": 838.54,
      "duration": 3.674,
      "language": "en"
    },
    {
      "text": "the first layer according to these weights corresponds to one of the",
      "start": 842.214,
      "duration": 3.673,
      "language": "en"
    },
    {
      "text": "terms in the matrix vector product of everything we have on the left here.",
      "start": 845.887,
      "duration": 3.993,
      "language": "en"
    },
    {
      "text": "By the way, so much of machine learning just comes down to having a good",
      "start": 854.0,
      "duration": 3.714,
      "language": "en"
    },
    {
      "text": "grasp of linear algebra, so for any of you who want a nice visual",
      "start": 857.714,
      "duration": 3.405,
      "language": "en"
    },
    {
      "text": "understanding for matrices and what matrix vector multiplication means,",
      "start": 861.119,
      "duration": 3.715,
      "language": "en"
    },
    {
      "text": "take a look at the series I did on linear algebra, especially chapter 3.",
      "start": 864.834,
      "duration": 3.766,
      "language": "en"
    },
    {
      "text": "Back to our expression, instead of talking about adding the bias to each one of",
      "start": 869.24,
      "duration": 4.353,
      "language": "en"
    },
    {
      "text": "these values independently, we represent it by organizing all those biases into",
      "start": 873.593,
      "duration": 4.409,
      "language": "en"
    },
    {
      "text": "a vector, and adding the entire vector to the previous matrix vector product.",
      "start": 878.002,
      "duration": 4.298,
      "language": "en"
    },
    {
      "text": "Then as a final step, I'll wrap a sigmoid around the outside here,",
      "start": 883.28,
      "duration": 3.534,
      "language": "en"
    },
    {
      "text": "and what that's supposed to represent is that you're going to apply the",
      "start": 886.814,
      "duration": 3.856,
      "language": "en"
    },
    {
      "text": "sigmoid function to each specific component of the resulting vector inside.",
      "start": 890.67,
      "duration": 4.07,
      "language": "en"
    },
    {
      "text": "So once you write down this weight matrix and these vectors as their own symbols,",
      "start": 895.94,
      "duration": 4.538,
      "language": "en"
    },
    {
      "text": "you can communicate the full transition of activations from one layer to the next in an",
      "start": 900.478,
      "duration": 4.93,
      "language": "en"
    },
    {
      "text": "extremely tight and neat little expression, and this makes the relevant code both a lot",
      "start": 905.408,
      "duration": 4.93,
      "language": "en"
    },
    {
      "text": "simpler and a lot faster, since many libraries optimize the heck out of matrix",
      "start": 910.338,
      "duration": 4.426,
      "language": "en"
    },
    {
      "text": "multiplication. Remember how earlier I said these neurons are simply things that hold numbers?",
      "start": 914.764,
      "duration": 6.696000000000026,
      "language": "en"
    },
    {
      "text": "Well of course the specific numbers that they hold depends on the image you feed in,",
      "start": 922.22,
      "duration": 5.11,
      "language": "en"
    },
    {
      "text": "so it's actually more accurate to think of each neuron as a function,",
      "start": 927.33,
      "duration": 4.258,
      "language": "en"
    },
    {
      "text": "one that takes in the outputs of all the neurons in the previous layer and spits out a",
      "start": 931.588,
      "duration": 5.292,
      "language": "en"
    },
    {
      "text": "number between 0 and 1. Really the entire network is just a function, one that takes in",
      "start": 936.88,
      "duration": 6.25,
      "language": "en"
    },
    {
      "text": "784 numbers as an input and spits out 10 numbers as an output.",
      "start": 943.13,
      "duration": 3.93,
      "language": "en"
    },
    {
      "text": "It's an absurdly complicated function, one that involves 13,000 parameters",
      "start": 947.56,
      "duration": 3.902,
      "language": "en"
    },
    {
      "text": "in the forms of these weights and biases that pick up on certain patterns,",
      "start": 951.462,
      "duration": 3.954,
      "language": "en"
    },
    {
      "text": "and which involves iterating many matrix vector products and the sigmoid",
      "start": 955.416,
      "duration": 3.849,
      "language": "en"
    },
    {
      "text": "squishification function, but it's just a function nonetheless.",
      "start": 959.265,
      "duration": 3.375,
      "language": "en"
    },
    {
      "text": "And in a way it's kind of reassuring that it looks complicated.",
      "start": 963.4,
      "duration": 3.26,
      "language": "en"
    },
    {
      "text": "I mean if it were any simpler, what hope would we have",
      "start": 967.34,
      "duration": 2.361,
      "language": "en"
    },
    {
      "text": "that it could take on the challenge of recognizing digits?",
      "start": 969.701,
      "duration": 2.579,
      "language": "en"
    },
    {
      "text": "And how does it take on that challenge? How does this network learn the appropriate weights and biases just by looking at data?",
      "start": 973.34,
      "duration": 6.019999999999982,
      "language": "en"
    },
    {
      "text": "Well that's what I'll show in the next video, and I'll also dig a little",
      "start": 980.14,
      "duration": 3.054,
      "language": "en"
    },
    {
      "text": "more into what this particular network we're seeing is really doing.",
      "start": 983.194,
      "duration": 2.926,
      "language": "en"
    },
    {
      "text": "Now is the point I suppose I should say subscribe to stay notified",
      "start": 987.58,
      "duration": 3.168,
      "language": "en"
    },
    {
      "text": "about when that video or any new videos come out,",
      "start": 990.748,
      "duration": 2.4,
      "language": "en"
    },
    {
      "text": "but realistically most of you don't actually receive notifications from YouTube, do you?",
      "start": 993.148,
      "duration": 4.272,
      "language": "en"
    },
    {
      "text": "Maybe more honestly I should say subscribe so that the neural networks",
      "start": 998.02,
      "duration": 3.256,
      "language": "en"
    },
    {
      "text": "that underlie YouTube's recommendation algorithm are primed to believe",
      "start": 1001.276,
      "duration": 3.302,
      "language": "en"
    },
    {
      "text": "that you want to see content from this channel get recommended to you.",
      "start": 1004.578,
      "duration": 3.302,
      "language": "en"
    },
    {
      "text": "Anyway, stay posted for more. Thank you very much to everyone supporting these videos on Patreon.",
      "start": 1008.56,
      "duration": 4.940000000000055,
      "language": "en"
    },
    {
      "text": "I've been a little slow to progress in the probability series this summer,",
      "start": 1014.0,
      "duration": 3.439,
      "language": "en"
    },
    {
      "text": "but I'm jumping back into it after this project,",
      "start": 1017.439,
      "duration": 2.277,
      "language": "en"
    },
    {
      "text": "so patrons you can look out for updates there.",
      "start": 1019.716,
      "duration": 2.184,
      "language": "en"
    },
    {
      "text": "To close things off here I have with me Lisha Li who did her PhD work on the",
      "start": 1023.6,
      "duration": 3.49,
      "language": "en"
    },
    {
      "text": "theoretical side of deep learning and who currently works at a venture capital",
      "start": 1027.09,
      "duration": 3.627,
      "language": "en"
    },
    {
      "text": "firm called Amplify Partners who kindly provided some of the funding for this video.",
      "start": 1030.717,
      "duration": 3.903,
      "language": "en"
    },
    {
      "text": "So Lisha one thing I think we should quickly bring up is this sigmoid function.",
      "start": 1035.46,
      "duration": 3.66,
      "language": "en"
    },
    {
      "text": "As I understand it early networks use this to squish the relevant weighted",
      "start": 1039.7,
      "duration": 3.458,
      "language": "en"
    },
    {
      "text": "sum into that interval between zero and one, you know kind of motivated",
      "start": 1043.158,
      "duration": 3.364,
      "language": "en"
    },
    {
      "text": "by this biological analogy of neurons either being inactive or active.",
      "start": 1046.522,
      "duration": 3.318,
      "language": "en"
    },
    {
      "text": "Exactly.\nBut relatively few modern networks actually use sigmoid anymore.",
      "start": 1050.28,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "Yeah.\nIt's kind of old school right? Yeah or rather ReLU seems to be much easier to train.",
      "start": 1054.32,
      "duration": 4.660000000000082,
      "language": "en"
    },
    {
      "text": "And ReLU, ReLU stands for rectified linear unit?",
      "start": 1059.4,
      "duration": 2.94,
      "language": "en"
    },
    {
      "text": "Yes it's this kind of function where you're just taking a max of zero",
      "start": 1062.68,
      "duration": 4.721,
      "language": "en"
    },
    {
      "text": "and a where a is given by what you were explaining in the video and",
      "start": 1067.401,
      "duration": 4.653,
      "language": "en"
    },
    {
      "text": "what this was sort of motivated from I think was a partially by a",
      "start": 1072.054,
      "duration": 4.516,
      "language": "en"
    },
    {
      "text": "biological analogy with how neurons would either be activated or not.",
      "start": 1076.57,
      "duration": 4.79,
      "language": "en"
    },
    {
      "text": "And so if it passes a certain threshold it would be the identity function but if it did",
      "start": 1081.36,
      "duration": 4.66,
      "language": "en"
    },
    {
      "text": "not then it would just not be activated so it'd be zero so it's kind of a simplification.",
      "start": 1086.02,
      "duration": 4.82,
      "language": "en"
    },
    {
      "text": "Using sigmoids didn't help training or it was very difficult to",
      "start": 1091.16,
      "duration": 4.535,
      "language": "en"
    },
    {
      "text": "train at some point and people just tried ReLU and it happened",
      "start": 1095.695,
      "duration": 4.534,
      "language": "en"
    },
    {
      "text": "to work very well for these incredibly deep neural networks.",
      "start": 1100.229,
      "duration": 4.391,
      "language": "en"
    },
    {
      "text": "All right thank you Lisha.",
      "start": 1105.1,
      "duration": 0.54,
      "language": "en"
    }
  ]
}