{
  "video_id": "msHyYioAyNE",
  "created_at": "2025-07-26T19:12:17.491189",
  "segment_count": 1634,
  "metadata": {
    "video_id": "msHyYioAyNE",
    "language": "en",
    "segment_count": 1634
  },
  "transcript": [
    {
      "text": "Okay, so last lecture I gave an overview",
      "start": 5.12,
      "duration": 5.439,
      "language": "en"
    },
    {
      "text": "of language models and what it means to",
      "start": 8.12,
      "duration": 3.96,
      "language": "en"
    },
    {
      "text": "build them from scratch and why we want",
      "start": 10.559,
      "duration": 2.96,
      "language": "en"
    },
    {
      "text": "to do that. Also talked about",
      "start": 12.08,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "tokenization which is going to be the",
      "start": 13.519,
      "duration": 4.721,
      "language": "en"
    },
    {
      "text": "first half of the first assignment. Um",
      "start": 15.44,
      "duration": 6.48,
      "language": "en"
    },
    {
      "text": "today's lecture will be going through",
      "start": 18.24,
      "duration": 6.72,
      "language": "en"
    },
    {
      "text": "actually building a model. Um we'll",
      "start": 21.92,
      "duration": 5.519,
      "language": "en"
    },
    {
      "text": "discuss the primitives in PyTorch that",
      "start": 24.96,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "are needed. Um, we're going to start",
      "start": 27.439,
      "duration": 5.041,
      "language": "en"
    },
    {
      "text": "with tensors, build models, optimizers,",
      "start": 29.92,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "and training loop. And we're going to",
      "start": 32.48,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "place close attention to efficiency, in",
      "start": 34.32,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "particular, how we're using resources,",
      "start": 37.84,
      "duration": 4.68,
      "language": "en"
    },
    {
      "text": "both um memory and",
      "start": 39.76,
      "duration": 6.08,
      "language": "en"
    },
    {
      "text": "compute. Okay. So, to motivate things a",
      "start": 42.52,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "bit, here's some here's some",
      "start": 45.84,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "questions. Okay. Um, these questions are",
      "start": 47.96,
      "duration": 4.759,
      "language": "en"
    },
    {
      "text": "going to be answerable by napkin math.",
      "start": 50.8,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "So, get your napkins out. Um, so how",
      "start": 52.719,
      "duration": 5.041,
      "language": "en"
    },
    {
      "text": "long would it take to train a 70 billion",
      "start": 55.76,
      "duration": 4.479,
      "language": "en"
    },
    {
      "text": "parameter dense transformer model on 15",
      "start": 57.76,
      "duration": 6.039,
      "language": "en"
    },
    {
      "text": "trillion tokens on 1,24",
      "start": 60.239,
      "duration": 6.56,
      "language": "en"
    },
    {
      "text": "H100s? Okay, so you know, I'm just going",
      "start": 63.799,
      "duration": 5.081,
      "language": "en"
    },
    {
      "text": "to sketch out the sort of the give you a",
      "start": 66.799,
      "duration": 3.761,
      "language": "en"
    },
    {
      "text": "flavor of the type of things that we",
      "start": 68.88,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "want to do. Okay, so here's how you go",
      "start": 70.56,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "about reasoning it. Um, you count the no",
      "start": 72.479,
      "duration": 7.081,
      "language": "en"
    },
    {
      "text": "total number of flops um needed",
      "start": 75.92,
      "duration": 7.12,
      "language": "en"
    },
    {
      "text": "to um, you know, train. So that's six",
      "start": 79.56,
      "duration": 5.72,
      "language": "en"
    },
    {
      "text": "times the number of parameters times the",
      "start": 83.04,
      "duration": 4.719,
      "language": "en"
    },
    {
      "text": "number of tokens. Okay. And where does",
      "start": 85.28,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "that come from? That will be uh what",
      "start": 87.759,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "we'll talk about in this lecture. Um you",
      "start": 89.84,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "can look at the promised number of flops",
      "start": 92.479,
      "duration": 4.841,
      "language": "en"
    },
    {
      "text": "per second that H100 gives",
      "start": 95.2,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "you. Um the MFU, which is something",
      "start": 97.32,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "we'll see later. Let's just set it to",
      "start": 100.24,
      "duration": 6.72,
      "language": "en"
    },
    {
      "text": "0.5. Um and you can look at the number",
      "start": 102.28,
      "duration": 7.24,
      "language": "en"
    },
    {
      "text": "of flops uh per day that your hardware",
      "start": 106.96,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "is going to give you at this particular",
      "start": 109.52,
      "duration": 5.72,
      "language": "en"
    },
    {
      "text": "MFU. So 1,024 of them",
      "start": 110.96,
      "duration": 9.759,
      "language": "en"
    },
    {
      "text": "um for um you know one day and then you",
      "start": 115.24,
      "duration": 7.559,
      "language": "en"
    },
    {
      "text": "just divide the total number of flops",
      "start": 120.719,
      "duration": 3.281,
      "language": "en"
    },
    {
      "text": "you need to train the model by the",
      "start": 122.799,
      "duration": 3.761,
      "language": "en"
    },
    {
      "text": "number of flops that you're supposed to",
      "start": 124.0,
      "duration": 7.239,
      "language": "en"
    },
    {
      "text": "get. Okay. And that gives you about",
      "start": 126.56,
      "duration": 8.319,
      "language": "en"
    },
    {
      "text": "144. Okay. So this is very simple uh",
      "start": 131.239,
      "duration": 6.121,
      "language": "en"
    },
    {
      "text": "calculations at the end of the day. Um",
      "start": 134.879,
      "duration": 5.121,
      "language": "en"
    },
    {
      "text": "we're going to go through a bit more",
      "start": 137.36,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "where uh these numbers uh come from and",
      "start": 140.0,
      "duration": 4.239,
      "language": "en"
    },
    {
      "text": "in particular where the six times number",
      "start": 142.48,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "of um parameters times number of tokens",
      "start": 144.239,
      "duration": 6.481,
      "language": "en"
    },
    {
      "text": "comes from. Um okay so here's a",
      "start": 147.44,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "question. What is the largest model you",
      "start": 150.72,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "can train on H eight H100s using atom W",
      "start": 152.0,
      "duration": 5.16,
      "language": "en"
    },
    {
      "text": "if you're not being uh too",
      "start": 155.12,
      "duration": 6.96,
      "language": "en"
    },
    {
      "text": "clever. Okay so um H100 has 80 gigabytes",
      "start": 157.16,
      "duration": 9.64,
      "language": "en"
    },
    {
      "text": "of HBM uh memory. Um the number of bytes",
      "start": 162.08,
      "duration": 7.28,
      "language": "en"
    },
    {
      "text": "per parameter that you need for the",
      "start": 166.8,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "parameters the gradients optimizer state",
      "start": 169.36,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "is uh 16 and we'll talk more about where",
      "start": 171.44,
      "duration": 5.439,
      "language": "en"
    },
    {
      "text": "that comes from. Um and the number of",
      "start": 174.56,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "parameters uh is basically total amount",
      "start": 176.879,
      "duration": 5.041,
      "language": "en"
    },
    {
      "text": "of memory divide by number of bytes you",
      "start": 179.92,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "need per parameter and that gives you uh",
      "start": 181.92,
      "duration": 8.08,
      "language": "en"
    },
    {
      "text": "about 40 uh billion parameters. Okay.",
      "start": 185.92,
      "duration": 6.959,
      "language": "en"
    },
    {
      "text": "And this is very rough because it it",
      "start": 190.0,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "doesn't take you into activations which",
      "start": 192.879,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "depends on batch size and sequence",
      "start": 195.44,
      "duration": 3.12,
      "language": "en"
    },
    {
      "text": "length which I'm not really going to",
      "start": 196.959,
      "duration": 3.521,
      "language": "en"
    },
    {
      "text": "talk about but will be important for",
      "start": 198.56,
      "duration": 3.0,
      "language": "en"
    },
    {
      "text": "assignment",
      "start": 200.48,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "one. Okay. So this is a rough back",
      "start": 201.56,
      "duration": 4.92,
      "language": "en"
    },
    {
      "text": "envelope calculation. And this is",
      "start": 204.0,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "something that you're probably not used",
      "start": 206.48,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "to doing. You just implement a model,",
      "start": 208.0,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "you train it and what happens happens.",
      "start": 209.76,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "But remember that efficiency is the name",
      "start": 211.84,
      "duration": 3.759,
      "language": "en"
    },
    {
      "text": "of the game. And to be efficient, you",
      "start": 213.84,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "have to know exactly how many flops",
      "start": 215.599,
      "duration": 3.521,
      "language": "en"
    },
    {
      "text": "you're actually expending because when",
      "start": 217.12,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "these numbers get large, these directly",
      "start": 219.12,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "translate into dollars and uh you want",
      "start": 221.2,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "that to be as small as possible. Okay,",
      "start": 223.84,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "so we'll talk more about the the details",
      "start": 226.72,
      "duration": 6.2,
      "language": "en"
    },
    {
      "text": "of how these numbers uh arise.",
      "start": 229.04,
      "duration": 7.04,
      "language": "en"
    },
    {
      "text": "Um you know, uh we will not actually go",
      "start": 232.92,
      "duration": 5.0,
      "language": "en"
    },
    {
      "text": "over the transformer. So Tatu is going",
      "start": 236.08,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "to talk at a um over the conceptual",
      "start": 237.92,
      "duration": 6.64,
      "language": "en"
    },
    {
      "text": "overview of that next time. Um, and",
      "start": 241.92,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "there's uh many ways you can learn about",
      "start": 244.56,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "transformer if you haven't already uh",
      "start": 246.48,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "looked at it. There's assignment one. If",
      "start": 248.4,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "you do assignment one, you'll definitely",
      "start": 250.0,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "know what a transformer is. Um, and the",
      "start": 251.28,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "handout actually does a pretty good job",
      "start": 253.599,
      "duration": 2.961,
      "language": "en"
    },
    {
      "text": "of walking through all the different",
      "start": 255.28,
      "duration": 2.959,
      "language": "en"
    },
    {
      "text": "pieces. There's a mathematical",
      "start": 256.56,
      "duration": 3.12,
      "language": "en"
    },
    {
      "text": "description. If you like pictures,",
      "start": 258.239,
      "duration": 3.361,
      "language": "en"
    },
    {
      "text": "there's pictures. There's a lot of stuff",
      "start": 259.68,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "you can uh look on online. So, but",
      "start": 261.6,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "instead I'm going to work with simpler",
      "start": 265.36,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "models and really talk about the",
      "start": 266.96,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "primitives and the resource accounting",
      "start": 269.199,
      "duration": 4.161,
      "language": "en"
    },
    {
      "text": "piece. Okay. So remember last time I",
      "start": 270.88,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "said what kinds of knowledge can you",
      "start": 273.36,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "learn? So mechanics um in this lecture",
      "start": 274.96,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "it's going to be just pietorch and",
      "start": 278.72,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "understanding how pietorch works at um a",
      "start": 280.8,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "fairly primitive uh level. So that's",
      "start": 283.6,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "will be pretty straightforward. Mindset",
      "start": 285.84,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "is about resource accounting and it's",
      "start": 288.0,
      "duration": 4.479,
      "language": "en"
    },
    {
      "text": "not hard. It's just you just have to do",
      "start": 290.8,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "it. And intuitions um unfortunately this",
      "start": 292.479,
      "duration": 5.681,
      "language": "en"
    },
    {
      "text": "is just going to be broad strokes uh you",
      "start": 296.32,
      "duration": 3.04,
      "language": "en"
    },
    {
      "text": "know for now. Actually, there's not",
      "start": 298.16,
      "duration": 2.8,
      "language": "en"
    },
    {
      "text": "really much intuition that I'm going to",
      "start": 299.36,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "talk about in terms of how um anything",
      "start": 300.96,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "we're doing translates to good models.",
      "start": 303.68,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "Um this is more about the M mechanics",
      "start": 305.36,
      "duration": 3.72,
      "language": "en"
    },
    {
      "text": "and",
      "start": 307.6,
      "duration": 4.76,
      "language": "en"
    },
    {
      "text": "mindset. Okay. So, let's start with",
      "start": 309.08,
      "duration": 6.679,
      "language": "en"
    },
    {
      "text": "memory accounting. Um and then I'll talk",
      "start": 312.36,
      "duration": 5.559,
      "language": "en"
    },
    {
      "text": "about compute accounting. Um and then",
      "start": 315.759,
      "duration": 6.561,
      "language": "en"
    },
    {
      "text": "we'll build up bottom up. Okay. So, the",
      "start": 317.919,
      "duration": 6.321,
      "language": "en"
    },
    {
      "text": "best place to start is a tensor. So",
      "start": 322.32,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "tensors are the building block of for",
      "start": 324.24,
      "duration": 4.239,
      "language": "en"
    },
    {
      "text": "storing everything in deep learning.",
      "start": 326.56,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "Parameters, gradients, optimizer, state,",
      "start": 328.479,
      "duration": 4.241,
      "language": "en"
    },
    {
      "text": "uh data, activations. There's sort of",
      "start": 330.8,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "these atoms. Um you can read lots of",
      "start": 332.72,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "documentation about them. Um you're",
      "start": 335.44,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "probably very familiar with how to",
      "start": 338.24,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "create tensors. There's creating tensors",
      "start": 340.08,
      "duration": 5.6,
      "language": "en"
    },
    {
      "text": "different ways. Um you can also create a",
      "start": 342.32,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "tensor and not initialize it and use",
      "start": 345.68,
      "duration": 5.519,
      "language": "en"
    },
    {
      "text": "some special uh initialization um for",
      "start": 347.52,
      "duration": 8.56,
      "language": "en"
    },
    {
      "text": "the parameters um to um if you want. Um",
      "start": 351.199,
      "duration": 7.961,
      "language": "en"
    },
    {
      "text": "okay so those are um you know",
      "start": 356.08,
      "duration": 6.959,
      "language": "en"
    },
    {
      "text": "tensors. Um so let's talk about memory",
      "start": 359.16,
      "duration": 7.319,
      "language": "en"
    },
    {
      "text": "and how much uh memory tensors take up.",
      "start": 363.039,
      "duration": 6.88,
      "language": "en"
    },
    {
      "text": "So every tensor that we'll probably be",
      "start": 366.479,
      "duration": 4.961,
      "language": "en"
    },
    {
      "text": "interested in is stored as a",
      "start": 369.919,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "floatingoint number. And so there's many",
      "start": 371.44,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "ways to represent floating point. So the",
      "start": 374.24,
      "duration": 6.799,
      "language": "en"
    },
    {
      "text": "most default uh way um is float 32. And",
      "start": 376.16,
      "duration": 8.319,
      "language": "en"
    },
    {
      "text": "float toy2 has 32 uh bits. They're",
      "start": 381.039,
      "duration": 5.6,
      "language": "en"
    },
    {
      "text": "allocated one for sign, eight for",
      "start": 384.479,
      "duration": 5.121,
      "language": "en"
    },
    {
      "text": "exponent, and 23 for the you know the",
      "start": 386.639,
      "duration": 4.961,
      "language": "en"
    },
    {
      "text": "fraction. So exponent gives you dynamic",
      "start": 389.6,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "range and fraction gives you you know uh",
      "start": 391.6,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "different basically specifies different",
      "start": 394.16,
      "duration": 6.159,
      "language": "en"
    },
    {
      "text": "values. Um so flow 32 is also known as",
      "start": 396.16,
      "duration": 8.159,
      "language": "en"
    },
    {
      "text": "FP32 or single uh you know precision um",
      "start": 400.319,
      "duration": 7.201,
      "language": "en"
    },
    {
      "text": "is sort of the gold uh standard um in",
      "start": 404.319,
      "duration": 5.681,
      "language": "en"
    },
    {
      "text": "you know computing. Some people also",
      "start": 407.52,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "refer to flow 3 as full precision.",
      "start": 410.0,
      "duration": 4.319,
      "language": "en"
    },
    {
      "text": "That's a little bit confusing because",
      "start": 412.72,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "full is really depending on who you're",
      "start": 414.319,
      "duration": 4.081,
      "language": "en"
    },
    {
      "text": "talking to. Um, if you're talking to a",
      "start": 416.319,
      "duration": 3.841,
      "language": "en"
    },
    {
      "text": "scientific computing person, they'll",
      "start": 418.4,
      "duration": 3.359,
      "language": "en"
    },
    {
      "text": "kind of laugh at you when you say float",
      "start": 420.16,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "32 is uh is really full because they'll",
      "start": 421.759,
      "duration": 6.081,
      "language": "en"
    },
    {
      "text": "use float 64 or even more. Um, but if",
      "start": 424.72,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "you're talking to a machine learning",
      "start": 427.84,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "person, float 32 is the max you'll ever",
      "start": 428.88,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "probably need to go because deep",
      "start": 431.44,
      "duration": 5.159,
      "language": "en"
    },
    {
      "text": "learning is is kind of sloppy like",
      "start": 433.52,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "that. Okay. So, let's look at the",
      "start": 436.599,
      "duration": 5.241,
      "language": "en"
    },
    {
      "text": "memory. So, the memory is it's very",
      "start": 439.52,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "simple. It's determined by the number of",
      "start": 441.84,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "values you have in your tensor and the",
      "start": 443.36,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "data type of each value. Okay, so if you",
      "start": 445.12,
      "duration": 6.479,
      "language": "en"
    },
    {
      "text": "create a torch sensor of 4x8 matrix, um",
      "start": 447.44,
      "duration": 6.8,
      "language": "en"
    },
    {
      "text": "the default it will give you a type of",
      "start": 451.599,
      "duration": 8.561,
      "language": "en"
    },
    {
      "text": "uh flow 32 um the size is 4 by8 and the",
      "start": 454.24,
      "duration": 9.12,
      "language": "en"
    },
    {
      "text": "number of elements is 32. Each element",
      "start": 460.16,
      "duration": 6.64,
      "language": "en"
    },
    {
      "text": "size is four bytes. Um 32 bits is four",
      "start": 463.36,
      "duration": 7.36,
      "language": "en"
    },
    {
      "text": "bytes. Um and the memory usage is simply",
      "start": 466.8,
      "duration": 5.839,
      "language": "en"
    },
    {
      "text": "the number of elements times the number",
      "start": 470.72,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "of um uh size of each element and",
      "start": 472.639,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "that'll give you 128 bytes. Okay, so",
      "start": 475.44,
      "duration": 5.199,
      "language": "en"
    },
    {
      "text": "this is should be pretty um easy and",
      "start": 477.759,
      "duration": 4.961,
      "language": "en"
    },
    {
      "text": "just to give some intuition if you get",
      "start": 480.639,
      "duration": 5.081,
      "language": "en"
    },
    {
      "text": "the one matrix in the F4 layer of uh",
      "start": 482.72,
      "duration": 6.24,
      "language": "en"
    },
    {
      "text": "GPT3 is this number by this number and",
      "start": 485.72,
      "duration": 6.36,
      "language": "en"
    },
    {
      "text": "that gives you 2.3 gigabytes. Okay, so",
      "start": 488.96,
      "duration": 5.679,
      "language": "en"
    },
    {
      "text": "that's one matrix is you know can these",
      "start": 492.08,
      "duration": 6.64,
      "language": "en"
    },
    {
      "text": "matrices can be pretty big.",
      "start": 494.639,
      "duration": 4.081,
      "language": "en"
    },
    {
      "text": "Okay, so uh float 32 is the default. Um",
      "start": 498.8,
      "duration": 7.679,
      "language": "en"
    },
    {
      "text": "but of course these matrices get big. So",
      "start": 502.72,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "um naturally you want to make them",
      "start": 506.479,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "smaller so you use less memory. And also",
      "start": 508.56,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "it turns out if you make them smaller",
      "start": 510.8,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "you also uh make it go faster too. Okay.",
      "start": 512.399,
      "duration": 9.12,
      "language": "en"
    },
    {
      "text": "Um so another type of uh representation",
      "start": 516.24,
      "duration": 8.08,
      "language": "en"
    },
    {
      "text": "is called float 16 and as the name",
      "start": 521.519,
      "duration": 6.561,
      "language": "en"
    },
    {
      "text": "suggests it's uh 16 bits um where both",
      "start": 524.32,
      "duration": 5.92,
      "language": "en"
    },
    {
      "text": "the exponent and the fraction are um",
      "start": 528.08,
      "duration": 8.8,
      "language": "en"
    },
    {
      "text": "shrunk down from um 8 to 5 and 23 to 10.",
      "start": 530.24,
      "duration": 8.719,
      "language": "en"
    },
    {
      "text": "Okay, so this is known as half precision",
      "start": 536.88,
      "duration": 6.44,
      "language": "en"
    },
    {
      "text": "and it cuts down half the memory. Um,",
      "start": 538.959,
      "duration": 7.841,
      "language": "en"
    },
    {
      "text": "and that's all great except for the",
      "start": 543.32,
      "duration": 6.84,
      "language": "en"
    },
    {
      "text": "dynamic range uh for these float 16",
      "start": 546.8,
      "duration": 5.039,
      "language": "en"
    },
    {
      "text": "isn't great. So, for example, if you try",
      "start": 550.16,
      "duration": 6.48,
      "language": "en"
    },
    {
      "text": "to make a number um like uh 10 e 1 e",
      "start": 551.839,
      "duration": 8.44,
      "language": "en"
    },
    {
      "text": "minus 8 in flow 16",
      "start": 556.64,
      "duration": 7.36,
      "language": "en"
    },
    {
      "text": "um it uh basically rounds down to zero",
      "start": 560.279,
      "duration": 7.0,
      "language": "en"
    },
    {
      "text": "and you get underflow. Okay. So uh the",
      "start": 564.0,
      "duration": 5.92,
      "language": "en"
    },
    {
      "text": "flow 16 is not great for representing um",
      "start": 567.279,
      "duration": 4.801,
      "language": "en"
    },
    {
      "text": "very small numbers or or very big",
      "start": 569.92,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "numbers as a matter of fact. Um so if",
      "start": 572.08,
      "duration": 6.4,
      "language": "en"
    },
    {
      "text": "you use float 16 u for training for",
      "start": 574.8,
      "duration": 5.92,
      "language": "en"
    },
    {
      "text": "small models it's probably going to be",
      "start": 578.48,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "okay but for large models when you're um",
      "start": 580.72,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "having lots of matrices and you can get",
      "start": 582.8,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "uh instability or underflow or overflow",
      "start": 585.92,
      "duration": 8.16,
      "language": "en"
    },
    {
      "text": "and bad things happen. Um okay so um one",
      "start": 588.48,
      "duration": 8.16,
      "language": "en"
    },
    {
      "text": "um thing that has happened which is nice",
      "start": 594.08,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "is there's been another representation",
      "start": 596.64,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "of B float 16 which stands for brain",
      "start": 599.12,
      "duration": 6.159,
      "language": "en"
    },
    {
      "text": "float um this was developed in 2018 to",
      "start": 602.08,
      "duration": 6.16,
      "language": "en"
    },
    {
      "text": "address the issue that for deep learning",
      "start": 605.279,
      "duration": 5.921,
      "language": "en"
    },
    {
      "text": "um we actually care about dynamic range",
      "start": 608.24,
      "duration": 6.159,
      "language": "en"
    },
    {
      "text": "more than uh we care about this",
      "start": 611.2,
      "duration": 6.079,
      "language": "en"
    },
    {
      "text": "fraction. So basically BF16 allocates",
      "start": 614.399,
      "duration": 5.0,
      "language": "en"
    },
    {
      "text": "more to the exponent and less to the",
      "start": 617.279,
      "duration": 4.641,
      "language": "en"
    },
    {
      "text": "fraction. Okay. So it uses the same",
      "start": 619.399,
      "duration": 5.641,
      "language": "en"
    },
    {
      "text": "memory as um floating point 16 but it",
      "start": 621.92,
      "duration": 6.8,
      "language": "en"
    },
    {
      "text": "has a dynamic range of float 32. Okay.",
      "start": 625.04,
      "duration": 6.239,
      "language": "en"
    },
    {
      "text": "So that sounds really good. Um and it",
      "start": 628.72,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "actually uh the catch is that this",
      "start": 631.279,
      "duration": 3.761,
      "language": "en"
    },
    {
      "text": "resolution which is determined by the",
      "start": 633.6,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "fraction is worse. Uh but this doesn't",
      "start": 635.04,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "matter as much uh for deep learning. So",
      "start": 636.88,
      "duration": 6.24,
      "language": "en"
    },
    {
      "text": "now if you try to create a a tensor with",
      "start": 639.92,
      "duration": 7.2,
      "language": "en"
    },
    {
      "text": "one E minus 8 in BF16 then you get",
      "start": 643.12,
      "duration": 6.8,
      "language": "en"
    },
    {
      "text": "something that's not",
      "start": 647.12,
      "duration": 2.8,
      "language": "en"
    },
    {
      "text": "zero. Okay. So um you can dive into the",
      "start": 650.44,
      "duration": 4.76,
      "language": "en"
    },
    {
      "text": "details. I'm not going to go into this",
      "start": 653.76,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "but you can stare at the actual full",
      "start": 655.2,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "specs of all the different floating",
      "start": 657.44,
      "duration": 3.32,
      "language": "en"
    },
    {
      "text": "point you know",
      "start": 659.2,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "operations. Okay. Okay, so BF16 is",
      "start": 660.76,
      "duration": 5.8,
      "language": "en"
    },
    {
      "text": "basically what uh you will typically use",
      "start": 663.279,
      "duration": 5.841,
      "language": "en"
    },
    {
      "text": "to do computations because um it's it's",
      "start": 666.56,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "sort of you know good enough um for fear",
      "start": 669.12,
      "duration": 6.08,
      "language": "en"
    },
    {
      "text": "computations. It turns out that um for",
      "start": 672.32,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "storing optimizer states and parameters",
      "start": 675.2,
      "duration": 6.72,
      "language": "en"
    },
    {
      "text": "you still need float 32 um for otherwise",
      "start": 677.76,
      "duration": 7.639,
      "language": "en"
    },
    {
      "text": "your training will go um haywire.",
      "start": 681.92,
      "duration": 9.039,
      "language": "en"
    },
    {
      "text": "Um so if you're bold so now we have um",
      "start": 685.399,
      "duration": 8.281,
      "language": "en"
    },
    {
      "text": "something called FP8 or uh 8bit and as",
      "start": 690.959,
      "duration": 5.281,
      "language": "en"
    },
    {
      "text": "the name suggests um this was developed",
      "start": 693.68,
      "duration": 7.36,
      "language": "en"
    },
    {
      "text": "in 2022 um um by you know Nvidia so now",
      "start": 696.24,
      "duration": 8.88,
      "language": "en"
    },
    {
      "text": "they have um essentially if you look at",
      "start": 701.04,
      "duration": 8.4,
      "language": "en"
    },
    {
      "text": "FP and BF16 it's like this and FP wow",
      "start": 705.12,
      "duration": 6.32,
      "language": "en"
    },
    {
      "text": "you really don't have that many bits to",
      "start": 709.44,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "store stuff right so it's very crude Um",
      "start": 711.44,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "there's there's two sort of variants",
      "start": 714.8,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "depending on if you want to have more",
      "start": 716.72,
      "duration": 6.32,
      "language": "en"
    },
    {
      "text": "resolution or more um dynamic range. Um",
      "start": 718.56,
      "duration": 6.8,
      "language": "en"
    },
    {
      "text": "and um I'm not going to say too much",
      "start": 723.04,
      "duration": 4.239,
      "language": "en"
    },
    {
      "text": "about this but FE8 is you know supported",
      "start": 725.36,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "by H100. It's not really available on",
      "start": 727.279,
      "duration": 4.841,
      "language": "en"
    },
    {
      "text": "previous generation.",
      "start": 730.16,
      "duration": 5.359,
      "language": "en"
    },
    {
      "text": "Um but but at a high level, you know,",
      "start": 732.12,
      "duration": 5.48,
      "language": "en"
    },
    {
      "text": "training with float 32, which is I",
      "start": 735.519,
      "duration": 5.521,
      "language": "en"
    },
    {
      "text": "think, you know, is is what you would do",
      "start": 737.6,
      "duration": 5.359,
      "language": "en"
    },
    {
      "text": "if you're not trying to optimize, you",
      "start": 741.04,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "know, too much and it's it's sort of",
      "start": 742.959,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "safe. It requires more memory. Um you",
      "start": 744.72,
      "duration": 7.08,
      "language": "en"
    },
    {
      "text": "can go down to FP8 or",
      "start": 748.399,
      "duration": 6.56,
      "language": "en"
    },
    {
      "text": "BF16. Um and but you can get some",
      "start": 751.8,
      "duration": 5.4,
      "language": "en"
    },
    {
      "text": "instability. Um, basically I don't think",
      "start": 754.959,
      "duration": 4.481,
      "language": "en"
    },
    {
      "text": "you would probably want to use uh a",
      "start": 757.2,
      "duration": 6.639,
      "language": "en"
    },
    {
      "text": "float 16 at this point for for deep",
      "start": 759.44,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "learning. And um you can become more",
      "start": 764.12,
      "duration": 7.56,
      "language": "en"
    },
    {
      "text": "sophisticated by looking at particular",
      "start": 768.32,
      "duration": 6.639,
      "language": "en"
    },
    {
      "text": "places in your pipeline either forward",
      "start": 771.68,
      "duration": 5.839,
      "language": "en"
    },
    {
      "text": "pass or backward pass or optimizers or",
      "start": 774.959,
      "duration": 5.521,
      "language": "en"
    },
    {
      "text": "gradient accumulation and really figure",
      "start": 777.519,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "out what the minimum precision you need",
      "start": 780.48,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "at this particular places. And that's",
      "start": 782.399,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "called gets into kind of mixed precision",
      "start": 784.88,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "training. So for example, some people",
      "start": 786.959,
      "duration": 5.761,
      "language": "en"
    },
    {
      "text": "like to use flow 32 for the you know the",
      "start": 789.279,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "attention to make sure that doesn't kind",
      "start": 792.72,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "of uh you know get messed up of what",
      "start": 794.959,
      "duration": 5.041,
      "language": "en"
    },
    {
      "text": "four simple F4 passes with Matt Mall's",
      "start": 797.2,
      "duration": 5.879,
      "language": "en"
    },
    {
      "text": "BF-16 is is is",
      "start": 800.0,
      "duration": 6.8,
      "language": "en"
    },
    {
      "text": "fine. Okay, pause a bit for for",
      "start": 803.079,
      "duration": 6.681,
      "language": "en"
    },
    {
      "text": "questions. So we talked about tensors",
      "start": 806.8,
      "duration": 6.32,
      "language": "en"
    },
    {
      "text": "and we looked at uh depending on how",
      "start": 809.76,
      "duration": 5.759,
      "language": "en"
    },
    {
      "text": "what representation how much storage",
      "start": 813.12,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "they take.",
      "start": 815.519,
      "duration": 3.681,
      "language": "en"
    },
    {
      "text": "Yeah. Can you just clarify about the mix",
      "start": 817.36,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "position like when you would use 32 in",
      "start": 819.2,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "the be? Yeah. So the question is when",
      "start": 821.04,
      "duration": 8.28,
      "language": "en"
    },
    {
      "text": "would you use um uh flow 32 or BF16?",
      "start": 823.92,
      "duration": 8.159,
      "language": "en"
    },
    {
      "text": "Um I don't have time to get into the you",
      "start": 829.32,
      "duration": 4.68,
      "language": "en"
    },
    {
      "text": "know the exact uh details and it sort of",
      "start": 832.079,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "varies depending on the model size and",
      "start": 834.0,
      "duration": 5.519,
      "language": "en"
    },
    {
      "text": "everything but um generally for the the",
      "start": 835.68,
      "duration": 5.519,
      "language": "en"
    },
    {
      "text": "parameters and optimizer states you use",
      "start": 839.519,
      "duration": 3.921,
      "language": "en"
    },
    {
      "text": "float 32. You can think about BF16 as",
      "start": 841.199,
      "duration": 4.801,
      "language": "en"
    },
    {
      "text": "something that's more transitory like",
      "start": 843.44,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "you you basically take your parameters",
      "start": 846.0,
      "duration": 4.959,
      "language": "en"
    },
    {
      "text": "you cast it to BF16 and you kind of run",
      "start": 848.0,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "ahead with that model but then the thing",
      "start": 850.959,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "that you're going to accumulate over",
      "start": 853.04,
      "duration": 5.88,
      "language": "en"
    },
    {
      "text": "time you want to have higher precision.",
      "start": 854.56,
      "duration": 4.36,
      "language": "en"
    },
    {
      "text": "Yeah. Okay. So now let's talk about uh",
      "start": 860.279,
      "duration": 7.201,
      "language": "en"
    },
    {
      "text": "compute. So that was memory.",
      "start": 864.88,
      "duration": 5.319,
      "language": "en"
    },
    {
      "text": "Um so",
      "start": 867.48,
      "duration": 5.96,
      "language": "en"
    },
    {
      "text": "uh compute obviously depends on what the",
      "start": 870.199,
      "duration": 5.801,
      "language": "en"
    },
    {
      "text": "hardware you know is by default tensors",
      "start": 873.44,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "are stored in CPU. So for example if you",
      "start": 876.0,
      "duration": 5.279,
      "language": "en"
    },
    {
      "text": "just in PyTorch um say X equals torch.0",
      "start": 877.92,
      "duration": 6.159,
      "language": "en"
    },
    {
      "text": "is 3232 then it'll put it on your CPU.",
      "start": 881.279,
      "duration": 6.721,
      "language": "en"
    },
    {
      "text": "it'll be in um the CPU memory. Um now,",
      "start": 884.079,
      "duration": 6.401,
      "language": "en"
    },
    {
      "text": "of course, that's no good because if",
      "start": 888.0,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "you're not using your GPU, then you're",
      "start": 890.48,
      "duration": 3.2,
      "language": "en"
    },
    {
      "text": "going to be orders of magnitude too",
      "start": 892.0,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "slow. So, you need to explicitly say in",
      "start": 893.68,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "PyTorch that you need to move it to the",
      "start": 896.8,
      "duration": 6.399,
      "language": "en"
    },
    {
      "text": "GPU and this is uh you it's actually",
      "start": 898.88,
      "duration": 6.88,
      "language": "en"
    },
    {
      "text": "just to make it very clear in pictures,",
      "start": 903.199,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "there's a CPU, it has RAM, and that um",
      "start": 905.76,
      "duration": 6.319,
      "language": "en"
    },
    {
      "text": "has to be moved over um to the the GPU.",
      "start": 908.959,
      "duration": 6.32,
      "language": "en"
    },
    {
      "text": "there's a data transfer um which is is c",
      "start": 912.079,
      "duration": 7.481,
      "language": "en"
    },
    {
      "text": "um which takes some work um takes some",
      "start": 915.279,
      "duration": 7.761,
      "language": "en"
    },
    {
      "text": "time okay so whenever you have a tensor",
      "start": 919.56,
      "duration": 5.959,
      "language": "en"
    },
    {
      "text": "in pietorch you should always keep in",
      "start": 923.04,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "your mind where is this residing because",
      "start": 925.519,
      "duration": 3.841,
      "language": "en"
    },
    {
      "text": "just looking at the variable or just",
      "start": 927.92,
      "duration": 2.719,
      "language": "en"
    },
    {
      "text": "looking at the code you can't always",
      "start": 929.36,
      "duration": 4.479,
      "language": "en"
    },
    {
      "text": "tell and if you want to be careful about",
      "start": 930.639,
      "duration": 5.841,
      "language": "en"
    },
    {
      "text": "computation and data movement you have",
      "start": 933.839,
      "duration": 6.081,
      "language": "en"
    },
    {
      "text": "to really know where it is um you can",
      "start": 936.48,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "probably do things like assert",
      "start": 939.92,
      "duration": 4.479,
      "language": "en"
    },
    {
      "text": "um where it is in various places of code",
      "start": 942.0,
      "duration": 5.72,
      "language": "en"
    },
    {
      "text": "just to um document or be",
      "start": 944.399,
      "duration": 6.521,
      "language": "en"
    },
    {
      "text": "sure. Okay. So",
      "start": 947.72,
      "duration": 7.88,
      "language": "en"
    },
    {
      "text": "um so let's look at uh what hardware we",
      "start": 950.92,
      "duration": 7.4,
      "language": "en"
    },
    {
      "text": "have. So we have um in this case we have",
      "start": 955.6,
      "duration": 6.32,
      "language": "en"
    },
    {
      "text": "one GPU. Um this was run on the H100",
      "start": 958.32,
      "duration": 6.4,
      "language": "en"
    },
    {
      "text": "clusters that you guys have access to.",
      "start": 961.92,
      "duration": 8.96,
      "language": "en"
    },
    {
      "text": "And um this um GPU is a you know H100 80",
      "start": 964.72,
      "duration": 9.4,
      "language": "en"
    },
    {
      "text": "GB of high bandwidth memory",
      "start": 970.88,
      "duration": 6.16,
      "language": "en"
    },
    {
      "text": "um and there's a you know it gives you",
      "start": 974.12,
      "duration": 7.44,
      "language": "en"
    },
    {
      "text": "the cache size and so on. Okay. So",
      "start": 977.04,
      "duration": 9.039,
      "language": "en"
    },
    {
      "text": "um so if you have remember the x is on",
      "start": 981.56,
      "duration": 8.04,
      "language": "en"
    },
    {
      "text": "uh CPU um you can move it uh just by",
      "start": 986.079,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "specifying you know two which is a kind",
      "start": 989.6,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "of a general pietorch function. Um you",
      "start": 991.839,
      "duration": 4.401,
      "language": "en"
    },
    {
      "text": "can also create a tensor directly on a",
      "start": 994.56,
      "duration": 4.36,
      "language": "en"
    },
    {
      "text": "GPU so you don't have to move it at all.",
      "start": 996.24,
      "duration": 6.719,
      "language": "en"
    },
    {
      "text": "Um and if everything goes well, I'm",
      "start": 998.92,
      "duration": 5.8,
      "language": "en"
    },
    {
      "text": "looking at the memory allocated before",
      "start": 1002.959,
      "duration": 5.281,
      "language": "en"
    },
    {
      "text": "and after. Uh the difference should be",
      "start": 1004.72,
      "duration": 10.4,
      "language": "en"
    },
    {
      "text": "exactly uh two 30 by 32x 32 um matrices",
      "start": 1008.24,
      "duration": 12.279,
      "language": "en"
    },
    {
      "text": "of four byte floats. Okay. So um it's",
      "start": 1015.12,
      "duration": 7.36,
      "language": "en"
    },
    {
      "text": "a192. Okay. Okay, so this is just a",
      "start": 1020.519,
      "duration": 5.721,
      "language": "en"
    },
    {
      "text": "sanity check that um the code is is uh",
      "start": 1022.48,
      "duration": 6.839,
      "language": "en"
    },
    {
      "text": "is doing what is",
      "start": 1026.24,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "advertised. Okay, so now you have your",
      "start": 1029.319,
      "duration": 6.6,
      "language": "en"
    },
    {
      "text": "answers on the GPU. Um what do you uh",
      "start": 1032.24,
      "duration": 6.16,
      "language": "en"
    },
    {
      "text": "do? So there's many operations um that",
      "start": 1035.919,
      "duration": 4.801,
      "language": "en"
    },
    {
      "text": "you'll be needing for assignment one and",
      "start": 1038.4,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "in general to do any deep learning",
      "start": 1040.72,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "application. Um and most tensors you",
      "start": 1042.559,
      "duration": 5.201,
      "language": "en"
    },
    {
      "text": "just create by performing operations on",
      "start": 1045.28,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "other tensors and each operations has",
      "start": 1047.76,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "some memory and compute uh footprint. So",
      "start": 1049.679,
      "duration": 5.921,
      "language": "en"
    },
    {
      "text": "let's make sure we understand uh that.",
      "start": 1052.48,
      "duration": 7.439,
      "language": "en"
    },
    {
      "text": "Um so first of all um what is actually a",
      "start": 1055.6,
      "duration": 7.12,
      "language": "en"
    },
    {
      "text": "tensor in PyTorch right tensors are a",
      "start": 1059.919,
      "duration": 6.161,
      "language": "en"
    },
    {
      "text": "mathematical object in PyTorch they're",
      "start": 1062.72,
      "duration": 6.64,
      "language": "en"
    },
    {
      "text": "actually pointers into some allocated",
      "start": 1066.08,
      "duration": 7.08,
      "language": "en"
    },
    {
      "text": "memory. Okay. So if you have let's say",
      "start": 1069.36,
      "duration": 8.48,
      "language": "en"
    },
    {
      "text": "um a matrix 4x4 matrix um what it",
      "start": 1073.16,
      "duration": 7.68,
      "language": "en"
    },
    {
      "text": "actually looks like is a long",
      "start": 1077.84,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "array and what the tensor has is",
      "start": 1080.84,
      "duration": 5.64,
      "language": "en"
    },
    {
      "text": "metadata that specifies how to get to",
      "start": 1083.84,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "address into that array. And the",
      "start": 1086.48,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "metadata is going to be um you know two",
      "start": 1088.48,
      "duration": 5.92,
      "language": "en"
    },
    {
      "text": "numbers a stride for each or actually",
      "start": 1091.2,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "number of one number per dimension of",
      "start": 1094.4,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "the of the tensor. in this case because",
      "start": 1096.88,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "there's um you know two dimensions it's",
      "start": 1099.52,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "uh you know stride zero and stride one",
      "start": 1102.16,
      "duration": 8.24,
      "language": "en"
    },
    {
      "text": "um stride zero uh specifies if you were",
      "start": 1105.36,
      "duration": 10.08,
      "language": "en"
    },
    {
      "text": "in dimension zero to get to the next row",
      "start": 1110.4,
      "duration": 7.12,
      "language": "en"
    },
    {
      "text": "to increment that index how many do you",
      "start": 1115.44,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "have to skip and so going down the rows",
      "start": 1117.52,
      "duration": 7.6,
      "language": "en"
    },
    {
      "text": "you skip four so stride uh zero is four",
      "start": 1120.32,
      "duration": 8.4,
      "language": "en"
    },
    {
      "text": "and um to go to the next column um you",
      "start": 1125.12,
      "duration": 8.72,
      "language": "en"
    },
    {
      "text": "skip one. So stride one is one. Okay. So",
      "start": 1128.72,
      "duration": 7.36,
      "language": "en"
    },
    {
      "text": "with that to find an element let's say",
      "start": 1133.84,
      "duration": 5.839,
      "language": "en"
    },
    {
      "text": "one two one comma two um it's simply",
      "start": 1136.08,
      "duration": 6.16,
      "language": "en"
    },
    {
      "text": "just um multiply the indexes by the",
      "start": 1139.679,
      "duration": 6.081,
      "language": "en"
    },
    {
      "text": "stride and you get to your index which",
      "start": 1142.24,
      "duration": 7.04,
      "language": "en"
    },
    {
      "text": "is uh six here. So that would be here or",
      "start": 1145.76,
      "duration": 6.24,
      "language": "en"
    },
    {
      "text": "or here. Okay. So that's basically",
      "start": 1149.28,
      "duration": 5.759,
      "language": "en"
    },
    {
      "text": "what's going underneath the hood um for",
      "start": 1152.0,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "for for tensors.",
      "start": 1155.039,
      "duration": 8.0,
      "language": "en"
    },
    {
      "text": "Okay, so this is um relevant because um",
      "start": 1157.44,
      "duration": 7.68,
      "language": "en"
    },
    {
      "text": "you can have multiple tensors that use",
      "start": 1163.039,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "the same storage and this is useful",
      "start": 1165.12,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "because you don't want to copy the",
      "start": 1167.039,
      "duration": 3.281,
      "language": "en"
    },
    {
      "text": "tensor all over the all over the place.",
      "start": 1168.4,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "So imagine you have a 2x3 matrix here.",
      "start": 1170.32,
      "duration": 7.04,
      "language": "en"
    },
    {
      "text": "Um many operations um don't actually",
      "start": 1173.6,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "create a new tensor. They just create a",
      "start": 1177.36,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "different view and doesn't make a copy.",
      "start": 1178.88,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "So you have to keep make sure that um um",
      "start": 1181.12,
      "duration": 6.4,
      "language": "en"
    },
    {
      "text": "your mutations uh if you start mutating",
      "start": 1184.88,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "one tensor it's going to cause the other",
      "start": 1187.52,
      "duration": 2.76,
      "language": "en"
    },
    {
      "text": "one to",
      "start": 1189.44,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "mutate. Okay. So um for example if you",
      "start": 1190.28,
      "duration": 7.04,
      "language": "en"
    },
    {
      "text": "just get um row",
      "start": 1194.16,
      "duration": 6.879,
      "language": "en"
    },
    {
      "text": "zero okay so remember y is uh this",
      "start": 1197.32,
      "duration": 8.12,
      "language": "en"
    },
    {
      "text": "tensor and sorry x is 1 2 3 4 5 6 and y",
      "start": 1201.039,
      "duration": 7.561,
      "language": "en"
    },
    {
      "text": "is x0 which is just the first",
      "start": 1205.44,
      "duration": 6.4,
      "language": "en"
    },
    {
      "text": "row. Okay. And um you can sort of double",
      "start": 1208.6,
      "duration": 4.92,
      "language": "en"
    },
    {
      "text": "check. There's this you know function I",
      "start": 1211.84,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "wrote that says if you look at the",
      "start": 1213.52,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "underlying storage whether these two",
      "start": 1215.6,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "tensors have the same storage or not.",
      "start": 1217.6,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "Okay. So this um definitely doesn't copy",
      "start": 1219.679,
      "duration": 6.641,
      "language": "en"
    },
    {
      "text": "the tensor. It just creates a view. Um",
      "start": 1222.64,
      "duration": 7.519,
      "language": "en"
    },
    {
      "text": "you can get column one. Um this also",
      "start": 1226.32,
      "duration": 7.359,
      "language": "en"
    },
    {
      "text": "doesn't uh copy the tensor. Um oops",
      "start": 1230.159,
      "duration": 6.241,
      "language": "en"
    },
    {
      "text": "don't need to do that. Um you can call a",
      "start": 1233.679,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "view function which can take any tensor",
      "start": 1236.4,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "and uh ch look at it in terms of the",
      "start": 1238.799,
      "duration": 8.961,
      "language": "en"
    },
    {
      "text": "different dimensions. Um 2x3 uh um",
      "start": 1242.24,
      "duration": 6.96,
      "language": "en"
    },
    {
      "text": "actually this should be maybe the other",
      "start": 1247.76,
      "duration": 5.039,
      "language": "en"
    },
    {
      "text": "way around. Uh as a 3x2 tensor",
      "start": 1249.2,
      "duration": 6.479,
      "language": "en"
    },
    {
      "text": "um so that's also doesn't you know",
      "start": 1252.799,
      "duration": 6.161,
      "language": "en"
    },
    {
      "text": "change uh do any copying. Um you can",
      "start": 1255.679,
      "duration": 7.841,
      "language": "en"
    },
    {
      "text": "transpose um that also doesn't copy. Um",
      "start": 1258.96,
      "duration": 6.56,
      "language": "en"
    },
    {
      "text": "and then you know like I said if you",
      "start": 1263.52,
      "duration": 6.08,
      "language": "en"
    },
    {
      "text": "start mutating X um then Y actually gets",
      "start": 1265.52,
      "duration": 7.039,
      "language": "en"
    },
    {
      "text": "mutated as well because X and Y are just",
      "start": 1269.6,
      "duration": 5.24,
      "language": "en"
    },
    {
      "text": "pointers into the same underlying",
      "start": 1272.559,
      "duration": 6.801,
      "language": "en"
    },
    {
      "text": "storage. Okay. So things are um one",
      "start": 1274.84,
      "duration": 6.36,
      "language": "en"
    },
    {
      "text": "thing that you have to be careful of is",
      "start": 1279.36,
      "duration": 5.199,
      "language": "en"
    },
    {
      "text": "that some views are contiguous which",
      "start": 1281.2,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "means that if you run through the tensor",
      "start": 1284.559,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "it's like just slide going through the",
      "start": 1286.24,
      "duration": 6.64,
      "language": "en"
    },
    {
      "text": "the uh array in your storage. um but",
      "start": 1288.88,
      "duration": 6.56,
      "language": "en"
    },
    {
      "text": "some are not. So in particular if you",
      "start": 1292.88,
      "duration": 5.6,
      "language": "en"
    },
    {
      "text": "transpose it now you're you know what",
      "start": 1295.44,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "does it mean when you're you're",
      "start": 1298.48,
      "duration": 2.72,
      "language": "en"
    },
    {
      "text": "transposing it you're you're sort of",
      "start": 1299.44,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "going down now so you're kind of if you",
      "start": 1301.2,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "imagine going through the tensor you're",
      "start": 1303.76,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "kind of skipping around and if you have",
      "start": 1305.6,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "a non-ontiguous tensor um then if you",
      "start": 1308.159,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "try to further view it in a different",
      "start": 1311.28,
      "duration": 4.92,
      "language": "en"
    },
    {
      "text": "way then this is not going to",
      "start": 1313.679,
      "duration": 6.88,
      "language": "en"
    },
    {
      "text": "work. Okay. So in some cases if you have",
      "start": 1316.2,
      "duration": 6.2,
      "language": "en"
    },
    {
      "text": "a non-ontiguous tensor you can make it",
      "start": 1320.559,
      "duration": 4.401,
      "language": "en"
    },
    {
      "text": "contiguous first and then uh you can",
      "start": 1322.4,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "apply whatever oper viewing operation",
      "start": 1324.96,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "you want to it and then in this case x",
      "start": 1326.96,
      "duration": 7.28,
      "language": "en"
    },
    {
      "text": "and y um do not have the same storage",
      "start": 1330.96,
      "duration": 6.4,
      "language": "en"
    },
    {
      "text": "because contiguous in this case uh makes",
      "start": 1334.24,
      "duration": 4.76,
      "language": "en"
    },
    {
      "text": "a",
      "start": 1337.36,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "copy. Okay. So um this is just ways of",
      "start": 1339.0,
      "duration": 8.679,
      "language": "en"
    },
    {
      "text": "slicing and dicing a tensor. Um uh views",
      "start": 1343.12,
      "duration": 7.439,
      "language": "en"
    },
    {
      "text": "are free so feel free to use them define",
      "start": 1347.679,
      "duration": 5.521,
      "language": "en"
    },
    {
      "text": "different variables um to make your it",
      "start": 1350.559,
      "duration": 5.201,
      "language": "en"
    },
    {
      "text": "sort of easier to read your code um",
      "start": 1353.2,
      "duration": 4.719,
      "language": "en"
    },
    {
      "text": "because they're not allocating any",
      "start": 1355.76,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "memory. Um but you know remember that",
      "start": 1357.919,
      "duration": 5.601,
      "language": "en"
    },
    {
      "text": "contiguous or reshape which is basically",
      "start": 1360.96,
      "duration": 6.8,
      "language": "en"
    },
    {
      "text": "contiguous view um uh can uh create a",
      "start": 1363.52,
      "duration": 7.96,
      "language": "en"
    },
    {
      "text": "copy and so just be careful what you're",
      "start": 1367.76,
      "duration": 8.88,
      "language": "en"
    },
    {
      "text": "doing. Okay, questions before moving",
      "start": 1371.48,
      "duration": 5.16,
      "language": "en"
    },
    {
      "text": "on. All right. So, um hopefully some a",
      "start": 1378.6,
      "duration": 6.12,
      "language": "en"
    },
    {
      "text": "lot of this will be review for those of",
      "start": 1382.799,
      "duration": 4.081,
      "language": "en"
    },
    {
      "text": "you have uh kind of done a lot of",
      "start": 1384.72,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "PyTorch before, but it's uh helpful to",
      "start": 1386.88,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "just do it systematically, make sure",
      "start": 1389.52,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "we're on the same page. So, um here are",
      "start": 1391.12,
      "duration": 6.48,
      "language": "en"
    },
    {
      "text": "some operations that uh do create new",
      "start": 1393.52,
      "duration": 6.8,
      "language": "en"
    },
    {
      "text": "tensors and in particular elementwise",
      "start": 1397.6,
      "duration": 4.959,
      "language": "en"
    },
    {
      "text": "operations all create new tensors",
      "start": 1400.32,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "obviously because you need a somewhere",
      "start": 1402.559,
      "duration": 5.081,
      "language": "en"
    },
    {
      "text": "else to store the new value. Um there's",
      "start": 1404.159,
      "duration": 7.201,
      "language": "en"
    },
    {
      "text": "a you know triangular U is also element",
      "start": 1407.64,
      "duration": 5.88,
      "language": "en"
    },
    {
      "text": "operation that uh comes in handy when",
      "start": 1411.36,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "you want to create a causal attention",
      "start": 1413.52,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "mask um which you'll need for your um",
      "start": 1416.0,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "your assignment. Um but nothing is",
      "start": 1418.48,
      "duration": 7.92,
      "language": "en"
    },
    {
      "text": "interesting that interesting uh here um",
      "start": 1420.96,
      "duration": 8.32,
      "language": "en"
    },
    {
      "text": "um okay so let's talk about map malls.",
      "start": 1426.4,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "So the bread and butter of deep learning",
      "start": 1429.28,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "is matrix multiplications. And I'm sure",
      "start": 1431.28,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "all of you have done a matrix",
      "start": 1434.0,
      "duration": 3.039,
      "language": "en"
    },
    {
      "text": "multiplication, but just in case this is",
      "start": 1435.2,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "what it looks like. Um you take a 16x32",
      "start": 1437.039,
      "duration": 5.681,
      "language": "en"
    },
    {
      "text": "times a 32x2 matrix, you get a 16x2",
      "start": 1440.32,
      "duration": 3.8,
      "language": "en"
    },
    {
      "text": "matrix.",
      "start": 1442.72,
      "duration": 3.0,
      "language": "en"
    },
    {
      "text": "Um",
      "start": 1444.12,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "and but in",
      "start": 1445.72,
      "duration": 5.319,
      "language": "en"
    },
    {
      "text": "general when we do you know our machine",
      "start": 1447.88,
      "duration": 6.52,
      "language": "en"
    },
    {
      "text": "learning application um all operations",
      "start": 1451.039,
      "duration": 6.721,
      "language": "en"
    },
    {
      "text": "are you want to do in a batch and in the",
      "start": 1454.4,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "case of language models this usually",
      "start": 1457.76,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "means um for every example in a batch",
      "start": 1459.52,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "and for every sequence in a batch you",
      "start": 1461.76,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "want to do something. Okay, so generally",
      "start": 1463.84,
      "duration": 4.719,
      "language": "en"
    },
    {
      "text": "what you're going to have in instead of",
      "start": 1466.72,
      "duration": 3.439,
      "language": "en"
    },
    {
      "text": "just a matrix is you're going to have a",
      "start": 1468.559,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "tensor where the dimensions are",
      "start": 1470.159,
      "duration": 3.921,
      "language": "en"
    },
    {
      "text": "typically batch sequence and then",
      "start": 1471.919,
      "duration": 4.161,
      "language": "en"
    },
    {
      "text": "whatever thing you're you're trying to",
      "start": 1474.08,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "do. In this case, it's a matrix for",
      "start": 1476.08,
      "duration": 6.479,
      "language": "en"
    },
    {
      "text": "every um token in your in your data set.",
      "start": 1477.76,
      "duration": 5.64,
      "language": "en"
    },
    {
      "text": "And",
      "start": 1482.559,
      "duration": 3.521,
      "language": "en"
    },
    {
      "text": "so you know PyTorch is nice enough to",
      "start": 1483.4,
      "duration": 5.0,
      "language": "en"
    },
    {
      "text": "make this uh work well for you. So when",
      "start": 1486.08,
      "duration": 5.199,
      "language": "en"
    },
    {
      "text": "you take this for",
      "start": 1488.4,
      "duration": 6.879,
      "language": "en"
    },
    {
      "text": "um the dimension tensor and this matrix",
      "start": 1491.279,
      "duration": 5.921,
      "language": "en"
    },
    {
      "text": "um what actually ends up happening is",
      "start": 1495.279,
      "duration": 4.961,
      "language": "en"
    },
    {
      "text": "that for every batch every example in",
      "start": 1497.2,
      "duration": 6.16,
      "language": "en"
    },
    {
      "text": "every token you're multiplying these two",
      "start": 1500.24,
      "duration": 6.24,
      "language": "en"
    },
    {
      "text": "um matrices. Okay. And then the result",
      "start": 1503.36,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "is that you get your your resulting",
      "start": 1506.48,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "matrix for each of the first two",
      "start": 1509.2,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "elements. So this is just like a there's",
      "start": 1511.12,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "nothing uh fancy going on but this is",
      "start": 1513.44,
      "duration": 5.359,
      "language": "en"
    },
    {
      "text": "just a pattern that um I think uh you is",
      "start": 1515.36,
      "duration": 6.799,
      "language": "en"
    },
    {
      "text": "helpful to you know think",
      "start": 1518.799,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "about okay so I'm going to take a little",
      "start": 1523.159,
      "duration": 6.961,
      "language": "en"
    },
    {
      "text": "bit of a a digression and talk about um",
      "start": 1526.88,
      "duration": 7.159,
      "language": "en"
    },
    {
      "text": "inops um and so the motivation",
      "start": 1530.12,
      "duration": 7.08,
      "language": "en"
    },
    {
      "text": "fors is the following so normally you in",
      "start": 1534.039,
      "duration": 6.52,
      "language": "en"
    },
    {
      "text": "pietorch you define some um you know",
      "start": 1537.2,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "tensors And then you see stuff like this",
      "start": 1540.559,
      "duration": 5.201,
      "language": "en"
    },
    {
      "text": "where you take x and multiply by y",
      "start": 1543.2,
      "duration": 5.92,
      "language": "en"
    },
    {
      "text": "transpose minus2 minus one and you're",
      "start": 1545.76,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "you kind of look at this and you say",
      "start": 1549.12,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "okay what is minus2 um well I think",
      "start": 1550.88,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "that's uh this sequence and then minus1",
      "start": 1553.6,
      "duration": 4.559,
      "language": "en"
    },
    {
      "text": "is this hidden because you're indexing",
      "start": 1556.64,
      "duration": 3.519,
      "language": "en"
    },
    {
      "text": "backwards. Um, and it's really easy to",
      "start": 1558.159,
      "duration": 4.081,
      "language": "en"
    },
    {
      "text": "mess this up because if you look at your",
      "start": 1560.159,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "code and you see minus one, minus two,",
      "start": 1562.24,
      "duration": 5.039,
      "language": "en"
    },
    {
      "text": "um, you're kind of if you're you're",
      "start": 1565.039,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "good, you write a bunch of comments, but",
      "start": 1567.279,
      "duration": 4.241,
      "language": "en"
    },
    {
      "text": "then the comments are can get out of",
      "start": 1569.36,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "date with the code and then, um, you you",
      "start": 1571.52,
      "duration": 6.72,
      "language": "en"
    },
    {
      "text": "have a bad time debugging. So the the",
      "start": 1574.08,
      "duration": 6.8,
      "language": "en"
    },
    {
      "text": "solution is to use inops here. So this",
      "start": 1578.24,
      "duration": 4.919,
      "language": "en"
    },
    {
      "text": "is inspired by Einstein's summation not",
      "start": 1580.88,
      "duration": 5.279,
      "language": "en"
    },
    {
      "text": "notation. Um, and the idea is that we're",
      "start": 1583.159,
      "duration": 5.281,
      "language": "en"
    },
    {
      "text": "just going to name all the dimensions.",
      "start": 1586.159,
      "duration": 6.241,
      "language": "en"
    },
    {
      "text": "um instead of you know relying on",
      "start": 1588.44,
      "duration": 7.16,
      "language": "en"
    },
    {
      "text": "indices essentially. Okay. So there's a",
      "start": 1592.4,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "library called Jack typing which um is",
      "start": 1595.6,
      "duration": 8.079,
      "language": "en"
    },
    {
      "text": "is is helpful for um as a as a way to",
      "start": 1597.6,
      "duration": 9.52,
      "language": "en"
    },
    {
      "text": "specify the dimensions in the types. So",
      "start": 1603.679,
      "duration": 6.161,
      "language": "en"
    },
    {
      "text": "normally in PyTorch you would just",
      "start": 1607.12,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "define write your code and then you",
      "start": 1609.84,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "would comment oh here's what the",
      "start": 1611.76,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "dimensions would be. So if you use jacks",
      "start": 1613.36,
      "duration": 4.559,
      "language": "en"
    },
    {
      "text": "typing then um you have this notation",
      "start": 1615.2,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "where as a string you just write down",
      "start": 1617.919,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "what the dimensions are. So this is a",
      "start": 1620.08,
      "duration": 5.0,
      "language": "en"
    },
    {
      "text": "slightly kind of more natural way of",
      "start": 1623.279,
      "duration": 5.121,
      "language": "en"
    },
    {
      "text": "documenting. Um now notice that there's",
      "start": 1625.08,
      "duration": 4.839,
      "language": "en"
    },
    {
      "text": "no enforcement here right because",
      "start": 1628.4,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "PyTorch types are sort of a a little bit",
      "start": 1629.919,
      "duration": 6.081,
      "language": "en"
    },
    {
      "text": "of a lie in in in PyTorch. Uh so it can",
      "start": 1632.32,
      "duration": 6.479,
      "language": "en"
    },
    {
      "text": "be enforced. You you can use a checker,",
      "start": 1636.0,
      "duration": 5.279,
      "language": "en"
    },
    {
      "text": "right? Yeah, you can write a check but",
      "start": 1638.799,
      "duration": 5.321,
      "language": "en"
    },
    {
      "text": "um not by default.",
      "start": 1641.279,
      "duration": 4.921,
      "language": "en"
    },
    {
      "text": "Yeah.",
      "start": 1644.12,
      "duration": 6.12,
      "language": "en"
    },
    {
      "text": "Um okay. So let's look at uh the you",
      "start": 1646.2,
      "duration": 5.8,
      "language": "en"
    },
    {
      "text": "know Ein sum. So Ein sum is basically",
      "start": 1650.24,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "matrix multiplication on steroids with",
      "start": 1652.0,
      "duration": 6.88,
      "language": "en"
    },
    {
      "text": "good bookkeeping. Um so here's our",
      "start": 1655.44,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "example here. We have X which is let's",
      "start": 1658.88,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "just think about this as uh you have a",
      "start": 1661.44,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "batch dimension, you have a sequence",
      "start": 1663.279,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "dimension and you have uh four um",
      "start": 1664.72,
      "duration": 6.88,
      "language": "en"
    },
    {
      "text": "hiddens and Y is the same size. um you",
      "start": 1667.039,
      "duration": 7.921,
      "language": "en"
    },
    {
      "text": "originally had to do this thing. Um and",
      "start": 1671.6,
      "duration": 7.439,
      "language": "en"
    },
    {
      "text": "now what you do instead is you basically",
      "start": 1674.96,
      "duration": 7.439,
      "language": "en"
    },
    {
      "text": "write down the the dimensions uh names",
      "start": 1679.039,
      "duration": 6.161,
      "language": "en"
    },
    {
      "text": "of the dimensions of the two uh tensors.",
      "start": 1682.399,
      "duration": 5.041,
      "language": "en"
    },
    {
      "text": "So batch sequence one hidden batch",
      "start": 1685.2,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "sequence two hidden and you just write",
      "start": 1687.44,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "what you dimension should appear in the",
      "start": 1690.32,
      "duration": 5.599,
      "language": "en"
    },
    {
      "text": "output. Okay. So I write batch here",
      "start": 1692.96,
      "duration": 6.319,
      "language": "en"
    },
    {
      "text": "because I just want to uh basically uh",
      "start": 1695.919,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "you know carry that over and then I",
      "start": 1699.279,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "write seek one and seek two and notice",
      "start": 1701.919,
      "duration": 5.281,
      "language": "en"
    },
    {
      "text": "that I don't write hidden and any",
      "start": 1704.799,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "dimension that is not named in output is",
      "start": 1707.2,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "just summed over and any dimension that",
      "start": 1709.12,
      "duration": 6.279,
      "language": "en"
    },
    {
      "text": "is um named is sort of just iterated",
      "start": 1711.36,
      "duration": 7.919,
      "language": "en"
    },
    {
      "text": "over. Okay. So",
      "start": 1715.399,
      "duration": 5.4,
      "language": "en"
    },
    {
      "text": "um once you get used to this, this is",
      "start": 1719.279,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "actually very very uh you know helpful.",
      "start": 1720.799,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "It may maybe looks if you're seeing this",
      "start": 1723.279,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "for the first time it might seem a bit",
      "start": 1725.12,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "you know strange and long but u trust me",
      "start": 1726.559,
      "duration": 3.441,
      "language": "en"
    },
    {
      "text": "once you get used to it'll be better",
      "start": 1728.64,
      "duration": 5.6,
      "language": "en"
    },
    {
      "text": "than um doing minus2 minus one. Um if",
      "start": 1730.0,
      "duration": 6.559,
      "language": "en"
    },
    {
      "text": "you're a little bit uh you know slicker",
      "start": 1734.24,
      "duration": 5.6,
      "language": "en"
    },
    {
      "text": "you can um use dot dot dot to represent",
      "start": 1736.559,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "broadcasting over any number of",
      "start": 1739.84,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "dimensions. So in this case um uh",
      "start": 1741.279,
      "duration": 5.201,
      "language": "en"
    },
    {
      "text": "instead of writing batch I can just",
      "start": 1744.88,
      "duration": 3.919,
      "language": "en"
    },
    {
      "text": "write dot dot dot and this would handle",
      "start": 1746.48,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "the case where instead of maybe batch I",
      "start": 1748.799,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "have batch one batch two or or some",
      "start": 1751.44,
      "duration": 6.4,
      "language": "en"
    },
    {
      "text": "other uh arbitrary long sequence.",
      "start": 1753.279,
      "duration": 7.841,
      "language": "en"
    },
    {
      "text": "Yeah question does for compile this like",
      "start": 1757.84,
      "duration": 6.28,
      "language": "en"
    },
    {
      "text": "is it guaranteed to compile to",
      "start": 1761.12,
      "duration": 6.4,
      "language": "en"
    },
    {
      "text": "like I guess so the question is is it",
      "start": 1764.12,
      "duration": 5.24,
      "language": "en"
    },
    {
      "text": "guaranteed to compile to something",
      "start": 1767.52,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "efficient? Um this uh I think the short",
      "start": 1769.36,
      "duration": 6.48,
      "language": "en"
    },
    {
      "text": "answer is yes. Um I don't know if you",
      "start": 1773.36,
      "duration": 4.799,
      "language": "en"
    },
    {
      "text": "have any you know nuances. We will",
      "start": 1775.84,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "figure out the best way to reduce the",
      "start": 1778.159,
      "duration": 3.921,
      "language": "en"
    },
    {
      "text": "best order of uh dimensions to reduce",
      "start": 1779.76,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "and then use that. If you're using it",
      "start": 1782.08,
      "duration": 3.199,
      "language": "en"
    },
    {
      "text": "within portra compile only do that one",
      "start": 1783.36,
      "duration": 3.439,
      "language": "en"
    },
    {
      "text": "time and then you know reuse the same",
      "start": 1785.279,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "implementation over and over again",
      "start": 1786.799,
      "duration": 6.561,
      "language": "en"
    },
    {
      "text": "better than anything designed by hand.",
      "start": 1789.039,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "Yeah. Okay.",
      "start": 1793.48,
      "duration": 7.24,
      "language": "en"
    },
    {
      "text": "So um so let's look at reduce. So reduce",
      "start": 1796.48,
      "duration": 7.36,
      "language": "en"
    },
    {
      "text": "operates on one tensor and it basically",
      "start": 1800.72,
      "duration": 5.6,
      "language": "en"
    },
    {
      "text": "aggregates some dimension or dimensions",
      "start": 1803.84,
      "duration": 4.719,
      "language": "en"
    },
    {
      "text": "of the tensor. So you have this tensor",
      "start": 1806.32,
      "duration": 4.719,
      "language": "en"
    },
    {
      "text": "uh before you would write mean to sum",
      "start": 1808.559,
      "duration": 6.081,
      "language": "en"
    },
    {
      "text": "over the the the final dimension and now",
      "start": 1811.039,
      "duration": 6.801,
      "language": "en"
    },
    {
      "text": "you basically say um actually okay so",
      "start": 1814.64,
      "duration": 6.56,
      "language": "en"
    },
    {
      "text": "this replace this with sum. So reduce",
      "start": 1817.84,
      "duration": 7.839,
      "language": "en"
    },
    {
      "text": "and um again you say hidden and hidden",
      "start": 1821.2,
      "duration": 6.719,
      "language": "en"
    },
    {
      "text": "is disappear disappeared. So which means",
      "start": 1825.679,
      "duration": 5.321,
      "language": "en"
    },
    {
      "text": "that you are um aggregating over that",
      "start": 1827.919,
      "duration": 5.601,
      "language": "en"
    },
    {
      "text": "dimension. Okay. So you can check that",
      "start": 1831.0,
      "duration": 7.0,
      "language": "en"
    },
    {
      "text": "this indeed kind of works over",
      "start": 1833.52,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "here. Okay. So, so maybe one final",
      "start": 1839.0,
      "duration": 8.679,
      "language": "en"
    },
    {
      "text": "example of uh this is um sometimes",
      "start": 1842.48,
      "duration": 7.76,
      "language": "en"
    },
    {
      "text": "um in a you know tensor one dimension",
      "start": 1847.679,
      "duration": 4.561,
      "language": "en"
    },
    {
      "text": "actually represents multiple dimensions",
      "start": 1850.24,
      "duration": 4.319,
      "language": "en"
    },
    {
      "text": "and you want to unpack that and operate",
      "start": 1852.24,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "over one of them and pack it back. So um",
      "start": 1854.559,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "in this case uh let's say you have batch",
      "start": 1857.76,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "uh sequence and then this",
      "start": 1860.399,
      "duration": 4.961,
      "language": "en"
    },
    {
      "text": "eightdimensional vector is actually a",
      "start": 1862.32,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "flattened representation of number of",
      "start": 1865.36,
      "duration": 4.52,
      "language": "en"
    },
    {
      "text": "heads times some you know hidden",
      "start": 1867.44,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "dimension. Okay so and then you have a",
      "start": 1869.88,
      "duration": 6.039,
      "language": "en"
    },
    {
      "text": "way a vector that is um needs to operate",
      "start": 1872.64,
      "duration": 5.759,
      "language": "en"
    },
    {
      "text": "on that hidden dimension. So you can do",
      "start": 1875.919,
      "duration": 6.801,
      "language": "en"
    },
    {
      "text": "this very elegantly uh using um inops uh",
      "start": 1878.399,
      "duration": 8.961,
      "language": "en"
    },
    {
      "text": "by um re calling rearrange and this",
      "start": 1882.72,
      "duration": 6.959,
      "language": "en"
    },
    {
      "text": "basically you can think about it we saw",
      "start": 1887.36,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "view uh before it's kind of like um kind",
      "start": 1889.679,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "of a you know fancier version which",
      "start": 1892.64,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "basically looks at the same data but uh",
      "start": 1894.799,
      "duration": 6.72,
      "language": "en"
    },
    {
      "text": "um you know differently. Um so here uh",
      "start": 1897.6,
      "duration": 6.079,
      "language": "en"
    },
    {
      "text": "it basically says this dimension is",
      "start": 1901.519,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "actually heads in hidden one. I'm going",
      "start": 1903.679,
      "duration": 6.081,
      "language": "en"
    },
    {
      "text": "to explode that into two dimensions. Um,",
      "start": 1905.519,
      "duration": 8.081,
      "language": "en"
    },
    {
      "text": "and uh, you have to specify the the",
      "start": 1909.76,
      "duration": 5.919,
      "language": "en"
    },
    {
      "text": "number of heads here because there's",
      "start": 1913.6,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "multiple ways to split a number into,",
      "start": 1915.679,
      "duration": 5.281,
      "language": "en"
    },
    {
      "text": "you know, two. Um, let's see. This might",
      "start": 1917.6,
      "duration": 5.679,
      "language": "en"
    },
    {
      "text": "be a little bit long. Um, okay, maybe",
      "start": 1920.96,
      "duration": 5.16,
      "language": "en"
    },
    {
      "text": "it's not worth looking at right now.",
      "start": 1923.279,
      "duration": 7.561,
      "language": "en"
    },
    {
      "text": "Um and given that x you can uh",
      "start": 1926.12,
      "duration": 8.64,
      "language": "en"
    },
    {
      "text": "perform your transformation using line",
      "start": 1930.84,
      "duration": 8.04,
      "language": "en"
    },
    {
      "text": "sum. So this is something hidden one",
      "start": 1934.76,
      "duration": 6.039,
      "language": "en"
    },
    {
      "text": "which corresponds to x and then hidden",
      "start": 1938.88,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "one hidden two which corresponds to w",
      "start": 1940.799,
      "duration": 5.88,
      "language": "en"
    },
    {
      "text": "and that gives you um something hidden",
      "start": 1943.44,
      "duration": 5.32,
      "language": "en"
    },
    {
      "text": "too.",
      "start": 1946.679,
      "duration": 5.641,
      "language": "en"
    },
    {
      "text": "Okay. Um and then you can rearrange",
      "start": 1948.76,
      "duration": 6.519,
      "language": "en"
    },
    {
      "text": "back. So this is the just the inverse of",
      "start": 1952.32,
      "duration": 5.359,
      "language": "en"
    },
    {
      "text": "breaking up. So you have the your two",
      "start": 1955.279,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "dimensions and you group it into one. So",
      "start": 1957.679,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "that's just a flattening operation",
      "start": 1960.159,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "that's uh um you know with uh everything",
      "start": 1961.919,
      "duration": 7.12,
      "language": "en"
    },
    {
      "text": "all the other dimensions kind of left",
      "start": 1965.519,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "alone. Okay. So there is a tutorial um",
      "start": 1969.96,
      "duration": 6.599,
      "language": "en"
    },
    {
      "text": "for for this that I would recommend you",
      "start": 1973.679,
      "duration": 5.201,
      "language": "en"
    },
    {
      "text": "go through and it gives you a bit more.",
      "start": 1976.559,
      "duration": 5.201,
      "language": "en"
    },
    {
      "text": "So um on you don't have to use this um",
      "start": 1978.88,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "because you're building it from scratch.",
      "start": 1981.76,
      "duration": 3.12,
      "language": "en"
    },
    {
      "text": "So you can kind of do anything you want",
      "start": 1983.12,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "but um in assignment one we do give you",
      "start": 1984.88,
      "duration": 4.799,
      "language": "en"
    },
    {
      "text": "guidance and it's something probably to",
      "start": 1987.44,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "invest",
      "start": 1989.679,
      "duration": 2.641,
      "language": "en"
    },
    {
      "text": "in.",
      "start": 1994.84,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "Okay.",
      "start": 1996.519,
      "duration": 6.16,
      "language": "en"
    },
    {
      "text": "Um so now let's talk about",
      "start": 1998.12,
      "duration": 8.679,
      "language": "en"
    },
    {
      "text": "uh computation uh no cost of t tensor",
      "start": 2002.679,
      "duration": 6.441,
      "language": "en"
    },
    {
      "text": "operations. So we introduce a bunch of",
      "start": 2006.799,
      "duration": 5.681,
      "language": "en"
    },
    {
      "text": "operations um and you know how much do",
      "start": 2009.12,
      "duration": 8.159,
      "language": "en"
    },
    {
      "text": "they cost? So a floatingoint operation",
      "start": 2012.48,
      "duration": 8.72,
      "language": "en"
    },
    {
      "text": "is uh any operation floating point like",
      "start": 2017.279,
      "duration": 6.321,
      "language": "en"
    },
    {
      "text": "addition or multiplication. These are",
      "start": 2021.2,
      "duration": 5.839,
      "language": "en"
    },
    {
      "text": "them and um these are kind of the you",
      "start": 2023.6,
      "duration": 6.16,
      "language": "en"
    },
    {
      "text": "know main ones that are going to um I",
      "start": 2027.039,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "think matter in terms of flop count. Um,",
      "start": 2029.76,
      "duration": 5.759,
      "language": "en"
    },
    {
      "text": "one thing that is uh is sort of a pet",
      "start": 2032.799,
      "duration": 4.401,
      "language": "en"
    },
    {
      "text": "peeve of mine is that when you say",
      "start": 2035.519,
      "duration": 3.681,
      "language": "en"
    },
    {
      "text": "flops, it's actually unclear what you",
      "start": 2037.2,
      "duration": 4.319,
      "language": "en"
    },
    {
      "text": "mean. So you could mean flops with a",
      "start": 2039.2,
      "duration": 6.16,
      "language": "en"
    },
    {
      "text": "lowercase s, which stands for number of",
      "start": 2041.519,
      "duration": 5.681,
      "language": "en"
    },
    {
      "text": "floating operations. This is measures",
      "start": 2045.36,
      "duration": 4.239,
      "language": "en"
    },
    {
      "text": "amount of computation that you've done",
      "start": 2047.2,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "or you could mean flops um also written",
      "start": 2049.599,
      "duration": 5.361,
      "language": "en"
    },
    {
      "text": "with a uppercase s which means floating",
      "start": 2053.04,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "points per second which is used to",
      "start": 2054.96,
      "duration": 5.439,
      "language": "en"
    },
    {
      "text": "measure the speed of hardware. So um",
      "start": 2057.44,
      "duration": 6.159,
      "language": "en"
    },
    {
      "text": "we're not going to in this class use",
      "start": 2060.399,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "uppercase s because I find that very",
      "start": 2063.599,
      "duration": 5.441,
      "language": "en"
    },
    {
      "text": "confusing and just write slash s to",
      "start": 2065.599,
      "duration": 5.721,
      "language": "en"
    },
    {
      "text": "denote that this is floating point per",
      "start": 2069.04,
      "duration": 4.52,
      "language": "en"
    },
    {
      "text": "second.",
      "start": 2071.32,
      "duration": 5.0,
      "language": "en"
    },
    {
      "text": "Okay. Okay. So just to give you uh some",
      "start": 2073.56,
      "duration": 5.96,
      "language": "en"
    },
    {
      "text": "intuition about flops. Uh GBD3 took",
      "start": 2076.32,
      "duration": 7.92,
      "language": "en"
    },
    {
      "text": "about uh 323 flops. GPD4 was 2 E25",
      "start": 2079.52,
      "duration": 9.28,
      "language": "en"
    },
    {
      "text": "flops. um speculation and there was a US",
      "start": 2084.24,
      "duration": 6.24,
      "language": "en"
    },
    {
      "text": "executive order that any foundation",
      "start": 2088.8,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "model with over 126 flops had to be",
      "start": 2090.48,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "reported to government um which now u",
      "start": 2093.2,
      "duration": 6.72,
      "language": "en"
    },
    {
      "text": "has been revoked um but the EU has still",
      "start": 2096.24,
      "duration": 6.48,
      "language": "en"
    },
    {
      "text": "they're going u still has a something",
      "start": 2099.92,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "that's hasn't the EU AI act which is",
      "start": 2102.72,
      "duration": 6.119,
      "language": "en"
    },
    {
      "text": "1e25 which hasn't been revoked",
      "start": 2105.2,
      "duration": 8.44,
      "language": "en"
    },
    {
      "text": "um so uh you know some intuitions uh",
      "start": 2108.839,
      "duration": 8.321,
      "language": "en"
    },
    {
      "text": "A100 has a peak performance of 312",
      "start": 2113.64,
      "duration": 6.479,
      "language": "en"
    },
    {
      "text": "teraflop per second.",
      "start": 2117.16,
      "duration": 8.36,
      "language": "en"
    },
    {
      "text": "Um and um H100 has a peak performance of",
      "start": 2120.119,
      "duration": 8.441,
      "language": "en"
    },
    {
      "text": "um 1979 teraflop per second with uh",
      "start": 2125.52,
      "duration": 6.559,
      "language": "en"
    },
    {
      "text": "sparsity and approximately 50% without",
      "start": 2128.56,
      "duration": 6.799,
      "language": "en"
    },
    {
      "text": "um and um if you look at uh you know the",
      "start": 2132.079,
      "duration": 6.721,
      "language": "en"
    },
    {
      "text": "Nvidia has these specification sheets um",
      "start": 2135.359,
      "duration": 6.48,
      "language": "en"
    },
    {
      "text": "so you can see that um the the flops",
      "start": 2138.8,
      "duration": 5.039,
      "language": "en"
    },
    {
      "text": "actually depends on what you're trying",
      "start": 2141.839,
      "duration": 6.961,
      "language": "en"
    },
    {
      "text": "to do. So if you're using BF or F FP32,",
      "start": 2143.839,
      "duration": 7.201,
      "language": "en"
    },
    {
      "text": "it's actually really really bad. Like",
      "start": 2148.8,
      "duration": 6.24,
      "language": "en"
    },
    {
      "text": "the if you run FP32 on H100, you're not",
      "start": 2151.04,
      "duration": 7.28,
      "language": "en"
    },
    {
      "text": "getting um it's it's orders of magnitude",
      "start": 2155.04,
      "duration": 7.92,
      "language": "en"
    },
    {
      "text": "worse than if you're doing um FP16 or um",
      "start": 2158.32,
      "duration": 7.519,
      "language": "en"
    },
    {
      "text": "and if you're willing to go down to FP8,",
      "start": 2162.96,
      "duration": 5.92,
      "language": "en"
    },
    {
      "text": "then it can be even faster. And you",
      "start": 2165.839,
      "duration": 4.401,
      "language": "en"
    },
    {
      "text": "know, for the for when I first read it,",
      "start": 2168.88,
      "duration": 2.4,
      "language": "en"
    },
    {
      "text": "I didn't realize, but there's an",
      "start": 2170.24,
      "duration": 2.48,
      "language": "en"
    },
    {
      "text": "asterisk here and this means with",
      "start": 2171.28,
      "duration": 4.799,
      "language": "en"
    },
    {
      "text": "sparsity. So uh usually you're in a lot",
      "start": 2172.72,
      "duration": 5.6,
      "language": "en"
    },
    {
      "text": "of the m matrices we have in this class",
      "start": 2176.079,
      "duration": 3.681,
      "language": "en"
    },
    {
      "text": "are dense. So you don't actually get",
      "start": 2178.32,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "this. You get something like uh you know",
      "start": 2179.76,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "it's exactly half that number. Exactly",
      "start": 2182.32,
      "duration": 4.799,
      "language": "en"
    },
    {
      "text": "half.",
      "start": 2184.56,
      "duration": 2.559,
      "language": "en"
    },
    {
      "text": "Okay. Okay.",
      "start": 2189.0,
      "duration": 3.64,
      "language": "en"
    },
    {
      "text": "So now you can do a bucket of envelope",
      "start": 2193.24,
      "duration": 4.839,
      "language": "en"
    },
    {
      "text": "of calculations. eight H100s for two",
      "start": 2195.599,
      "duration": 7.52,
      "language": "en"
    },
    {
      "text": "weeks is um just you know eight times",
      "start": 2198.079,
      "duration": 6.721,
      "language": "en"
    },
    {
      "text": "the number of flops per second times the",
      "start": 2203.119,
      "duration": 4.641,
      "language": "en"
    },
    {
      "text": "number of seconds in a in a in a week.",
      "start": 2204.8,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "Um actually this is this might be one",
      "start": 2207.76,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "week. Okay, so that's one week and",
      "start": 2210.24,
      "duration": 6.72,
      "language": "en"
    },
    {
      "text": "that's uh 4.7 * e to the 21",
      "start": 2212.72,
      "duration": 8.0,
      "language": "en"
    },
    {
      "text": "um which is you know some number and you",
      "start": 2216.96,
      "duration": 6.08,
      "language": "en"
    },
    {
      "text": "can kind of contextualize the the flop",
      "start": 2220.72,
      "duration": 6.56,
      "language": "en"
    },
    {
      "text": "counts with other um model counts.",
      "start": 2223.04,
      "duration": 7.36,
      "language": "en"
    },
    {
      "text": "Yeah. Sparity mean? So that means if so",
      "start": 2227.28,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "what does sparsity mean? That means if",
      "start": 2230.4,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "your matrices are uh sparse it's a",
      "start": 2232.0,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "specific like structured sparity. It's",
      "start": 2234.48,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "like two out of four elements in each",
      "start": 2236.4,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "like group of four elements is zero.",
      "start": 2238.8,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "That's the only case if you get that",
      "start": 2241.68,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "that speed. No one uses it.",
      "start": 2242.88,
      "duration": 8.4,
      "language": "en"
    },
    {
      "text": "Yeah, it's a marketing department uses",
      "start": 2246.48,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "it.",
      "start": 2251.8,
      "duration": 5.4,
      "language": "en"
    },
    {
      "text": "Okay. So, um let's go through a simple",
      "start": 2253.88,
      "duration": 4.92,
      "language": "en"
    },
    {
      "text": "example. So, remember we're not going to",
      "start": 2257.2,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "touch the transformer, but I think even",
      "start": 2258.8,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "a linear model gives us a lot of the",
      "start": 2260.8,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "building blocks and intuitions. So",
      "start": 2262.48,
      "duration": 5.56,
      "language": "en"
    },
    {
      "text": "suppose we have end points each point is",
      "start": 2264.64,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "d-dimensional and the linear model is",
      "start": 2268.04,
      "duration": 4.84,
      "language": "en"
    },
    {
      "text": "just going to match map each dimensional",
      "start": 2270.48,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "vector to a k dimensional uh vector.",
      "start": 2272.88,
      "duration": 6.16,
      "language": "en"
    },
    {
      "text": "Okay. So let's set some number of points",
      "start": 2275.68,
      "duration": 6.96,
      "language": "en"
    },
    {
      "text": "um is B, dimension is D, K is the number",
      "start": 2279.04,
      "duration": 7.6,
      "language": "en"
    },
    {
      "text": "of outputs. Um and let's create our data",
      "start": 2282.64,
      "duration": 10.8,
      "language": "en"
    },
    {
      "text": "matrix X um our weight matrix uh uh W",
      "start": 2286.64,
      "duration": 9.76,
      "language": "en"
    },
    {
      "text": "and the linear model is just some map.",
      "start": 2293.44,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "So nothing you know too too interesting",
      "start": 2296.4,
      "duration": 6.52,
      "language": "en"
    },
    {
      "text": "going on. And",
      "start": 2299.28,
      "duration": 7.36,
      "language": "en"
    },
    {
      "text": "um you know the question is how many",
      "start": 2302.92,
      "duration": 5.96,
      "language": "en"
    },
    {
      "text": "flops was that?",
      "start": 2306.64,
      "duration": 4.959,
      "language": "en"
    },
    {
      "text": "And uh the way you would you look at",
      "start": 2308.88,
      "duration": 7.12,
      "language": "en"
    },
    {
      "text": "this is you say well um when you do the",
      "start": 2311.599,
      "duration": 7.441,
      "language": "en"
    },
    {
      "text": "matrix multiplication you have basically",
      "start": 2316.0,
      "duration": 8.24,
      "language": "en"
    },
    {
      "text": "for every j k triple I have to multiply",
      "start": 2319.04,
      "duration": 6.52,
      "language": "en"
    },
    {
      "text": "two numbers",
      "start": 2324.24,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "together and I also have to add that",
      "start": 2325.56,
      "duration": 7.88,
      "language": "en"
    },
    {
      "text": "number to the the the total. Okay. So",
      "start": 2329.28,
      "duration": 7.24,
      "language": "en"
    },
    {
      "text": "the answer is 2 times",
      "start": 2333.44,
      "duration": 6.24,
      "language": "en"
    },
    {
      "text": "um the basically the product of all the",
      "start": 2336.52,
      "duration": 5.96,
      "language": "en"
    },
    {
      "text": "dimensions involved. So the the left",
      "start": 2339.68,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "dimension, the middle dimension and the",
      "start": 2342.48,
      "duration": 2.92,
      "language": "en"
    },
    {
      "text": "right",
      "start": 2343.839,
      "duration": 4.481,
      "language": "en"
    },
    {
      "text": "dimension. Okay, so this is something",
      "start": 2345.4,
      "duration": 5.32,
      "language": "en"
    },
    {
      "text": "that you should just kind of remember if",
      "start": 2348.32,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "you're doing a matrix multiplication.",
      "start": 2350.72,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "The number of flops is two times um the",
      "start": 2352.24,
      "duration": 7.24,
      "language": "en"
    },
    {
      "text": "product of the three dimensions.",
      "start": 2355.68,
      "duration": 3.8,
      "language": "en"
    },
    {
      "text": "Okay, so the flops of other operations",
      "start": 2361.04,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "are usually kind of linear in the size",
      "start": 2363.8,
      "duration": 7.88,
      "language": "en"
    },
    {
      "text": "of um the the matrix or tensor. Um and",
      "start": 2366.8,
      "duration": 6.4,
      "language": "en"
    },
    {
      "text": "in general, no other operation you",
      "start": 2371.68,
      "duration": 3.679,
      "language": "en"
    },
    {
      "text": "encounter in deep learning is expensive",
      "start": 2373.2,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "as matrix multiplication for large",
      "start": 2375.359,
      "duration": 5.601,
      "language": "en"
    },
    {
      "text": "enough uh matrices. So this is why I",
      "start": 2377.599,
      "duration": 5.201,
      "language": "en"
    },
    {
      "text": "think a lot of the napkin math is very",
      "start": 2380.96,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "simple because we're only looking at the",
      "start": 2382.8,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "matrix multiplications that are going",
      "start": 2385.2,
      "duration": 6.879,
      "language": "en"
    },
    {
      "text": "are performed by the model. Um now of",
      "start": 2388.16,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "course there are regimes where if your",
      "start": 2392.079,
      "duration": 4.401,
      "language": "en"
    },
    {
      "text": "matrices are um small enough then the",
      "start": 2393.44,
      "duration": 4.639,
      "language": "en"
    },
    {
      "text": "cost of other things starts to dominate",
      "start": 2396.48,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "but generally that's not a good regime",
      "start": 2398.079,
      "duration": 4.241,
      "language": "en"
    },
    {
      "text": "you want to be in because the hardware",
      "start": 2400.48,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "is designed for big much multiplication.",
      "start": 2402.32,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "So um sort of by it's a little bit",
      "start": 2405.04,
      "duration": 5.039,
      "language": "en"
    },
    {
      "text": "circular but by kind of we end up in",
      "start": 2408.0,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "this regime where we only consider uh",
      "start": 2410.079,
      "duration": 4.881,
      "language": "en"
    },
    {
      "text": "models where the mammals are the",
      "start": 2412.88,
      "duration": 5.88,
      "language": "en"
    },
    {
      "text": "dominant um uh you know",
      "start": 2414.96,
      "duration": 7.52,
      "language": "en"
    },
    {
      "text": "cost. Okay any questions about this this",
      "start": 2418.76,
      "duration": 6.44,
      "language": "en"
    },
    {
      "text": "number two times the product of the",
      "start": 2422.48,
      "duration": 4.879,
      "language": "en"
    },
    {
      "text": "three dimensions. This is just a useful",
      "start": 2425.2,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "thing. Would the algorithm of matrix",
      "start": 2427.359,
      "duration": 4.161,
      "language": "en"
    },
    {
      "text": "motivation always be the same? because",
      "start": 2429.52,
      "duration": 3.88,
      "language": "en"
    },
    {
      "text": "the chip might have optimized",
      "start": 2431.52,
      "duration": 7.76,
      "language": "en"
    },
    {
      "text": "that they always the same.",
      "start": 2433.4,
      "duration": 8.6,
      "language": "en"
    },
    {
      "text": "Yeah. So the question is like uh is the",
      "start": 2439.28,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "does this essentially does this depend",
      "start": 2442.0,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "on the the matrix multiplication you",
      "start": 2444.24,
      "duration": 7.52,
      "language": "en"
    },
    {
      "text": "know algorithm. Um in general I guess",
      "start": 2446.56,
      "duration": 7.92,
      "language": "en"
    },
    {
      "text": "we'll look at this the next week when we",
      "start": 2451.76,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "or the week after when we look at",
      "start": 2454.48,
      "duration": 3.119,
      "language": "en"
    },
    {
      "text": "kernels. I mean actually there's a lot",
      "start": 2456.0,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "of optimization that goes underneath",
      "start": 2457.599,
      "duration": 5.281,
      "language": "en"
    },
    {
      "text": "under the hood when it comes to matrix",
      "start": 2460.0,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "uh multiplications. Um and there's a lot",
      "start": 2462.88,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "of specialization depending on the",
      "start": 2465.52,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "shape. Um so this is I would say this is",
      "start": 2467.04,
      "duration": 6.24,
      "language": "en"
    },
    {
      "text": "just a kind of a crude you know estimate",
      "start": 2470.48,
      "duration": 7.28,
      "language": "en"
    },
    {
      "text": "um that is ba basically like the right",
      "start": 2473.28,
      "duration": 6.88,
      "language": "en"
    },
    {
      "text": "order of",
      "start": 2477.76,
      "duration": 2.4,
      "language": "en"
    },
    {
      "text": "magnitude. Okay. So um yeah uh additions",
      "start": 2481.96,
      "duration": 5.399,
      "language": "en"
    },
    {
      "text": "and multiplications are considered",
      "start": 2486.079,
      "duration": 3.361,
      "language": "en"
    },
    {
      "text": "equivalent. Yeah, additions and",
      "start": 2487.359,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "multiplications are considered are",
      "start": 2489.44,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "equivalent.",
      "start": 2491.119,
      "duration": 6.161,
      "language": "en"
    },
    {
      "text": "So um one way I find helpful to",
      "start": 2493.92,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "interpret this so at the end of the day",
      "start": 2497.28,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "this is just a matrix multiplication but",
      "start": 2498.8,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "I'm going to try to give a little bit of",
      "start": 2500.88,
      "duration": 5.199,
      "language": "en"
    },
    {
      "text": "meaning um to this um which is why I've",
      "start": 2502.56,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "set up this as kind of a little toy",
      "start": 2506.079,
      "duration": 5.441,
      "language": "en"
    },
    {
      "text": "machine learning problem. So B is really",
      "start": 2507.76,
      "duration": 6.64,
      "language": "en"
    },
    {
      "text": "stands for the number of data points and",
      "start": 2511.52,
      "duration": 6.319,
      "language": "en"
    },
    {
      "text": "DK is the number of parameters. So for",
      "start": 2514.4,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "this particular model the number of",
      "start": 2517.839,
      "duration": 3.681,
      "language": "en"
    },
    {
      "text": "flops that's required for forward pass",
      "start": 2519.44,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "is two times the number of tokens or",
      "start": 2521.52,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "number of data points times the number",
      "start": 2523.839,
      "duration": 2.201,
      "language": "en"
    },
    {
      "text": "of",
      "start": 2525.2,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "parameters. Okay. So this turns out to",
      "start": 2526.04,
      "duration": 5.48,
      "language": "en"
    },
    {
      "text": "actually generalize to transformers.",
      "start": 2529.52,
      "duration": 5.799,
      "language": "en"
    },
    {
      "text": "There's an asterisk there because uh",
      "start": 2531.52,
      "duration": 6.4,
      "language": "en"
    },
    {
      "text": "um there there's you know the sequence",
      "start": 2535.319,
      "duration": 5.641,
      "language": "en"
    },
    {
      "text": "length and other stuff but this is",
      "start": 2537.92,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "roughly right for uh if your sequent",
      "start": 2540.96,
      "duration": 3.96,
      "language": "en"
    },
    {
      "text": "length isn't isn't too",
      "start": 2543.28,
      "duration": 3.64,
      "language": "en"
    },
    {
      "text": "large.",
      "start": 2544.92,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "Um",
      "start": 2546.92,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "so okay so",
      "start": 2549.64,
      "duration": 5.479,
      "language": "en"
    },
    {
      "text": "now this is just a number of",
      "start": 2552.92,
      "duration": 6.12,
      "language": "en"
    },
    {
      "text": "floatingoint operations right um so how",
      "start": 2555.119,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "does this actually translate to a walk",
      "start": 2559.04,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "like time which is presumably the thing",
      "start": 2561.119,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "you actually care about how long do you",
      "start": 2563.28,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "have to wait for your run um so let's",
      "start": 2565.119,
      "duration": 7.921,
      "language": "en"
    },
    {
      "text": "time this so I have this function that",
      "start": 2569.04,
      "duration": 6.92,
      "language": "en"
    },
    {
      "text": "is just going to",
      "start": 2573.04,
      "duration": 5.16,
      "language": "en"
    },
    {
      "text": "um",
      "start": 2575.96,
      "duration": 5.399,
      "language": "en"
    },
    {
      "text": "um do it five times and I'm going to",
      "start": 2578.2,
      "duration": 6.28,
      "language": "en"
    },
    {
      "text": "perform the matrix multiply operation.",
      "start": 2581.359,
      "duration": 5.201,
      "language": "en"
    },
    {
      "text": "Um we'll talk a little bit later about",
      "start": 2584.48,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "this um um two weeks from now why the",
      "start": 2586.56,
      "duration": 7.44,
      "language": "en"
    },
    {
      "text": "other code is here. Um but um for now we",
      "start": 2590.16,
      "duration": 7.12,
      "language": "en"
    },
    {
      "text": "get an actual time. So that matrix uh",
      "start": 2594.0,
      "duration": 6.8,
      "language": "en"
    },
    {
      "text": "took you know 0.16 seconds",
      "start": 2597.28,
      "duration": 7.44,
      "language": "en"
    },
    {
      "text": "um and the actual flops per second which",
      "start": 2600.8,
      "duration": 6.4,
      "language": "en"
    },
    {
      "text": "is how many flops did it do per second",
      "start": 2604.72,
      "duration": 6.2,
      "language": "en"
    },
    {
      "text": "is uh 5.4",
      "start": 2607.2,
      "duration": 7.879,
      "language": "en"
    },
    {
      "text": "E13. Okay. So now you can compare this",
      "start": 2610.92,
      "duration": 6.76,
      "language": "en"
    },
    {
      "text": "with you know the marketing materials",
      "start": 2615.079,
      "duration": 6.801,
      "language": "en"
    },
    {
      "text": "and um for the A100 and H100",
      "start": 2617.68,
      "duration": 7.6,
      "language": "en"
    },
    {
      "text": "um and you know as we look at the spec",
      "start": 2621.88,
      "duration": 6.36,
      "language": "en"
    },
    {
      "text": "sheet the the flops depends on the data",
      "start": 2625.28,
      "duration": 7.2,
      "language": "en"
    },
    {
      "text": "type and we see that the the promise",
      "start": 2628.24,
      "duration": 8.44,
      "language": "en"
    },
    {
      "text": "flops per second which um you know for",
      "start": 2632.48,
      "duration": 10.16,
      "language": "en"
    },
    {
      "text": "H100 for I guess this is for float 32 is",
      "start": 2636.68,
      "duration": 9.72,
      "language": "en"
    },
    {
      "text": "you know 67 you know teraflops as we",
      "start": 2642.64,
      "duration": 5.24,
      "language": "en"
    },
    {
      "text": "looked",
      "start": 2646.4,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "and so that is the number of promise",
      "start": 2647.88,
      "duration": 9.0,
      "language": "en"
    },
    {
      "text": "flops per second we we had um and now if",
      "start": 2650.88,
      "duration": 10.0,
      "language": "en"
    },
    {
      "text": "you look at the uh there's a helpful",
      "start": 2656.88,
      "duration": 6.92,
      "language": "en"
    },
    {
      "text": "notion called model flops utilization or",
      "start": 2660.88,
      "duration": 5.6,
      "language": "en"
    },
    {
      "text": "MFU which is the actual number of flops",
      "start": 2663.8,
      "duration": 6.36,
      "language": "en"
    },
    {
      "text": "divided by the promise flops okay so you",
      "start": 2666.48,
      "duration": 5.359,
      "language": "en"
    },
    {
      "text": "take the actual number of flops remember",
      "start": 2670.16,
      "duration": 2.56,
      "language": "en"
    },
    {
      "text": "which",
      "start": 2671.839,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "what you actually uh you know witnessed",
      "start": 2672.72,
      "duration": 4.879,
      "language": "en"
    },
    {
      "text": "the number of floatingoint operations",
      "start": 2675.599,
      "duration": 6.961,
      "language": "en"
    },
    {
      "text": "that that are useful for your model um",
      "start": 2677.599,
      "duration": 7.361,
      "language": "en"
    },
    {
      "text": "uh divided by the actual time it took",
      "start": 2682.56,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "divided by this promise flops per second",
      "start": 2684.96,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "which is you know from the glossy",
      "start": 2686.96,
      "duration": 7.32,
      "language": "en"
    },
    {
      "text": "brochure um you can get a MFU of uh 0.8",
      "start": 2688.8,
      "duration": 7.4,
      "language": "en"
    },
    {
      "text": "eight.",
      "start": 2694.28,
      "duration": 5.079,
      "language": "en"
    },
    {
      "text": "Okay. So, usually you see people talking",
      "start": 2696.2,
      "duration": 6.52,
      "language": "en"
    },
    {
      "text": "about their MFUs and um something",
      "start": 2699.359,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "greater than 0.5 is you usually",
      "start": 2702.72,
      "duration": 5.119,
      "language": "en"
    },
    {
      "text": "considered to be good and if you're like",
      "start": 2705.119,
      "duration": 5.281,
      "language": "en"
    },
    {
      "text": "you know 5% MFU that's uh considered to",
      "start": 2707.839,
      "duration": 4.721,
      "language": "en"
    },
    {
      "text": "be really bad. usually can't get, you",
      "start": 2710.4,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "know, close to that close to, you know,",
      "start": 2712.56,
      "duration": 7.36,
      "language": "en"
    },
    {
      "text": "uh, you know, 90 or 100 um because this",
      "start": 2715.68,
      "duration": 5.919,
      "language": "en"
    },
    {
      "text": "is sort of ignoring all sort of",
      "start": 2719.92,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "communication and overhead. It's just",
      "start": 2721.599,
      "duration": 5.961,
      "language": "en"
    },
    {
      "text": "like the literal uh computation of the",
      "start": 2723.44,
      "duration": 7.04,
      "language": "en"
    },
    {
      "text": "flops. Okay. And usually MFU is much",
      "start": 2727.56,
      "duration": 4.12,
      "language": "en"
    },
    {
      "text": "higher if the magic matrix",
      "start": 2730.48,
      "duration": 5.119,
      "language": "en"
    },
    {
      "text": "multiplications dominate. Um Um, okay.",
      "start": 2731.68,
      "duration": 6.96,
      "language": "en"
    },
    {
      "text": "So that's MFU. Any any questions about",
      "start": 2735.599,
      "duration": 6.321,
      "language": "en"
    },
    {
      "text": "uh this? Yeah. Uh you're using the",
      "start": 2738.64,
      "duration": 6.4,
      "language": "en"
    },
    {
      "text": "promise flop per sec uh not considering",
      "start": 2741.92,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "the sparse.",
      "start": 2745.04,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "So this promised flop per sec is uh not",
      "start": 2746.72,
      "duration": 8.0,
      "language": "en"
    },
    {
      "text": "considering this as much. Yeah.",
      "start": 2750.8,
      "duration": 7.039,
      "language": "en"
    },
    {
      "text": "Um one a note is like uh this is",
      "start": 2754.72,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "actually a you know there's also",
      "start": 2757.839,
      "duration": 5.0,
      "language": "en"
    },
    {
      "text": "something called hardware you know flops",
      "start": 2760.0,
      "duration": 7.68,
      "language": "en"
    },
    {
      "text": "uization and the the inov the motivation",
      "start": 2762.839,
      "duration": 8.041,
      "language": "en"
    },
    {
      "text": "here is that we are all we're trying to",
      "start": 2767.68,
      "duration": 5.32,
      "language": "en"
    },
    {
      "text": "look at",
      "start": 2770.88,
      "duration": 4.959,
      "language": "en"
    },
    {
      "text": "the it's it's called model because we're",
      "start": 2773.0,
      "duration": 4.839,
      "language": "en"
    },
    {
      "text": "looking at the number of effective",
      "start": 2775.839,
      "duration": 4.561,
      "language": "en"
    },
    {
      "text": "useful operations that the model is you",
      "start": 2777.839,
      "duration": 6.161,
      "language": "en"
    },
    {
      "text": "know performing okay And uh so it's a",
      "start": 2780.4,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "way of kind of standardizing. It's not",
      "start": 2784.0,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "the actual number of flops that are are",
      "start": 2785.92,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "done because you could have optimization",
      "start": 2789.44,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "in your code that cache a few things or",
      "start": 2791.76,
      "duration": 5.839,
      "language": "en"
    },
    {
      "text": "redo um you know um recomputation of",
      "start": 2794.4,
      "duration": 6.48,
      "language": "en"
    },
    {
      "text": "some things and in some sense you're",
      "start": 2797.599,
      "duration": 5.121,
      "language": "en"
    },
    {
      "text": "still computing the same model. So what",
      "start": 2800.88,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "matters is that you're this is sort of",
      "start": 2802.72,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "trying to look at the model complexity",
      "start": 2804.88,
      "duration": 3.199,
      "language": "en"
    },
    {
      "text": "and you shouldn't be penalized just",
      "start": 2806.56,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "because you were clever in your MFU if",
      "start": 2808.079,
      "duration": 4.641,
      "language": "en"
    },
    {
      "text": "you were clever and you didn't actually",
      "start": 2810.88,
      "duration": 5.56,
      "language": "en"
    },
    {
      "text": "do the flops um but you you said you",
      "start": 2812.72,
      "duration": 6.48,
      "language": "en"
    },
    {
      "text": "did. Okay. So you can also do the same",
      "start": 2816.44,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "with",
      "start": 2819.2,
      "duration": 2.919,
      "language": "en"
    },
    {
      "text": "BF16.",
      "start": 2820.599,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "Um Oops.",
      "start": 2822.119,
      "duration": 9.561,
      "language": "en"
    },
    {
      "text": "And here we see that for uh BF the um",
      "start": 2824.839,
      "duration": 10.28,
      "language": "en"
    },
    {
      "text": "the time is actually much uh better",
      "start": 2831.68,
      "duration": 6.76,
      "language": "en"
    },
    {
      "text": "right so 03 um instead",
      "start": 2835.119,
      "duration": 6.321,
      "language": "en"
    },
    {
      "text": "of.16 so the actual flops per second is",
      "start": 2838.44,
      "duration": 7.159,
      "language": "en"
    },
    {
      "text": "higher um the even accounting for",
      "start": 2841.44,
      "duration": 7.2,
      "language": "en"
    },
    {
      "text": "sparsity the the promise flops is uh",
      "start": 2845.599,
      "duration": 6.081,
      "language": "en"
    },
    {
      "text": "still quite high so the MFU actually um",
      "start": 2848.64,
      "duration": 6.6,
      "language": "en"
    },
    {
      "text": "lower for BF uh",
      "start": 2851.68,
      "duration": 6.6,
      "language": "en"
    },
    {
      "text": "16. Um this is you know maybe",
      "start": 2855.24,
      "duration": 6.52,
      "language": "en"
    },
    {
      "text": "surprisingly low but",
      "start": 2858.28,
      "duration": 5.64,
      "language": "en"
    },
    {
      "text": "um but sometimes the promise flops is a",
      "start": 2861.76,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "bit of a you know",
      "start": 2863.92,
      "duration": 2.96,
      "language": "en"
    },
    {
      "text": "optimistic. So always benchmark your",
      "start": 2867.4,
      "duration": 5.32,
      "language": "en"
    },
    {
      "text": "code uh and don't just uh kind of assume",
      "start": 2869.92,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "that you're going to get certain um",
      "start": 2872.72,
      "duration": 2.599,
      "language": "en"
    },
    {
      "text": "levels of",
      "start": 2874.24,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "performance. Okay. So uh just to",
      "start": 2875.319,
      "duration": 7.321,
      "language": "en"
    },
    {
      "text": "summarize um matrix multiplications u",
      "start": 2878.88,
      "duration": 7.439,
      "language": "en"
    },
    {
      "text": "dominate the the compute and the general",
      "start": 2882.64,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "rule of the thumb is that it's two times",
      "start": 2886.319,
      "duration": 5.241,
      "language": "en"
    },
    {
      "text": "the product of the dimensions",
      "start": 2888.64,
      "duration": 5.08,
      "language": "en"
    },
    {
      "text": "flops. Um",
      "start": 2891.56,
      "duration": 6.039,
      "language": "en"
    },
    {
      "text": "the flops per second floating points per",
      "start": 2893.72,
      "duration": 7.399,
      "language": "en"
    },
    {
      "text": "second depends on you know the hardware",
      "start": 2897.599,
      "duration": 6.161,
      "language": "en"
    },
    {
      "text": "and also the data type. So the fancier",
      "start": 2901.119,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "the hardware you have, the higher it is,",
      "start": 2903.76,
      "duration": 5.359,
      "language": "en"
    },
    {
      "text": "the the smaller the data type, the",
      "start": 2906.319,
      "duration": 7.52,
      "language": "en"
    },
    {
      "text": "usually the faster it is. Um and uh MFU",
      "start": 2909.119,
      "duration": 8.561,
      "language": "en"
    },
    {
      "text": "is a useful notion to look at how well",
      "start": 2913.839,
      "duration": 6.561,
      "language": "en"
    },
    {
      "text": "you've you're essentially squeezing you",
      "start": 2917.68,
      "duration": 6.639,
      "language": "en"
    },
    {
      "text": "know your your hardware.",
      "start": 2920.4,
      "duration": 6.64,
      "language": "en"
    },
    {
      "text": "Yeah, I I've heard that often uh in",
      "start": 2924.319,
      "duration": 3.841,
      "language": "en"
    },
    {
      "text": "order to get like the maximum",
      "start": 2927.04,
      "duration": 2.559,
      "language": "en"
    },
    {
      "text": "utilization, you want to use these like",
      "start": 2928.16,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "tensor cores on the machine. And so like",
      "start": 2929.599,
      "duration": 5.041,
      "language": "en"
    },
    {
      "text": "does PyTorch by default use these tensor",
      "start": 2932.64,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "cores and like are these uh accounting",
      "start": 2934.64,
      "duration": 5.919,
      "language": "en"
    },
    {
      "text": "for that? Yeah. So the question is uh",
      "start": 2938.16,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "what about those tensor cores? So if you",
      "start": 2940.559,
      "duration": 7.76,
      "language": "en"
    },
    {
      "text": "go to this um spec sheet um you'll see",
      "start": 2942.88,
      "duration": 7.16,
      "language": "en"
    },
    {
      "text": "that",
      "start": 2948.319,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "um you know these are all on the tensor",
      "start": 2950.04,
      "duration": 5.72,
      "language": "en"
    },
    {
      "text": "core. So the tensor core is basically",
      "start": 2953.359,
      "duration": 4.401,
      "language": "en"
    },
    {
      "text": "you know you know specialized hardware",
      "start": 2955.76,
      "duration": 8.12,
      "language": "en"
    },
    {
      "text": "to do um map models. Um so if you are",
      "start": 2957.76,
      "duration": 10.16,
      "language": "en"
    },
    {
      "text": "um you know if you're so by default it",
      "start": 2963.88,
      "duration": 5.959,
      "language": "en"
    },
    {
      "text": "should use it and if you especially if",
      "start": 2967.92,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "you're using PyTorch you know compile it",
      "start": 2969.839,
      "duration": 5.121,
      "language": "en"
    },
    {
      "text": "will generate the the code that will um",
      "start": 2971.92,
      "duration": 6.56,
      "language": "en"
    },
    {
      "text": "use the the hardware",
      "start": 2974.96,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "properly. Okay. So um let's talk a",
      "start": 2983.24,
      "duration": 6.52,
      "language": "en"
    },
    {
      "text": "little about you know gradients. So, and",
      "start": 2986.88,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "the the reason is that we've only looked",
      "start": 2989.76,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "at matrix multiplication or in other",
      "start": 2992.0,
      "duration": 5.839,
      "language": "en"
    },
    {
      "text": "words basically feed forward um forward",
      "start": 2995.04,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "passes and the number of flops. Um but",
      "start": 2997.839,
      "duration": 3.841,
      "language": "en"
    },
    {
      "text": "there's also a computation that comes",
      "start": 3000.16,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "from computing gradients and we want to",
      "start": 3001.68,
      "duration": 6.919,
      "language": "en"
    },
    {
      "text": "track down how much uh that is. Um",
      "start": 3003.52,
      "duration": 8.36,
      "language": "en"
    },
    {
      "text": "okay so just to consider a simple",
      "start": 3008.599,
      "duration": 7.081,
      "language": "en"
    },
    {
      "text": "example simple linear model um where you",
      "start": 3011.88,
      "duration": 6.12,
      "language": "en"
    },
    {
      "text": "take the uh the prediction of a linear",
      "start": 3015.68,
      "duration": 6.639,
      "language": "en"
    },
    {
      "text": "model and you um you look at the MSE",
      "start": 3018.0,
      "duration": 6.88,
      "language": "en"
    },
    {
      "text": "with respect to five. though not a very",
      "start": 3022.319,
      "duration": 4.561,
      "language": "en"
    },
    {
      "text": "interesting loss but I think it'll it's",
      "start": 3024.88,
      "duration": 5.56,
      "language": "en"
    },
    {
      "text": "illustrative for uh looking at the the",
      "start": 3026.88,
      "duration": 5.6,
      "language": "en"
    },
    {
      "text": "gradients. Okay. So remember in the",
      "start": 3030.44,
      "duration": 4.04,
      "language": "en"
    },
    {
      "text": "forward pass you have your x you have",
      "start": 3032.48,
      "duration": 6.56,
      "language": "en"
    },
    {
      "text": "your your w which um you want to compute",
      "start": 3034.48,
      "duration": 6.8,
      "language": "en"
    },
    {
      "text": "the gradient with respect to you make a",
      "start": 3039.04,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "prediction um by taking a linear product",
      "start": 3041.28,
      "duration": 8.4,
      "language": "en"
    },
    {
      "text": "and then you you have your loss. Okay.",
      "start": 3044.88,
      "duration": 7.199,
      "language": "en"
    },
    {
      "text": "And in the backward pass uh you just",
      "start": 3049.68,
      "duration": 7.12,
      "language": "en"
    },
    {
      "text": "call loss backwards. And um in this case",
      "start": 3052.079,
      "duration": 7.201,
      "language": "en"
    },
    {
      "text": "the the gradient which is this variable",
      "start": 3056.8,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "attached to the tensor is um turns out",
      "start": 3059.28,
      "duration": 5.64,
      "language": "en"
    },
    {
      "text": "to be what you what you",
      "start": 3062.8,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "want. Okay. So everyone has done uh you",
      "start": 3064.92,
      "duration": 7.28,
      "language": "en"
    },
    {
      "text": "know gradients in PyTorch before.",
      "start": 3068.16,
      "duration": 8.159,
      "language": "en"
    },
    {
      "text": "Um so let's look at how many flops uh",
      "start": 3072.2,
      "duration": 7.76,
      "language": "en"
    },
    {
      "text": "are required for computing gradients.",
      "start": 3076.319,
      "duration": 7.201,
      "language": "en"
    },
    {
      "text": "Um, okay.",
      "start": 3079.96,
      "duration": 3.56,
      "language": "en"
    },
    {
      "text": "So, let's uh look at a slightly more",
      "start": 3084.44,
      "duration": 8.52,
      "language": "en"
    },
    {
      "text": "complicated model. So, now it's uh a",
      "start": 3088.4,
      "duration": 8.24,
      "language": "en"
    },
    {
      "text": "two-layer linear model where you have X",
      "start": 3092.96,
      "duration": 9.359,
      "language": "en"
    },
    {
      "text": "which is B by D times um W1 which is D",
      "start": 3096.64,
      "duration": 9.12,
      "language": "en"
    },
    {
      "text": "by D. So that's the first layer and then",
      "start": 3102.319,
      "duration": 6.321,
      "language": "en"
    },
    {
      "text": "you take the your hidden activations um",
      "start": 3105.76,
      "duration": 5.839,
      "language": "en"
    },
    {
      "text": "H1 and you pass it through another",
      "start": 3108.64,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "linear layer W2 and to get a K",
      "start": 3111.599,
      "duration": 5.201,
      "language": "en"
    },
    {
      "text": "dimensional vector and you do some",
      "start": 3114.4,
      "duration": 4.959,
      "language": "en"
    },
    {
      "text": "compute some",
      "start": 3116.8,
      "duration": 2.559,
      "language": "en"
    },
    {
      "text": "loss. Okay, so this is a two-layer",
      "start": 3119.8,
      "duration": 5.48,
      "language": "en"
    },
    {
      "text": "linear network. And just as a kind of",
      "start": 3122.319,
      "duration": 4.561,
      "language": "en"
    },
    {
      "text": "review, if you look at the number of",
      "start": 3125.28,
      "duration": 5.92,
      "language": "en"
    },
    {
      "text": "forward flops, um what you had to do was",
      "start": 3126.88,
      "duration": 8.32,
      "language": "en"
    },
    {
      "text": "um you have to multiply look at W1. You",
      "start": 3131.2,
      "duration": 7.119,
      "language": "en"
    },
    {
      "text": "have to multiply X by W1",
      "start": 3135.2,
      "duration": 6.2,
      "language": "en"
    },
    {
      "text": "um and add it to your",
      "start": 3138.319,
      "duration": 6.881,
      "language": "en"
    },
    {
      "text": "H1 and you have to take H1 and W2 and",
      "start": 3141.4,
      "duration": 8.12,
      "language": "en"
    },
    {
      "text": "you have to add it to your your H2.",
      "start": 3145.2,
      "duration": 6.48,
      "language": "en"
    },
    {
      "text": "Okay, so the no total number of flops",
      "start": 3149.52,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "again is 2 * the the product of all the",
      "start": 3151.68,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "dimensions in your map mall plus two",
      "start": 3154.96,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "times the product dimensions your m all",
      "start": 3157.52,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "for the second matrix. Okay, in other",
      "start": 3159.76,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "words, two times the total number of",
      "start": 3162.16,
      "duration": 3.72,
      "language": "en"
    },
    {
      "text": "parameters in this",
      "start": 3163.92,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "case. Okay, so so what about the",
      "start": 3165.88,
      "duration": 6.52,
      "language": "en"
    },
    {
      "text": "backward pass? So this part will be um a",
      "start": 3169.2,
      "duration": 4.119,
      "language": "en"
    },
    {
      "text": "little bit more",
      "start": 3172.4,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "involved. Um so we can recall the model",
      "start": 3173.319,
      "duration": 8.04,
      "language": "en"
    },
    {
      "text": "uh x to h1 to h2 and the loss. So in the",
      "start": 3177.28,
      "duration": 5.839,
      "language": "en"
    },
    {
      "text": "backward path you have to compute a",
      "start": 3181.359,
      "duration": 4.561,
      "language": "en"
    },
    {
      "text": "bunch of gradients and the gradients",
      "start": 3183.119,
      "duration": 5.841,
      "language": "en"
    },
    {
      "text": "that are relevant is you have to compute",
      "start": 3185.92,
      "duration": 8.159,
      "language": "en"
    },
    {
      "text": "uh the gradient with respect to um h1",
      "start": 3188.96,
      "duration": 10.68,
      "language": "en"
    },
    {
      "text": "um you know h2 w1 and",
      "start": 3194.079,
      "duration": 9.441,
      "language": "en"
    },
    {
      "text": "w2 of of the loss. So D loss D each of",
      "start": 3199.64,
      "duration": 6.12,
      "language": "en"
    },
    {
      "text": "these variables. Okay. So how long does",
      "start": 3203.52,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "it take to compute that? Let's just look",
      "start": 3205.76,
      "duration": 7.52,
      "language": "en"
    },
    {
      "text": "at W2 for now. Okay. So the things that",
      "start": 3208.4,
      "duration": 7.84,
      "language": "en"
    },
    {
      "text": "touch W2 um you can compute by looking",
      "start": 3213.28,
      "duration": 7.96,
      "language": "en"
    },
    {
      "text": "at the the chain rule. Um so",
      "start": 3216.24,
      "duration": 10.44,
      "language": "en"
    },
    {
      "text": "W2 grad. So the gradient with of d loss",
      "start": 3221.24,
      "duration": 14.24,
      "language": "en"
    },
    {
      "text": "dw2 is um you you sum um h1 times uh the",
      "start": 3226.68,
      "duration": 13.6,
      "language": "en"
    },
    {
      "text": "gradient of the loss with respect to h2.",
      "start": 3235.48,
      "duration": 8.04,
      "language": "en"
    },
    {
      "text": "Okay. So that's just a chain rule for um",
      "start": 3240.28,
      "duration": 8.76,
      "language": "en"
    },
    {
      "text": "for w2. And this is um so all the",
      "start": 3243.52,
      "duration": 7.52,
      "language": "en"
    },
    {
      "text": "gradients are the you know the same size",
      "start": 3249.04,
      "duration": 5.96,
      "language": "en"
    },
    {
      "text": "as the underlying you know of vectors.",
      "start": 3251.04,
      "duration": 5.72,
      "language": "en"
    },
    {
      "text": "Um",
      "start": 3255.0,
      "duration": 6.44,
      "language": "en"
    },
    {
      "text": "so this turns out to be um essentially",
      "start": 3256.76,
      "duration": 6.799,
      "language": "en"
    },
    {
      "text": "looks like a matrix uh you know",
      "start": 3261.44,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "multiplication and so the same you know",
      "start": 3263.559,
      "duration": 5.481,
      "language": "en"
    },
    {
      "text": "calculus holds which is that it's 2",
      "start": 3266.64,
      "duration": 6.16,
      "language": "en"
    },
    {
      "text": "times the number of uh the product of",
      "start": 3269.04,
      "duration": 7.559,
      "language": "en"
    },
    {
      "text": "all the dimensions B * D *",
      "start": 3272.8,
      "duration": 7.2,
      "language": "en"
    },
    {
      "text": "K okay but this is only the gradient",
      "start": 3276.599,
      "duration": 6.24,
      "language": "en"
    },
    {
      "text": "with respect to uh",
      "start": 3280.0,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "W2 We also need to compute the gradient",
      "start": 3282.839,
      "duration": 5.881,
      "language": "en"
    },
    {
      "text": "with respect to h1 because we have to",
      "start": 3285.76,
      "duration": 6.079,
      "language": "en"
    },
    {
      "text": "keep on back propagating um to w1 and",
      "start": 3288.72,
      "duration": 8.28,
      "language": "en"
    },
    {
      "text": "and so on. Okay. So that is uh going to",
      "start": 3291.839,
      "duration": 11.321,
      "language": "en"
    },
    {
      "text": "be um you know the product of w2",
      "start": 3297.0,
      "duration": 9.839,
      "language": "en"
    },
    {
      "text": "um you know times uh you know",
      "start": 3303.16,
      "duration": 5.959,
      "language": "en"
    },
    {
      "text": "h2 sorry I think this should be that",
      "start": 3306.839,
      "duration": 6.561,
      "language": "en"
    },
    {
      "text": "grad of h2 h2.grad red.",
      "start": 3309.119,
      "duration": 7.881,
      "language": "en"
    },
    {
      "text": "Um so that turns out to also be",
      "start": 3313.4,
      "duration": 7.08,
      "language": "en"
    },
    {
      "text": "essentially you know um looks like the",
      "start": 3317.0,
      "duration": 7.559,
      "language": "en"
    },
    {
      "text": "matrix multiplication and it's the same",
      "start": 3320.48,
      "duration": 6.72,
      "language": "en"
    },
    {
      "text": "number of flops for for computing the",
      "start": 3324.559,
      "duration": 5.321,
      "language": "en"
    },
    {
      "text": "gradient of uh each",
      "start": 3327.2,
      "duration": 6.24,
      "language": "en"
    },
    {
      "text": "one. Okay. So when you add the two uh so",
      "start": 3329.88,
      "duration": 7.4,
      "language": "en"
    },
    {
      "text": "that's just for w2. Um you do the same",
      "start": 3333.44,
      "duration": 6.8,
      "language": "en"
    },
    {
      "text": "thing for W1 and that's uh which has D *",
      "start": 3337.28,
      "duration": 7.079,
      "language": "en"
    },
    {
      "text": "D parameters and when you add it all uh",
      "start": 3340.24,
      "duration": 10.0,
      "language": "en"
    },
    {
      "text": "up it's uh so for this um for W2 the",
      "start": 3344.359,
      "duration": 10.76,
      "language": "en"
    },
    {
      "text": "amount of computation was 4 * B * D * K",
      "start": 3350.24,
      "duration": 11.92,
      "language": "en"
    },
    {
      "text": "and for W1 it's also 4 * B * D * uh D",
      "start": 3355.119,
      "duration": 10.321,
      "language": "en"
    },
    {
      "text": "because W1 is D by",
      "start": 3362.16,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "Okay. So, um I know there's a lot of",
      "start": 3369.0,
      "duration": 6.44,
      "language": "en"
    },
    {
      "text": "symbols here. I'm going to try also to",
      "start": 3372.24,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "uh give you a visual account for this.",
      "start": 3375.44,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "So, this is from a a blog post that I",
      "start": 3377.68,
      "duration": 6.32,
      "language": "en"
    },
    {
      "text": "think um may work uh better. We'll see.",
      "start": 3379.92,
      "duration": 5.679,
      "language": "en"
    },
    {
      "text": "Um okay, I have to wait for the",
      "start": 3384.0,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "animation to loop back. So, so basically",
      "start": 3385.599,
      "duration": 4.641,
      "language": "en"
    },
    {
      "text": "this is uh one layer of the neuron now.",
      "start": 3388.16,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "It has um you know the hiddens and then",
      "start": 3390.24,
      "duration": 6.4,
      "language": "en"
    },
    {
      "text": "the weights to the next layer. And so I",
      "start": 3393.28,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "have to Okay, the problem with this",
      "start": 3396.64,
      "duration": 4.36,
      "language": "en"
    },
    {
      "text": "animation is I have to",
      "start": 3398.64,
      "duration": 6.56,
      "language": "en"
    },
    {
      "text": "wait. Okay, ready set. Okay, so first I",
      "start": 3401.0,
      "duration": 7.96,
      "language": "en"
    },
    {
      "text": "have to multiply W and A and I have to",
      "start": 3405.2,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "add it to this. That's a forward pass.",
      "start": 3408.96,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "And now I'm going to multiply this these",
      "start": 3410.72,
      "duration": 4.639,
      "language": "en"
    },
    {
      "text": "two and then add it to to that. And I'm",
      "start": 3412.4,
      "duration": 4.959,
      "language": "en"
    },
    {
      "text": "going to multiply and then add it to",
      "start": 3415.359,
      "duration": 4.841,
      "language": "en"
    },
    {
      "text": "that. Okay.",
      "start": 3417.359,
      "duration": 5.081,
      "language": "en"
    },
    {
      "text": "Any",
      "start": 3420.2,
      "duration": 4.28,
      "language": "en"
    },
    {
      "text": "questions? Which a way to slow this",
      "start": 3422.44,
      "duration": 6.919,
      "language": "en"
    },
    {
      "text": "down? Um, but you know, the details",
      "start": 3424.48,
      "duration": 6.96,
      "language": "en"
    },
    {
      "text": "maybe I'll I'll let you kind of ruminate",
      "start": 3429.359,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "on, but um the the high level is that",
      "start": 3431.44,
      "duration": 3.919,
      "language": "en"
    },
    {
      "text": "there's two times the number of",
      "start": 3433.839,
      "duration": 3.921,
      "language": "en"
    },
    {
      "text": "parameters for the forward pass and four",
      "start": 3435.359,
      "duration": 3.681,
      "language": "en"
    },
    {
      "text": "times the number of parameters for the",
      "start": 3437.76,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "backward pass. And um we can just kind",
      "start": 3439.04,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "of work it out uh via the chain row",
      "start": 3442.24,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "here. Yeah. For the homeworks, are we",
      "start": 3444.0,
      "duration": 5.599,
      "language": "en"
    },
    {
      "text": "also using the you said some PyTorch",
      "start": 3446.16,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "implementation is allowed, some isn't.",
      "start": 3449.599,
      "duration": 4.641,
      "language": "en"
    },
    {
      "text": "Are we allowed to use docu or we are",
      "start": 3451.2,
      "duration": 6.24,
      "language": "en"
    },
    {
      "text": "doing the like entirely by hand doing",
      "start": 3454.24,
      "duration": 6.319,
      "language": "en"
    },
    {
      "text": "the gradient? Uh so the question is in",
      "start": 3457.44,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "the homework are you going to compute",
      "start": 3460.559,
      "duration": 3.921,
      "language": "en"
    },
    {
      "text": "gradients by hand? And the answer is no.",
      "start": 3462.48,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "Uh you're going to just use pietorch",
      "start": 3464.48,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "gradient. This is uh just to break it",
      "start": 3466.88,
      "duration": 7.0,
      "language": "en"
    },
    {
      "text": "down so we can do the the um counting",
      "start": 3469.2,
      "duration": 7.76,
      "language": "en"
    },
    {
      "text": "flops. Okay. Any any questions about",
      "start": 3473.88,
      "duration": 7.04,
      "language": "en"
    },
    {
      "text": "this before I move on.",
      "start": 3476.96,
      "duration": 3.96,
      "language": "en"
    },
    {
      "text": "Okay, just to summarize the forward pass",
      "start": 3485.359,
      "duration": 6.72,
      "language": "en"
    },
    {
      "text": "is for this particular model is 2 times",
      "start": 3488.24,
      "duration": 5.599,
      "language": "en"
    },
    {
      "text": "the number of data points times the",
      "start": 3492.079,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "number of parameters and backward is",
      "start": 3493.839,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "four times the number of data points",
      "start": 3496.079,
      "duration": 3.04,
      "language": "en"
    },
    {
      "text": "times the number of parameters which",
      "start": 3497.44,
      "duration": 3.359,
      "language": "en"
    },
    {
      "text": "means that total it's six times number",
      "start": 3499.119,
      "duration": 4.561,
      "language": "en"
    },
    {
      "text": "of data points times parameters okay and",
      "start": 3500.799,
      "duration": 6.081,
      "language": "en"
    },
    {
      "text": "that's explains why uh there was that",
      "start": 3503.68,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "six in the beginning when I asked the",
      "start": 3506.88,
      "duration": 3.32,
      "language": "en"
    },
    {
      "text": "motivating uh",
      "start": 3508.48,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "question So now this is for a simple you",
      "start": 3510.2,
      "duration": 7.0,
      "language": "en"
    },
    {
      "text": "know linear model. Um it turns out that",
      "start": 3514.0,
      "duration": 6.559,
      "language": "en"
    },
    {
      "text": "mo many models this is uh basically the",
      "start": 3517.2,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "bulk of a computation.",
      "start": 3520.559,
      "duration": 5.361,
      "language": "en"
    },
    {
      "text": "um when essentially every computation",
      "start": 3522.88,
      "duration": 6.959,
      "language": "en"
    },
    {
      "text": "you do has uh you know touches",
      "start": 3525.92,
      "duration": 6.96,
      "language": "en"
    },
    {
      "text": "essentially a new parameters roughly",
      "start": 3529.839,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "right and you know obviously this",
      "start": 3532.88,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "doesn't hold you can find models where",
      "start": 3535.119,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "this doesn't hold because you can have",
      "start": 3537.28,
      "duration": 3.12,
      "language": "en"
    },
    {
      "text": "like one parameter through parameter",
      "start": 3538.559,
      "duration": 4.081,
      "language": "en"
    },
    {
      "text": "sharing um and have you know a billion",
      "start": 3540.4,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "flops but that's generally what not what",
      "start": 3542.64,
      "duration": 6.679,
      "language": "en"
    },
    {
      "text": "models look like.",
      "start": 3545.76,
      "duration": 3.559,
      "language": "en"
    },
    {
      "text": "Okay, so let me um move",
      "start": 3549.68,
      "duration": 9.24,
      "language": "en"
    },
    {
      "text": "on. Um so far I've I've basically",
      "start": 3554.44,
      "duration": 7.08,
      "language": "en"
    },
    {
      "text": "finished talking about the the resource",
      "start": 3558.92,
      "duration": 4.84,
      "language": "en"
    },
    {
      "text": "accounting. So we looked at tensors. We",
      "start": 3561.52,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "looked at some computation on tensors.",
      "start": 3563.76,
      "duration": 5.599,
      "language": "en"
    },
    {
      "text": "We looked at how uh much tensors take to",
      "start": 3565.44,
      "duration": 6.56,
      "language": "en"
    },
    {
      "text": "store and also how many flops takes",
      "start": 3569.359,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "tensors uh take when you do various",
      "start": 3572.0,
      "duration": 5.119,
      "language": "en"
    },
    {
      "text": "operations on them. Um now let's start",
      "start": 3574.319,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "building up uh different you know",
      "start": 3577.119,
      "duration": 5.121,
      "language": "en"
    },
    {
      "text": "models. Um I think this part isn't",
      "start": 3579.599,
      "duration": 6.96,
      "language": "en"
    },
    {
      "text": "necessarily going to be um you know that",
      "start": 3582.24,
      "duration": 6.48,
      "language": "en"
    },
    {
      "text": "uh you know conceptually you know",
      "start": 3586.559,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "interesting or challenging but it's more",
      "start": 3588.72,
      "duration": 4.119,
      "language": "en"
    },
    {
      "text": "for maybe just you know",
      "start": 3590.799,
      "duration": 5.081,
      "language": "en"
    },
    {
      "text": "completeness. Okay. So",
      "start": 3592.839,
      "duration": 6.441,
      "language": "en"
    },
    {
      "text": "um so parameters in PyTorch are stored",
      "start": 3595.88,
      "duration": 6.36,
      "language": "en"
    },
    {
      "text": "as these nn parameter",
      "start": 3599.28,
      "duration": 6.4,
      "language": "en"
    },
    {
      "text": "um objects. Um let's talk a little bit",
      "start": 3602.24,
      "duration": 6.119,
      "language": "en"
    },
    {
      "text": "about parameter initialization.",
      "start": 3605.68,
      "duration": 9.0,
      "language": "en"
    },
    {
      "text": "Um so if you have let's say a",
      "start": 3608.359,
      "duration": 10.921,
      "language": "en"
    },
    {
      "text": "uh um you know parameter that has uh",
      "start": 3614.68,
      "duration": 7.72,
      "language": "en"
    },
    {
      "text": "okay so you you generate um a okay do",
      "start": 3619.28,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "your sorry your w parameter is an input",
      "start": 3622.4,
      "duration": 6.159,
      "language": "en"
    },
    {
      "text": "dimension by hidden dimension matrix um",
      "start": 3625.28,
      "duration": 5.039,
      "language": "en"
    },
    {
      "text": "you're still in the linear model case so",
      "start": 3628.559,
      "duration": 3.841,
      "language": "en"
    },
    {
      "text": "let's just generate an input and let's",
      "start": 3630.319,
      "duration": 5.201,
      "language": "en"
    },
    {
      "text": "feed it through the output okay so rand",
      "start": 3632.4,
      "duration": 5.199,
      "language": "en"
    },
    {
      "text": "and unit no gausian",
      "start": 3635.52,
      "duration": 6.319,
      "language": "en"
    },
    {
      "text": "is uh you know seems innocuous. Um what",
      "start": 3637.599,
      "duration": 6.081,
      "language": "en"
    },
    {
      "text": "happens when you do this is that if you",
      "start": 3641.839,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "look at the output um you get some",
      "start": 3643.68,
      "duration": 5.679,
      "language": "en"
    },
    {
      "text": "pretty large numbers right and this is",
      "start": 3646.16,
      "duration": 6.48,
      "language": "en"
    },
    {
      "text": "because when you you know have the the",
      "start": 3649.359,
      "duration": 6.401,
      "language": "en"
    },
    {
      "text": "number uh grows as essentially the",
      "start": 3652.64,
      "duration": 5.679,
      "language": "en"
    },
    {
      "text": "square root of the the hidden dimension.",
      "start": 3655.76,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "Um and so when you have large models, uh",
      "start": 3658.319,
      "duration": 6.161,
      "language": "en"
    },
    {
      "text": "this is going to you know blow up and um",
      "start": 3661.28,
      "duration": 6.44,
      "language": "en"
    },
    {
      "text": "and training can be very unstable.",
      "start": 3664.48,
      "duration": 7.44,
      "language": "en"
    },
    {
      "text": "Um so so typically what you want to do",
      "start": 3667.72,
      "duration": 7.56,
      "language": "en"
    },
    {
      "text": "is initialize in a way that's um you",
      "start": 3671.92,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "know invariant to hidden or at least you",
      "start": 3675.28,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "know when you you're guaranteed that",
      "start": 3677.44,
      "duration": 3.2,
      "language": "en"
    },
    {
      "text": "it's not going to you know blow up. And",
      "start": 3678.72,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "one simple way to do this is just",
      "start": 3680.64,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "rescale by the one over square root of",
      "start": 3682.559,
      "duration": 7.601,
      "language": "en"
    },
    {
      "text": "number of you know of inputs. Um so",
      "start": 3685.52,
      "duration": 8.16,
      "language": "en"
    },
    {
      "text": "basically let's redo this w equals you",
      "start": 3690.16,
      "duration": 6.24,
      "language": "en"
    },
    {
      "text": "know parameter where I simply divide by",
      "start": 3693.68,
      "duration": 6.08,
      "language": "en"
    },
    {
      "text": "the square root of the input dimension.",
      "start": 3696.4,
      "duration": 6.159,
      "language": "en"
    },
    {
      "text": "Um and then now when you uh feed it",
      "start": 3699.76,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "through the output now you get things",
      "start": 3702.559,
      "duration": 4.161,
      "language": "en"
    },
    {
      "text": "that are stable around you know this",
      "start": 3704.4,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "will actually concentrate to um you know",
      "start": 3706.72,
      "duration": 7.04,
      "language": "en"
    },
    {
      "text": "something like normal 01.",
      "start": 3709.68,
      "duration": 7.04,
      "language": "en"
    },
    {
      "text": "Okay, so this is basically you know this",
      "start": 3713.76,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "has been explored pretty extensively in",
      "start": 3716.72,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "deep learning literature is known uh up",
      "start": 3719.2,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "to a constant as Xavier initialization",
      "start": 3721.76,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "and typically I guess it's uh fairly",
      "start": 3724.24,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "common if you want to be extra safe uh",
      "start": 3726.72,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "you don't trust the normal because it",
      "start": 3729.2,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "doesn't have it has unbounded tails and",
      "start": 3730.799,
      "duration": 4.081,
      "language": "en"
    },
    {
      "text": "you just say I'm going to truncate to",
      "start": 3733.2,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "minus 33 so I don't get any large values",
      "start": 3734.88,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "and I don't want any to to mess with",
      "start": 3737.44,
      "duration": 5.0,
      "language": "en"
    },
    {
      "text": "that.",
      "start": 3739.68,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "Okay.",
      "start": 3742.44,
      "duration": 3.0,
      "language": "en"
    },
    {
      "text": "Um Okay. So",
      "start": 3745.48,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "um so let's build a you know just a",
      "start": 3750.64,
      "duration": 4.52,
      "language": "en"
    },
    {
      "text": "simple model.",
      "start": 3753.28,
      "duration": 6.24,
      "language": "en"
    },
    {
      "text": "Um it's going to have um dimensions and",
      "start": 3755.16,
      "duration": 7.399,
      "language": "en"
    },
    {
      "text": "two layers. There's this you know I just",
      "start": 3759.52,
      "duration": 4.799,
      "language": "en"
    },
    {
      "text": "made up this name cruncher. It's a",
      "start": 3762.559,
      "duration": 4.481,
      "language": "en"
    },
    {
      "text": "custom model which is a deep linear",
      "start": 3764.319,
      "duration": 7.441,
      "language": "en"
    },
    {
      "text": "network which has um num layers layers",
      "start": 3767.04,
      "duration": 9.84,
      "language": "en"
    },
    {
      "text": "and each layer is a a linear um model",
      "start": 3771.76,
      "duration": 8.359,
      "language": "en"
    },
    {
      "text": "which has essentially just a matrix",
      "start": 3776.88,
      "duration": 5.719,
      "language": "en"
    },
    {
      "text": "multiplication. Okay.",
      "start": 3780.119,
      "duration": 7.081,
      "language": "en"
    },
    {
      "text": "Um so um this parameters of these this",
      "start": 3782.599,
      "duration": 10.921,
      "language": "en"
    },
    {
      "text": "model is looks like um I have um",
      "start": 3787.2,
      "duration": 10.399,
      "language": "en"
    },
    {
      "text": "um layers for the the first layer um",
      "start": 3793.52,
      "duration": 7.2,
      "language": "en"
    },
    {
      "text": "which is a D byD matrix uh the second",
      "start": 3797.599,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "layer which is also a D byD matrix and",
      "start": 3800.72,
      "duration": 6.04,
      "language": "en"
    },
    {
      "text": "then I have a um a head um or a final",
      "start": 3802.319,
      "duration": 8.72,
      "language": "en"
    },
    {
      "text": "layer. Okay. So if I get the number of",
      "start": 3806.76,
      "duration": 8.279,
      "language": "en"
    },
    {
      "text": "of parameters of this model, then it's",
      "start": 3811.039,
      "duration": 9.121,
      "language": "en"
    },
    {
      "text": "going to be um d^2 + d ^2 + d. Okay, so",
      "start": 3815.039,
      "duration": 8.481,
      "language": "en"
    },
    {
      "text": "nothing too surprising there. Um and I'm",
      "start": 3820.16,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "going to move it to the GPU because I",
      "start": 3823.52,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "run want this to run one fast. And um",
      "start": 3825.68,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "I'm going to generate some random data",
      "start": 3829.52,
      "duration": 4.319,
      "language": "en"
    },
    {
      "text": "and feed it through the data. And the",
      "start": 3831.68,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "forward pass is just going through um",
      "start": 3833.839,
      "duration": 5.681,
      "language": "en"
    },
    {
      "text": "the layers um and then finally applying",
      "start": 3836.16,
      "duration": 5.6,
      "language": "en"
    },
    {
      "text": "the",
      "start": 3839.52,
      "duration": 2.24,
      "language": "en"
    },
    {
      "text": "head. Um okay. So with that model I",
      "start": 3842.2,
      "duration": 6.76,
      "language": "en"
    },
    {
      "text": "let's try to I'm going to use this model",
      "start": 3847.039,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "and do some you know stuff with it. Um",
      "start": 3848.96,
      "duration": 5.599,
      "language": "en"
    },
    {
      "text": "but just one kind of general digression.",
      "start": 3851.599,
      "duration": 7.76,
      "language": "en"
    },
    {
      "text": "Um, randomness is something that uh is",
      "start": 3854.559,
      "duration": 8.081,
      "language": "en"
    },
    {
      "text": "is sort of can be annoying in some cases",
      "start": 3859.359,
      "duration": 4.881,
      "language": "en"
    },
    {
      "text": "if you're trying to reproduce a bug for",
      "start": 3862.64,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "example. It shows up in many places",
      "start": 3864.24,
      "duration": 4.599,
      "language": "en"
    },
    {
      "text": "initialization dropout data ordering.",
      "start": 3866.48,
      "duration": 6.4,
      "language": "en"
    },
    {
      "text": "Um, and just a best practice is I we",
      "start": 3868.839,
      "duration": 7.041,
      "language": "en"
    },
    {
      "text": "recommend you always pass a fix a random",
      "start": 3872.88,
      "duration": 6.4,
      "language": "en"
    },
    {
      "text": "seed. Um, so you can reproduce your your",
      "start": 3875.88,
      "duration": 6.64,
      "language": "en"
    },
    {
      "text": "model or at least as well as you can.",
      "start": 3879.28,
      "duration": 6.96,
      "language": "en"
    },
    {
      "text": "Um and in particular having a difference",
      "start": 3882.52,
      "duration": 5.079,
      "language": "en"
    },
    {
      "text": "random seat for every source of",
      "start": 3886.24,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "randomness is is you know nice because",
      "start": 3887.599,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "then you can for example fix",
      "start": 3890.319,
      "duration": 3.681,
      "language": "en"
    },
    {
      "text": "initialization or fix the data ordering",
      "start": 3891.92,
      "duration": 4.879,
      "language": "en"
    },
    {
      "text": "but vary other other things. Um",
      "start": 3894.0,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "determinism is your your friend when",
      "start": 3896.799,
      "duration": 5.361,
      "language": "en"
    },
    {
      "text": "you're debugging. Um and you know in",
      "start": 3898.64,
      "duration": 6.159,
      "language": "en"
    },
    {
      "text": "code unfortunately there's you know many",
      "start": 3902.16,
      "duration": 5.439,
      "language": "en"
    },
    {
      "text": "places where you can use randomness and",
      "start": 3904.799,
      "duration": 6.161,
      "language": "en"
    },
    {
      "text": "just be uh cognizant of you know which",
      "start": 3907.599,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "one you're using and just if you want to",
      "start": 3910.96,
      "duration": 6.839,
      "language": "en"
    },
    {
      "text": "be safe just uh set the C2 for all of",
      "start": 3913.599,
      "duration": 8.801,
      "language": "en"
    },
    {
      "text": "them. Um data loading um I guess I'll go",
      "start": 3917.799,
      "duration": 8.681,
      "language": "en"
    },
    {
      "text": "through this quickly. It's it's not um",
      "start": 3922.4,
      "duration": 5.919,
      "language": "en"
    },
    {
      "text": "it'll be useful for your your",
      "start": 3926.48,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "assignment. Um so in language modeling",
      "start": 3928.319,
      "duration": 5.601,
      "language": "en"
    },
    {
      "text": "data is typically just a sequence of",
      "start": 3932.0,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "integers because this is remember output",
      "start": 3933.92,
      "duration": 5.439,
      "language": "en"
    },
    {
      "text": "by the tokenizer. Um and you serialize",
      "start": 3935.839,
      "duration": 6.48,
      "language": "en"
    },
    {
      "text": "them into um you can serialize them into",
      "start": 3939.359,
      "duration": 8.281,
      "language": "en"
    },
    {
      "text": "numpire arrays. Um and one I guess thing",
      "start": 3942.319,
      "duration": 9.441,
      "language": "en"
    },
    {
      "text": "that's maybe useful is that uh you don't",
      "start": 3947.64,
      "duration": 5.959,
      "language": "en"
    },
    {
      "text": "want to load all your data into memory",
      "start": 3951.76,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "at at once because for example the llama",
      "start": 3953.599,
      "duration": 6.321,
      "language": "en"
    },
    {
      "text": "data is the 2.8 8 terabytes. Um, but you",
      "start": 3956.4,
      "duration": 6.32,
      "language": "en"
    },
    {
      "text": "can sort of pretend to load it by using",
      "start": 3959.92,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "this uh handy function called memap. So,",
      "start": 3962.72,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "which gives you essentially a variable",
      "start": 3965.2,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "that is mapped to a file. So when you",
      "start": 3968.24,
      "duration": 6.879,
      "language": "en"
    },
    {
      "text": "try to access um the data it actually on",
      "start": 3970.88,
      "duration": 7.84,
      "language": "en"
    },
    {
      "text": "the on demand loads the file and then",
      "start": 3975.119,
      "duration": 6.321,
      "language": "en"
    },
    {
      "text": "using that you can uh create a data",
      "start": 3978.72,
      "duration": 6.399,
      "language": "en"
    },
    {
      "text": "loader that um you know is a you know",
      "start": 3981.44,
      "duration": 7.84,
      "language": "en"
    },
    {
      "text": "samples data from your um batch. So I'm",
      "start": 3985.119,
      "duration": 6.081,
      "language": "en"
    },
    {
      "text": "going to skip over that just in interest",
      "start": 3989.28,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "of time. Um let's talk a little bit",
      "start": 3991.2,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "about you know optimizer. So we've",
      "start": 3994.24,
      "duration": 3.879,
      "language": "en"
    },
    {
      "text": "defined our",
      "start": 3996.4,
      "duration": 4.199,
      "language": "en"
    },
    {
      "text": "model.",
      "start": 3998.119,
      "duration": 8.041,
      "language": "en"
    },
    {
      "text": "Um so there's many optimizers. Um just",
      "start": 4000.599,
      "duration": 7.96,
      "language": "en"
    },
    {
      "text": "kind of maybe going through the",
      "start": 4006.16,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "intuitions behind some of them. So of",
      "start": 4008.559,
      "duration": 3.121,
      "language": "en"
    },
    {
      "text": "course there's stocastic gradient",
      "start": 4010.559,
      "duration": 2.881,
      "language": "en"
    },
    {
      "text": "descent. You compute the gradient of",
      "start": 4011.68,
      "duration": 3.359,
      "language": "en"
    },
    {
      "text": "your batch. You take a step in that",
      "start": 4013.44,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "direction. No questions asked. Um",
      "start": 4015.039,
      "duration": 4.881,
      "language": "en"
    },
    {
      "text": "there's a idea called momentum which",
      "start": 4017.52,
      "duration": 6.16,
      "language": "en"
    },
    {
      "text": "dates back to classic optimization nest.",
      "start": 4019.92,
      "duration": 7.28,
      "language": "en"
    },
    {
      "text": "um where you uh have a running average",
      "start": 4023.68,
      "duration": 7.2,
      "language": "en"
    },
    {
      "text": "of your um gradients and you update",
      "start": 4027.2,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "against the you know the running average",
      "start": 4030.88,
      "duration": 3.959,
      "language": "en"
    },
    {
      "text": "instead of your instantaneous you know",
      "start": 4032.88,
      "duration": 6.56,
      "language": "en"
    },
    {
      "text": "gradient. Um and then you have um adrad",
      "start": 4034.839,
      "duration": 8.121,
      "language": "en"
    },
    {
      "text": "which um you you know you scale the",
      "start": 4039.44,
      "duration": 6.96,
      "language": "en"
    },
    {
      "text": "gradients by uh your",
      "start": 4042.96,
      "duration": 7.2,
      "language": "en"
    },
    {
      "text": "um your the average over the the norms",
      "start": 4046.4,
      "duration": 6.639,
      "language": "en"
    },
    {
      "text": "of your or I guess not the norms the the",
      "start": 4050.16,
      "duration": 4.959,
      "language": "en"
    },
    {
      "text": "square of the gradients. You also have",
      "start": 4053.039,
      "duration": 5.841,
      "language": "en"
    },
    {
      "text": "RMS prop which uh is a improved version",
      "start": 4055.119,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "of adigra which uses a exponential",
      "start": 4058.88,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "averaging rather than just like a flat",
      "start": 4061.119,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "average. And then finally atom which",
      "start": 4063.76,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "appeared in 2014 which is essentially",
      "start": 4066.319,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "combining RMS prop and momentum. So",
      "start": 4068.48,
      "duration": 6.28,
      "language": "en"
    },
    {
      "text": "that's why you're maintaining both uh",
      "start": 4071.359,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "your running average of your gradients",
      "start": 4074.76,
      "duration": 4.12,
      "language": "en"
    },
    {
      "text": "but also running average of your",
      "start": 4077.039,
      "duration": 4.881,
      "language": "en"
    },
    {
      "text": "gradient squared. Okay. So since you're",
      "start": 4078.88,
      "duration": 6.88,
      "language": "en"
    },
    {
      "text": "going to implement uh Adam um in",
      "start": 4081.92,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "homework one I'm not going to do that",
      "start": 4085.76,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "instead I'm going to implement um you",
      "start": 4087.44,
      "duration": 6.56,
      "language": "en"
    },
    {
      "text": "know adrad so so the way you implement",
      "start": 4090.0,
      "duration": 4.839,
      "language": "en"
    },
    {
      "text": "an",
      "start": 4094.0,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "optimizer in in you know pietorch is",
      "start": 4094.839,
      "duration": 5.081,
      "language": "en"
    },
    {
      "text": "that you override the the optimizer",
      "start": 4098.159,
      "duration": 7.2,
      "language": "en"
    },
    {
      "text": "class and you have to um uh let's see",
      "start": 4099.92,
      "duration": 6.279,
      "language": "en"
    },
    {
      "text": "maybe",
      "start": 4105.359,
      "duration": 3.281,
      "language": "en"
    },
    {
      "text": "I'll and I'll get to the implementation",
      "start": 4106.199,
      "duration": 6.761,
      "language": "en"
    },
    {
      "text": "once we step through it Um so let's",
      "start": 4108.64,
      "duration": 7.76,
      "language": "en"
    },
    {
      "text": "define some data. Um compute the forward",
      "start": 4112.96,
      "duration": 8.16,
      "language": "en"
    },
    {
      "text": "pass on the loss and then you compute",
      "start": 4116.4,
      "duration": 7.359,
      "language": "en"
    },
    {
      "text": "the gradients and then you when you call",
      "start": 4121.12,
      "duration": 5.48,
      "language": "en"
    },
    {
      "text": "optimizer.step",
      "start": 4123.759,
      "duration": 6.96,
      "language": "en"
    },
    {
      "text": "um this is where the optimizer uh",
      "start": 4126.6,
      "duration": 7.559,
      "language": "en"
    },
    {
      "text": "actually is active. So what this looks",
      "start": 4130.719,
      "duration": 7.681,
      "language": "en"
    },
    {
      "text": "like is um your parameters are grouped u",
      "start": 4134.159,
      "duration": 6.401,
      "language": "en"
    },
    {
      "text": "by for example you have one for the",
      "start": 4138.4,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "layer zero layer one and then the final",
      "start": 4140.56,
      "duration": 8.4,
      "language": "en"
    },
    {
      "text": "you know weights um and you can access a",
      "start": 4142.96,
      "duration": 10.0,
      "language": "en"
    },
    {
      "text": "state which is a a dictionary from",
      "start": 4148.96,
      "duration": 6.799,
      "language": "en"
    },
    {
      "text": "parameters to um you know whatever you",
      "start": 4152.96,
      "duration": 7.68,
      "language": "en"
    },
    {
      "text": "want to store as optimizer state. um the",
      "start": 4155.759,
      "duration": 7.281,
      "language": "en"
    },
    {
      "text": "gradient of that parameter you assume is",
      "start": 4160.64,
      "duration": 6.719,
      "language": "en"
    },
    {
      "text": "already uh calculated um by the the",
      "start": 4163.04,
      "duration": 7.36,
      "language": "en"
    },
    {
      "text": "backward pass. Um and now you can do",
      "start": 4167.359,
      "duration": 5.88,
      "language": "en"
    },
    {
      "text": "things like um you know",
      "start": 4170.4,
      "duration": 7.04,
      "language": "en"
    },
    {
      "text": "in in adrad you're storing the sum of",
      "start": 4173.239,
      "duration": 7.401,
      "language": "en"
    },
    {
      "text": "the gradient uh squares. So you can get",
      "start": 4177.44,
      "duration": 6.399,
      "language": "en"
    },
    {
      "text": "that G2 variable um and you can update",
      "start": 4180.64,
      "duration": 5.199,
      "language": "en"
    },
    {
      "text": "that based on the square of the",
      "start": 4183.839,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "gradient. So this is element wise",
      "start": 4185.839,
      "duration": 4.561,
      "language": "en"
    },
    {
      "text": "squaring of the um the gradient and you",
      "start": 4187.359,
      "duration": 4.44,
      "language": "en"
    },
    {
      "text": "put it back into the",
      "start": 4190.4,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "state. Okay. So then your obviously your",
      "start": 4191.799,
      "duration": 5.641,
      "language": "en"
    },
    {
      "text": "optimizer is responsible for updating",
      "start": 4194.96,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "the parameters and this is just the you",
      "start": 4197.44,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "know you update the learning rate times",
      "start": 4200.48,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "the gradient divided by this uh scaling.",
      "start": 4202.32,
      "duration": 6.879,
      "language": "en"
    },
    {
      "text": "So now this state is kept over across",
      "start": 4205.36,
      "duration": 7.44,
      "language": "en"
    },
    {
      "text": "multiple uh invocations of um you know",
      "start": 4209.199,
      "duration": 5.761,
      "language": "en"
    },
    {
      "text": "the",
      "start": 4212.8,
      "duration": 2.16,
      "language": "en"
    },
    {
      "text": "optimizer. Okay. So um and then at the",
      "start": 4216.84,
      "duration": 6.92,
      "language": "en"
    },
    {
      "text": "you know end of your optimizer step you",
      "start": 4221.92,
      "duration": 4.319,
      "language": "en"
    },
    {
      "text": "can you know free up the memory uh just",
      "start": 4223.76,
      "duration": 6.64,
      "language": "en"
    },
    {
      "text": "to um which is I think going to actually",
      "start": 4226.239,
      "duration": 6.881,
      "language": "en"
    },
    {
      "text": "be more important when you look when we",
      "start": 4230.4,
      "duration": 5.279,
      "language": "en"
    },
    {
      "text": "talk about model parallelism.",
      "start": 4233.12,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "Okay, so let's talk about the memory",
      "start": 4235.679,
      "duration": 6.241,
      "language": "en"
    },
    {
      "text": "requirements of uh the optimizer states",
      "start": 4238.0,
      "duration": 5.64,
      "language": "en"
    },
    {
      "text": "and actually basically at this point",
      "start": 4241.92,
      "duration": 6.799,
      "language": "en"
    },
    {
      "text": "everything. So um you need to uh the",
      "start": 4243.64,
      "duration": 8.599,
      "language": "en"
    },
    {
      "text": "number of parameters in this model is d²",
      "start": 4248.719,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "times the number of layers plus d for",
      "start": 4252.239,
      "duration": 6.92,
      "language": "en"
    },
    {
      "text": "the final uh head. Okay.",
      "start": 4254.719,
      "duration": 7.321,
      "language": "en"
    },
    {
      "text": "Um the number of",
      "start": 4259.159,
      "duration": 5.08,
      "language": "en"
    },
    {
      "text": "activations. So this is something we",
      "start": 4262.04,
      "duration": 4.44,
      "language": "en"
    },
    {
      "text": "didn't do before but now for this simple",
      "start": 4264.239,
      "duration": 5.92,
      "language": "en"
    },
    {
      "text": "model it's fairly easy to do. It's just",
      "start": 4266.48,
      "duration": 7.52,
      "language": "en"
    },
    {
      "text": "um B times you know D times the the",
      "start": 4270.159,
      "duration": 6.08,
      "language": "en"
    },
    {
      "text": "number of of layers you have for every",
      "start": 4274.0,
      "duration": 3.679,
      "language": "en"
    },
    {
      "text": "layer for every data point for every",
      "start": 4276.239,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "dimension you have to hold the",
      "start": 4277.679,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "activations.",
      "start": 4279.679,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "Um for the gradients um this is the same",
      "start": 4281.28,
      "duration": 6.959,
      "language": "en"
    },
    {
      "text": "as the number of parameters and the",
      "start": 4285.679,
      "duration": 5.48,
      "language": "en"
    },
    {
      "text": "number of optimizer states in for",
      "start": 4288.239,
      "duration": 6.401,
      "language": "en"
    },
    {
      "text": "adigrad it's uh you remember we had to",
      "start": 4291.159,
      "duration": 5.441,
      "language": "en"
    },
    {
      "text": "store",
      "start": 4294.64,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "um the the gradient squared. So that's",
      "start": 4296.6,
      "duration": 5.96,
      "language": "en"
    },
    {
      "text": "another copy of the parameters. So",
      "start": 4300.32,
      "duration": 3.48,
      "language": "en"
    },
    {
      "text": "putting all",
      "start": 4302.56,
      "duration": 6.8,
      "language": "en"
    },
    {
      "text": "together we have um the total memory is",
      "start": 4303.8,
      "duration": 9.2,
      "language": "en"
    },
    {
      "text": "assuming you know FP32 which means four",
      "start": 4309.36,
      "duration": 6.319,
      "language": "en"
    },
    {
      "text": "bytes times the number of parameters,",
      "start": 4313.0,
      "duration": 4.76,
      "language": "en"
    },
    {
      "text": "number of activations, number of",
      "start": 4315.679,
      "duration": 3.961,
      "language": "en"
    },
    {
      "text": "gradients and number of optimizer",
      "start": 4317.76,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "states. Okay. And that gives us uh you",
      "start": 4319.64,
      "duration": 6.519,
      "language": "en"
    },
    {
      "text": "know some number which",
      "start": 4323.12,
      "duration": 5.88,
      "language": "en"
    },
    {
      "text": "um is 496",
      "start": 4326.159,
      "duration": 6.481,
      "language": "en"
    },
    {
      "text": "here. Okay. So this is um a fairly",
      "start": 4329.0,
      "duration": 5.56,
      "language": "en"
    },
    {
      "text": "simple calculation in the assignment",
      "start": 4332.64,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "one. Um you're going to do this for the",
      "start": 4334.56,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "transformer which is a little bit more",
      "start": 4337.04,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "involved because you have to there's not",
      "start": 4339.12,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "just matrix multiplications but there's",
      "start": 4341.44,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "um many matrices there's attention and",
      "start": 4343.44,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "there's all these other things but the",
      "start": 4346.0,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "general form of the calculation is the",
      "start": 4347.84,
      "duration": 6.16,
      "language": "en"
    },
    {
      "text": "same. You have parameters activations",
      "start": 4351.36,
      "duration": 7.4,
      "language": "en"
    },
    {
      "text": "gradients and optimizer states.",
      "start": 4354.0,
      "duration": 4.76,
      "language": "en"
    },
    {
      "text": "Okay. And",
      "start": 4360.96,
      "duration": 7.199,
      "language": "en"
    },
    {
      "text": "the so and the flops required again for",
      "start": 4362.92,
      "duration": 8.84,
      "language": "en"
    },
    {
      "text": "this model is six times uh the number of",
      "start": 4368.159,
      "duration": 5.601,
      "language": "en"
    },
    {
      "text": "tokens or number of data points times",
      "start": 4371.76,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "the number of parameters and um you know",
      "start": 4373.76,
      "duration": 6.32,
      "language": "en"
    },
    {
      "text": "that's basically concludes the resource",
      "start": 4377.52,
      "duration": 6.12,
      "language": "en"
    },
    {
      "text": "uh accounting for this particular model.",
      "start": 4380.08,
      "duration": 6.96,
      "language": "en"
    },
    {
      "text": "Um and uh if for reference if you're",
      "start": 4383.64,
      "duration": 5.72,
      "language": "en"
    },
    {
      "text": "curious about working this out for for",
      "start": 4387.04,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "transformers you can um consult um some",
      "start": 4389.36,
      "duration": 4.52,
      "language": "en"
    },
    {
      "text": "of these",
      "start": 4392.4,
      "duration": 5.6,
      "language": "en"
    },
    {
      "text": "articles. Okay. So in the remaining time",
      "start": 4393.88,
      "duration": 6.44,
      "language": "en"
    },
    {
      "text": "I think um maybe I'll pause for",
      "start": 4398.0,
      "duration": 5.6,
      "language": "en"
    },
    {
      "text": "questions. Um and we talked about",
      "start": 4400.32,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "building up the tensors and then we",
      "start": 4403.6,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "built a kind of a very small model and",
      "start": 4405.6,
      "duration": 5.599,
      "language": "en"
    },
    {
      "text": "you know we talked about optimization",
      "start": 4408.48,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "and how many how much memory and how",
      "start": 4411.199,
      "duration": 6.441,
      "language": "en"
    },
    {
      "text": "much uh compute was required.",
      "start": 4413.52,
      "duration": 4.12,
      "language": "en"
    },
    {
      "text": "Yeah.",
      "start": 4418.8,
      "duration": 3.0,
      "language": "en"
    },
    {
      "text": "So the question is why do you need to",
      "start": 4422.239,
      "duration": 4.161,
      "language": "en"
    },
    {
      "text": "store the activations? So naively you",
      "start": 4423.76,
      "duration": 4.959,
      "language": "en"
    },
    {
      "text": "need to store the activations because",
      "start": 4426.4,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "when you're uh when you're doing the",
      "start": 4428.719,
      "duration": 6.561,
      "language": "en"
    },
    {
      "text": "peer pass the gradients um of let's say",
      "start": 4431.52,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "the first layer depend on the",
      "start": 4435.28,
      "duration": 2.959,
      "language": "en"
    },
    {
      "text": "activation. So the gradients of the I",
      "start": 4436.48,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "layer depends on the activation there.",
      "start": 4438.239,
      "duration": 6.641,
      "language": "en"
    },
    {
      "text": "Um now if you're smarter you don't have",
      "start": 4441.04,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "to store the activations or you don't",
      "start": 4444.88,
      "duration": 3.359,
      "language": "en"
    },
    {
      "text": "have to store all of them. You can",
      "start": 4446.56,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "recomputee them and that's something a",
      "start": 4448.239,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "technique will uh called activation",
      "start": 4450.48,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "checkpointing which we're going to talk",
      "start": 4452.56,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "about later.",
      "start": 4454.0,
      "duration": 2.64,
      "language": "en"
    },
    {
      "text": "Okay, so let's just do this quick uh you",
      "start": 4459.679,
      "duration": 4.641,
      "language": "en"
    },
    {
      "text": "know actually there's not much to say",
      "start": 4462.64,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "here but you know here's your typical",
      "start": 4464.32,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "you know training loop um where you",
      "start": 4466.239,
      "duration": 7.121,
      "language": "en"
    },
    {
      "text": "define the model define the optimizer",
      "start": 4470.32,
      "duration": 7.04,
      "language": "en"
    },
    {
      "text": "and you get the data um feed forward",
      "start": 4473.36,
      "duration": 7.04,
      "language": "en"
    },
    {
      "text": "backward and take a step in a parameter",
      "start": 4477.36,
      "duration": 5.16,
      "language": "en"
    },
    {
      "text": "space",
      "start": 4480.4,
      "duration": 5.319,
      "language": "en"
    },
    {
      "text": "um And",
      "start": 4482.52,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "um I",
      "start": 4485.719,
      "duration": 4.681,
      "language": "en"
    },
    {
      "text": "guess it'll be more interesting I I",
      "start": 4488.04,
      "duration": 4.119,
      "language": "en"
    },
    {
      "text": "guess next time I should show like",
      "start": 4490.4,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "actual one plug which I isn't available",
      "start": 4492.159,
      "duration": 6.441,
      "language": "en"
    },
    {
      "text": "on this uh on this version but",
      "start": 4494.56,
      "duration": 5.8,
      "language": "en"
    },
    {
      "text": "um",
      "start": 4498.6,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "so one note about checkpointing",
      "start": 4500.36,
      "duration": 6.92,
      "language": "en"
    },
    {
      "text": "um so training language models takes a",
      "start": 4504.12,
      "duration": 6.44,
      "language": "en"
    },
    {
      "text": "long time and you certainly will crash",
      "start": 4507.28,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "at some point so you don't want to lose",
      "start": 4510.56,
      "duration": 2.8,
      "language": "en"
    },
    {
      "text": "all your progress.",
      "start": 4511.92,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "Um so you want to periodically save your",
      "start": 4513.36,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "model to disk and just to be very clear",
      "start": 4515.28,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "the thing you want to save is both the",
      "start": 4518.4,
      "duration": 5.4,
      "language": "en"
    },
    {
      "text": "model and uh the",
      "start": 4520.96,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "optimizer and probably you know which",
      "start": 4523.8,
      "duration": 4.04,
      "language": "en"
    },
    {
      "text": "iteration you're on. Um I should add",
      "start": 4525.92,
      "duration": 6.04,
      "language": "en"
    },
    {
      "text": "that and then you can just load it up.",
      "start": 4527.84,
      "duration": 8.0,
      "language": "en"
    },
    {
      "text": "Um one maybe final note and I'll uh end",
      "start": 4531.96,
      "duration": 7.719,
      "language": "en"
    },
    {
      "text": "is um I alluded to kind of mix precision",
      "start": 4535.84,
      "duration": 8.56,
      "language": "en"
    },
    {
      "text": "your training. Um you know choice of the",
      "start": 4539.679,
      "duration": 6.881,
      "language": "en"
    },
    {
      "text": "data type has the different trade-offs.",
      "start": 4544.4,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "If you have higher precision it's uh",
      "start": 4546.56,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "more accurate and stable um but it's",
      "start": 4548.8,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "more expensive and lower precision vice",
      "start": 4551.92,
      "duration": 7.92,
      "language": "en"
    },
    {
      "text": "versa. Um and as we mentioned before by",
      "start": 4554.32,
      "duration": 7.76,
      "language": "en"
    },
    {
      "text": "default the recommendation is use float",
      "start": 4559.84,
      "duration": 6.24,
      "language": "en"
    },
    {
      "text": "32 um but try to use VF uh 16 or even",
      "start": 4562.08,
      "duration": 7.84,
      "language": "en"
    },
    {
      "text": "FP8 whenever possible. So you can use um",
      "start": 4566.08,
      "duration": 6.639,
      "language": "en"
    },
    {
      "text": "lower precision for the FE forward pass",
      "start": 4569.92,
      "duration": 6.319,
      "language": "en"
    },
    {
      "text": "um but flow 32 for the rest. Um and this",
      "start": 4572.719,
      "duration": 5.201,
      "language": "en"
    },
    {
      "text": "is an idea that goes back to the you",
      "start": 4576.239,
      "duration": 4.081,
      "language": "en"
    },
    {
      "text": "know 2017 there's exploring mixed",
      "start": 4577.92,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "precision training. Um, PyTorch has um",
      "start": 4580.32,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "some tools that automatically allow you",
      "start": 4583.36,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "to do, you know, mixed precision",
      "start": 4585.28,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "training. Um, because it can be sort of",
      "start": 4587.36,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "annoying to have to specify which parts",
      "start": 4590.08,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "of your model it needs to be, you know,",
      "start": 4593.36,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "what precision. Um, generally you define",
      "start": 4595.52,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "your model as, you know, sort of this",
      "start": 4598.48,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "clean modular thing and and the",
      "start": 4600.32,
      "duration": 4.879,
      "language": "en"
    },
    {
      "text": "specifying the precision is sort of like",
      "start": 4602.8,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "a, you know, something that needs to cut",
      "start": 4605.199,
      "duration": 4.761,
      "language": "en"
    },
    {
      "text": "across that.",
      "start": 4607.28,
      "duration": 6.8,
      "language": "en"
    },
    {
      "text": "Um and one I guess maybe one kind of",
      "start": 4609.96,
      "duration": 6.36,
      "language": "en"
    },
    {
      "text": "general comment is that people are",
      "start": 4614.08,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "pushing the envelope on what precision",
      "start": 4616.32,
      "duration": 5.6,
      "language": "en"
    },
    {
      "text": "is needed. there's some you know papers",
      "start": 4619.6,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "that show you can actually use um FP8",
      "start": 4621.92,
      "duration": 6.279,
      "language": "en"
    },
    {
      "text": "you know all the way you know through um",
      "start": 4624.96,
      "duration": 6.719,
      "language": "en"
    },
    {
      "text": "there's um I guess one of the challenges",
      "start": 4628.199,
      "duration": 5.161,
      "language": "en"
    },
    {
      "text": "is of course when you have lower",
      "start": 4631.679,
      "duration": 4.161,
      "language": "en"
    },
    {
      "text": "precision it gets very numerically un",
      "start": 4633.36,
      "duration": 4.799,
      "language": "en"
    },
    {
      "text": "unstable but then you can do various",
      "start": 4635.84,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "tricks to you know control the uh the",
      "start": 4638.159,
      "duration": 5.441,
      "language": "en"
    },
    {
      "text": "numeric of your model during training so",
      "start": 4641.52,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "that you don't get into these you know",
      "start": 4643.6,
      "duration": 5.92,
      "language": "en"
    },
    {
      "text": "bad regimes so this is where I think um",
      "start": 4645.44,
      "duration": 6.16,
      "language": "en"
    },
    {
      "text": "the systems and the model architecture",
      "start": 4649.52,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "design kind of are synergistic because",
      "start": 4651.6,
      "duration": 7.16,
      "language": "en"
    },
    {
      "text": "you want to design models now that we",
      "start": 4654.64,
      "duration": 6.559,
      "language": "en"
    },
    {
      "text": "have a lot of model design is just",
      "start": 4658.76,
      "duration": 4.76,
      "language": "en"
    },
    {
      "text": "governed by hardware. So even the",
      "start": 4661.199,
      "duration": 4.081,
      "language": "en"
    },
    {
      "text": "transformer as we mentioned last time is",
      "start": 4663.52,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "governed by the having GPUs and now if",
      "start": 4665.28,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "we notice that you know Nvidia chips",
      "start": 4668.48,
      "duration": 6.08,
      "language": "en"
    },
    {
      "text": "have the property that if um lower",
      "start": 4670.96,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "precision even like int four for example",
      "start": 4674.56,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "is one thing. Now if you can make your",
      "start": 4676.64,
      "duration": 6.72,
      "language": "en"
    },
    {
      "text": "model training actually work on in four",
      "start": 4679.92,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "which is I think you know quite quite",
      "start": 4683.36,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "hard then you can get massive you know",
      "start": 4685.6,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "speed ups and your model will be more",
      "start": 4688.0,
      "duration": 5.0,
      "language": "en"
    },
    {
      "text": "you know efficient. Um",
      "start": 4689.92,
      "duration": 6.239,
      "language": "en"
    },
    {
      "text": "now there's another uh thing which we'll",
      "start": 4693.0,
      "duration": 5.159,
      "language": "en"
    },
    {
      "text": "talk about later which is you know often",
      "start": 4696.159,
      "duration": 5.361,
      "language": "en"
    },
    {
      "text": "you'll train your model using more sane",
      "start": 4698.159,
      "duration": 4.961,
      "language": "en"
    },
    {
      "text": "you know floating point but when it",
      "start": 4701.52,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "comes to inference you can go crazy and",
      "start": 4703.12,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "you take your preach model and then you",
      "start": 4705.52,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "can quantize it and get a lot of the",
      "start": 4707.84,
      "duration": 5.24,
      "language": "en"
    },
    {
      "text": "gains from very very uh aggressive",
      "start": 4710.4,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "quantization. So somehow training is a",
      "start": 4713.08,
      "duration": 5.32,
      "language": "en"
    },
    {
      "text": "lot um more difficult to do with low",
      "start": 4715.92,
      "duration": 3.759,
      "language": "en"
    },
    {
      "text": "precision but once you have a trained",
      "start": 4718.4,
      "duration": 5.08,
      "language": "en"
    },
    {
      "text": "model it's much easier to um make it low",
      "start": 4719.679,
      "duration": 7.441,
      "language": "en"
    },
    {
      "text": "precision. Okay. So um I will wrap up",
      "start": 4723.48,
      "duration": 7.04,
      "language": "en"
    },
    {
      "text": "there just to conclude um we have talked",
      "start": 4727.12,
      "duration": 6.64,
      "language": "en"
    },
    {
      "text": "about the different primitives to use to",
      "start": 4730.52,
      "duration": 5.08,
      "language": "en"
    },
    {
      "text": "train a model building up from tensors",
      "start": 4733.76,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "all the way to the training loop. We",
      "start": 4735.6,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "talked about u memory accounting and",
      "start": 4737.28,
      "duration": 4.879,
      "language": "en"
    },
    {
      "text": "flops accounting. um for these simple",
      "start": 4739.92,
      "duration": 5.279,
      "language": "en"
    },
    {
      "text": "models. Um hopefully once you go through",
      "start": 4742.159,
      "duration": 5.361,
      "language": "en"
    },
    {
      "text": "assignment one, all these concepts will",
      "start": 4745.199,
      "duration": 5.281,
      "language": "en"
    },
    {
      "text": "be really solid because you'll be um uh",
      "start": 4747.52,
      "duration": 5.8,
      "language": "en"
    },
    {
      "text": "applying these ideas for the actual",
      "start": 4750.48,
      "duration": 8.28,
      "language": "en"
    },
    {
      "text": "transformer. Okay, see you next time.",
      "start": 4753.32,
      "duration": 5.44,
      "language": "en"
    }
  ]
}