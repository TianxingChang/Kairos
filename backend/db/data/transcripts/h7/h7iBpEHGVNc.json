{
  "video_id": "h7iBpEHGVNc",
  "created_at": "2025-07-26T20:36:47.786329",
  "segment_count": 1332,
  "metadata": {
    "video_id": "h7iBpEHGVNc",
    "language": "en",
    "segment_count": 1332
  },
  "transcript": [
    {
      "text": "- Okay so welcome to\nCS 231N Lecture three.",
      "start": 8.555,
      "duration": 3.927,
      "language": "en"
    },
    {
      "text": "Today we're going to talk about\nloss functions and optimization",
      "start": 12.482,
      "duration": 2.912,
      "language": "en"
    },
    {
      "text": "but as usual, before we\nget to the main content",
      "start": 15.394,
      "duration": 2.126,
      "language": "en"
    },
    {
      "text": "of the lecture, there's a\ncouple administrative things",
      "start": 17.52,
      "duration": 2.242,
      "language": "en"
    },
    {
      "text": "to talk about. So the first thing is that\nassignment one has been released.",
      "start": 19.762,
      "duration": 5.584999999999997,
      "language": "en"
    },
    {
      "text": "You can find the link up on the website.",
      "start": 25.347,
      "duration": 2.342,
      "language": "en"
    },
    {
      "text": "And since we were a little bit late in getting this assignment\nout to you guys,",
      "start": 27.689,
      "duration": 3.196999999999999,
      "language": "en"
    },
    {
      "text": "we've decided to change\nthe due date to Thursday,",
      "start": 30.886,
      "duration": 3.095,
      "language": "en"
    },
    {
      "text": "April 20th at 11:59 p.m.,",
      "start": 33.981,
      "duration": 2.083,
      "language": "en"
    },
    {
      "text": "this will give you a full\ntwo weeks from the assignment",
      "start": 37.174,
      "duration": 2.907,
      "language": "en"
    },
    {
      "text": "release date to go and\nactually finish and work on it,",
      "start": 40.081,
      "duration": 3.421,
      "language": "en"
    },
    {
      "text": "so we'll update the syllabus\nfor this new due date",
      "start": 43.502,
      "duration": 3.797,
      "language": "en"
    },
    {
      "text": "in a little bit later today.",
      "start": 47.299,
      "duration": 2.588,
      "language": "en"
    },
    {
      "text": "And as a reminder, when you\ncomplete the assignment, you should go turn in the\nfinal zip file on Canvas",
      "start": 49.887,
      "duration": 5.530000000000001,
      "language": "en"
    },
    {
      "text": "so we can grade it and get\nyour grades back as quickly",
      "start": 55.417,
      "duration": 2.162,
      "language": "en"
    },
    {
      "text": "as possible. So the next thing is always\ncheck out Piazza for interesting",
      "start": 57.579,
      "duration": 6.6540000000000035,
      "language": "en"
    },
    {
      "text": "administrative stuff. So this week I wanted to\nhighlight that we have several",
      "start": 64.233,
      "duration": 4.355000000000004,
      "language": "en"
    },
    {
      "text": "example project ideas as\na pinned post on Piazza.",
      "start": 68.588,
      "duration": 3.644,
      "language": "en"
    },
    {
      "text": "So we went out and solicited\nexample of project ideas",
      "start": 72.232,
      "duration": 3.498,
      "language": "en"
    },
    {
      "text": "from various people in the\nStanford community or affiliated",
      "start": 75.73,
      "duration": 2.29,
      "language": "en"
    },
    {
      "text": "to Stanford, and they came\nup with some interesting",
      "start": 78.02,
      "duration": 2.931,
      "language": "en"
    },
    {
      "text": "suggestions for projects\nthat they might want students",
      "start": 80.951,
      "duration": 2.762,
      "language": "en"
    },
    {
      "text": "in the class to work on. So check out this pinned post\non Piazza and if you want",
      "start": 83.713,
      "duration": 4.0730000000000075,
      "language": "en"
    },
    {
      "text": "to work on any of these projects,\nthen feel free to contact",
      "start": 87.786,
      "duration": 3.598,
      "language": "en"
    },
    {
      "text": "the project mentors\ndirectly about these things.",
      "start": 91.384,
      "duration": 3.647,
      "language": "en"
    },
    {
      "text": "Aditionally we posted office\nhours on the course website,",
      "start": 95.031,
      "duration": 2.859,
      "language": "en"
    },
    {
      "text": "this is a Google calendar, so\nthis is something that people",
      "start": 97.89,
      "duration": 3.666,
      "language": "en"
    },
    {
      "text": "have been asking about\nand now it's up there.",
      "start": 101.556,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "The final administrative\nnote is about Google Cloud,",
      "start": 105.877,
      "duration": 3.23,
      "language": "en"
    },
    {
      "text": "as a reminder, because we're\nsupported by Google Cloud",
      "start": 109.107,
      "duration": 3.438,
      "language": "en"
    },
    {
      "text": "in this class, we're able to\ngive each of you an additional",
      "start": 112.545,
      "duration": 2.586,
      "language": "en"
    },
    {
      "text": "$100 credit for Google Cloud\nto work on your assignments",
      "start": 115.131,
      "duration": 2.702,
      "language": "en"
    },
    {
      "text": "and projects, and the exact\ndetails of how to redeem",
      "start": 117.833,
      "duration": 3.654,
      "language": "en"
    },
    {
      "text": "that credit will go out later\ntoday, most likely on Piazza.",
      "start": 121.487,
      "duration": 4.559,
      "language": "en"
    },
    {
      "text": "So if there's, I guess if\nthere's no questions about",
      "start": 126.046,
      "duration": 2.564,
      "language": "en"
    },
    {
      "text": "administrative stuff then we'll\nmove on to course content.",
      "start": 128.61,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "Okay cool. So recall from last time in lecture two,",
      "start": 134.24,
      "duration": 4.556999999999988,
      "language": "en"
    },
    {
      "text": "we were really talking about\nthe challenges of recognition",
      "start": 138.797,
      "duration": 2.415,
      "language": "en"
    },
    {
      "text": "and trying to hone in on this idea of a data-driven approach.",
      "start": 141.212,
      "duration": 4.064000000000021,
      "language": "en"
    },
    {
      "text": "We talked about this idea\nof image classification,",
      "start": 145.276,
      "duration": 2.445,
      "language": "en"
    },
    {
      "text": "talked about why it's hard,\nthere's this semantic gap",
      "start": 147.721,
      "duration": 2.239,
      "language": "en"
    },
    {
      "text": "between the giant grid of\nnumbers that the computer sees",
      "start": 149.96,
      "duration": 4.042,
      "language": "en"
    },
    {
      "text": "and the actual image that you see.",
      "start": 154.002,
      "duration": 2.61,
      "language": "en"
    },
    {
      "text": "We talked about various\nchallenges regarding this around illumination,\ndeformation, et cetera,",
      "start": 156.612,
      "duration": 4.14500000000001,
      "language": "en"
    },
    {
      "text": "and why this is actually a\nreally, really hard problem",
      "start": 160.757,
      "duration": 2.167,
      "language": "en"
    },
    {
      "text": "even though it's super\neasy for people to do",
      "start": 162.924,
      "duration": 2.062,
      "language": "en"
    },
    {
      "text": "with their human eyes\nand human visual system.",
      "start": 164.986,
      "duration": 3.726,
      "language": "en"
    },
    {
      "text": "Then also recall last time\nwe talked about the k-nearest",
      "start": 168.712,
      "duration": 2.509,
      "language": "en"
    },
    {
      "text": "neighbor classifier as kind\nof a simple introduction",
      "start": 171.221,
      "duration": 3.068,
      "language": "en"
    },
    {
      "text": "to this whole data-driven mindset. We talked about the CIFAR-10\ndata set where you can see",
      "start": 174.289,
      "duration": 4.503000000000014,
      "language": "en"
    },
    {
      "text": "an example of these images\non the upper left here,",
      "start": 178.792,
      "duration": 2.832,
      "language": "en"
    },
    {
      "text": "where CIFAR-10 gives you\nthese 10 different categories,",
      "start": 181.624,
      "duration": 2.864,
      "language": "en"
    },
    {
      "text": "airplane, automobile, whatnot,",
      "start": 184.488,
      "duration": 2.099,
      "language": "en"
    },
    {
      "text": "and we talked about how the\nk-nearest neighbor classifier",
      "start": 186.587,
      "duration": 2.84,
      "language": "en"
    },
    {
      "text": "can be used to learn decision boundaries",
      "start": 189.427,
      "duration": 2.575,
      "language": "en"
    },
    {
      "text": "to separate these data points into classes",
      "start": 192.002,
      "duration": 2.402,
      "language": "en"
    },
    {
      "text": "based on the training data.",
      "start": 194.404,
      "duration": 2.142,
      "language": "en"
    },
    {
      "text": "This also led us to a\ndiscussion of the idea of cross",
      "start": 196.546,
      "duration": 2.853,
      "language": "en"
    },
    {
      "text": "validation and setting\nhyper parameters by dividing",
      "start": 199.399,
      "duration": 2.356,
      "language": "en"
    },
    {
      "text": "your data into train,\nvalidation and test sets.",
      "start": 201.755,
      "duration": 4.235,
      "language": "en"
    },
    {
      "text": "Then also recall last time\nwe talked about linear",
      "start": 205.99,
      "duration": 2.018,
      "language": "en"
    },
    {
      "text": "classification as the first\nsort of building block",
      "start": 208.008,
      "duration": 2.849,
      "language": "en"
    },
    {
      "text": "as we move toward neural networks.",
      "start": 210.857,
      "duration": 2.353,
      "language": "en"
    },
    {
      "text": "Recall that the linear\nclassifier is an example",
      "start": 213.21,
      "duration": 2.316,
      "language": "en"
    },
    {
      "text": "of a parametric classifier\nwhere all of our knowledge",
      "start": 215.526,
      "duration": 3.812,
      "language": "en"
    },
    {
      "text": "about the training data gets summarized into this parameter matrix W that is set",
      "start": 219.338,
      "duration": 4.808000000000021,
      "language": "en"
    },
    {
      "text": "during the process of training.",
      "start": 224.146,
      "duration": 2.098,
      "language": "en"
    },
    {
      "text": "And this linear classifier\nrecall is super simple,",
      "start": 226.244,
      "duration": 3.004,
      "language": "en"
    },
    {
      "text": "where we're going to take\nthe image and stretch it out into a long vector.",
      "start": 229.248,
      "duration": 3.362000000000023,
      "language": "en"
    },
    {
      "text": "So here the image is x and\nthen we take that image",
      "start": 232.61,
      "duration": 3.164,
      "language": "en"
    },
    {
      "text": "which might be 32 by 32 by\n3 pixels, stretch it out",
      "start": 235.774,
      "duration": 3.321,
      "language": "en"
    },
    {
      "text": "into a long column vector of 32 times 32",
      "start": 239.095,
      "duration": 2.956,
      "language": "en"
    },
    {
      "text": "times 3 entries, where the 32 and 32 are\nthe height and width,",
      "start": 242.051,
      "duration": 5.152000000000015,
      "language": "en"
    },
    {
      "text": "and the 3 give you\nthe three color channels, red, green, blue.",
      "start": 247.203,
      "duration": 3.3189999999999884,
      "language": "en"
    },
    {
      "text": "Then there exists some parameter matrix, W",
      "start": 250.522,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "which will take this long column vector",
      "start": 254.361,
      "duration": 2.12,
      "language": "en"
    },
    {
      "text": "representing the image\npixels, and convert this",
      "start": 256.481,
      "duration": 2.836,
      "language": "en"
    },
    {
      "text": "and give you 10 numbers giving scores",
      "start": 259.317,
      "duration": 2.325,
      "language": "en"
    },
    {
      "text": "for each of the 10 classes\nin the case of CIFAR-10.",
      "start": 261.642,
      "duration": 3.545,
      "language": "en"
    },
    {
      "text": "Where we kind of had this interpretation where larger values of those scores,",
      "start": 265.187,
      "duration": 5.229999999999961,
      "language": "en"
    },
    {
      "text": "so a larger value for the cat\nclass means the classifier",
      "start": 270.417,
      "duration": 2.73,
      "language": "en"
    },
    {
      "text": "thinks that the cat is\nmore likely for that image,",
      "start": 273.147,
      "duration": 2.534,
      "language": "en"
    },
    {
      "text": "and lower values for\nmaybe the dog or car class",
      "start": 275.681,
      "duration": 2.669,
      "language": "en"
    },
    {
      "text": "indicate lower probabilities\nof those classes being present",
      "start": 278.35,
      "duration": 3.003,
      "language": "en"
    },
    {
      "text": "in the image. Also, so I think this point\nwas a little bit unclear",
      "start": 281.353,
      "duration": 5.211000000000013,
      "language": "en"
    },
    {
      "text": "last time that linear classification\nhas this interpretation",
      "start": 286.564,
      "duration": 3.645,
      "language": "en"
    },
    {
      "text": "as learning templates per class,",
      "start": 290.209,
      "duration": 2.216,
      "language": "en"
    },
    {
      "text": "where if you look at the\ndiagram on the lower left,",
      "start": 292.425,
      "duration": 2.703,
      "language": "en"
    },
    {
      "text": "you think that, so for\nevery pixel in the image,",
      "start": 295.128,
      "duration": 3.171,
      "language": "en"
    },
    {
      "text": "and for every one of our 10 classes,",
      "start": 298.299,
      "duration": 2.112,
      "language": "en"
    },
    {
      "text": "there exists some entry in this matrix W,",
      "start": 300.411,
      "duration": 2.833,
      "language": "en"
    },
    {
      "text": "telling us how much does that\npixel influence that class.",
      "start": 303.244,
      "duration": 4.11,
      "language": "en"
    },
    {
      "text": "So that means that each of\nthese rows in the matrix W",
      "start": 307.354,
      "duration": 3.062,
      "language": "en"
    },
    {
      "text": "ends up corresponding to\na template for the class.",
      "start": 310.416,
      "duration": 2.796,
      "language": "en"
    },
    {
      "text": "And if we take those rows and unravel,",
      "start": 313.212,
      "duration": 2.267,
      "language": "en"
    },
    {
      "text": "so each of those rows again corresponds",
      "start": 315.479,
      "duration": 2.245,
      "language": "en"
    },
    {
      "text": "to a weighting between the values of,",
      "start": 317.724,
      "duration": 2.816,
      "language": "en"
    },
    {
      "text": "between the pixel values of\nthe image and that class,",
      "start": 320.54,
      "duration": 2.811,
      "language": "en"
    },
    {
      "text": "so if we take that row and\nunravel it back into an image,",
      "start": 323.351,
      "duration": 2.895,
      "language": "en"
    },
    {
      "text": "then we can visualize the\nlearned template for each",
      "start": 326.246,
      "duration": 2.541,
      "language": "en"
    },
    {
      "text": "of these classes. We also had this interpretation\nof linear classification",
      "start": 328.787,
      "duration": 4.537000000000035,
      "language": "en"
    },
    {
      "text": "as learning linear decision\nboundaries between pixels",
      "start": 333.324,
      "duration": 2.875,
      "language": "en"
    },
    {
      "text": "in some high dimensional\nspace where the dimensions",
      "start": 336.199,
      "duration": 2.389,
      "language": "en"
    },
    {
      "text": "of the space correspond\nto the values of the pixel",
      "start": 338.588,
      "duration": 3.023,
      "language": "en"
    },
    {
      "text": "intensity values of the image.",
      "start": 341.611,
      "duration": 2.963,
      "language": "en"
    },
    {
      "text": "So this is kind of where\nwe left off last time.",
      "start": 344.574,
      "duration": 3.797,
      "language": "en"
    },
    {
      "text": "And so where we kind of\nstopped, where we ended up last",
      "start": 348.371,
      "duration": 3.244,
      "language": "en"
    },
    {
      "text": "time is we got this idea\nof a linear classifier,",
      "start": 351.615,
      "duration": 3.326,
      "language": "en"
    },
    {
      "text": "and we didn't talk about how\nto actually choose the W.",
      "start": 354.941,
      "duration": 3.413,
      "language": "en"
    },
    {
      "text": "How to actually use the training data to determine which value\nof W should be best.",
      "start": 358.354,
      "duration": 5.074000000000012,
      "language": "en"
    },
    {
      "text": "So kind of where we stopped off at is that for some setting\nof W, we can use this W",
      "start": 363.428,
      "duration": 5.663999999999987,
      "language": "en"
    },
    {
      "text": "to come up with 10 with our\nclass scores for any image.",
      "start": 369.092,
      "duration": 3.776,
      "language": "en"
    },
    {
      "text": "So and some of these class\nscores might be better or worse.",
      "start": 372.868,
      "duration": 3.529,
      "language": "en"
    },
    {
      "text": "So here in this simple example, we've shown maybe just a\ntraining data set of three images",
      "start": 376.397,
      "duration": 5.23599999999999,
      "language": "en"
    },
    {
      "text": "along with the 10 class scores\npredicted for some value of W",
      "start": 381.633,
      "duration": 3.751,
      "language": "en"
    },
    {
      "text": "for those images. And you can see that some\nof these scores are better",
      "start": 385.384,
      "duration": 3.262999999999977,
      "language": "en"
    },
    {
      "text": "or worse than others. So for example in the image\non the left, if you look up,",
      "start": 388.647,
      "duration": 4.497000000000014,
      "language": "en"
    },
    {
      "text": "it's actually a cat because you're a human and you can tell these things,",
      "start": 393.144,
      "duration": 3.579999999999984,
      "language": "en"
    },
    {
      "text": "but if we look at the\nassigned probabilities, cat,",
      "start": 396.724,
      "duration": 3.028,
      "language": "en"
    },
    {
      "text": "well not probabilities but scores,",
      "start": 399.752,
      "duration": 2.116,
      "language": "en"
    },
    {
      "text": "then the classifier maybe\nfor this setting of W",
      "start": 401.868,
      "duration": 2.368,
      "language": "en"
    },
    {
      "text": "gave the cat class a score\nof 2.9 for this image,",
      "start": 404.236,
      "duration": 4.646,
      "language": "en"
    },
    {
      "text": "whereas the frog class gave 3.78.",
      "start": 408.882,
      "duration": 2.936,
      "language": "en"
    },
    {
      "text": "So maybe the classifier\nis not doing not so good",
      "start": 411.818,
      "duration": 2.091,
      "language": "en"
    },
    {
      "text": "on this image, that's bad,\nwe wanted the true class",
      "start": 413.909,
      "duration": 2.327,
      "language": "en"
    },
    {
      "text": "to be actually the highest class score,",
      "start": 416.236,
      "duration": 2.484,
      "language": "en"
    },
    {
      "text": "whereas for some of these\nother examples, like the car",
      "start": 418.72,
      "duration": 2.189,
      "language": "en"
    },
    {
      "text": "for example, you see\nthat the automobile class",
      "start": 420.909,
      "duration": 2.62,
      "language": "en"
    },
    {
      "text": "has a score of six which is much higher than any of the others, so that's good.",
      "start": 423.529,
      "duration": 4.089999999999975,
      "language": "en"
    },
    {
      "text": "And the frog, the predicted\nscores are maybe negative four,",
      "start": 427.619,
      "duration": 3.814,
      "language": "en"
    },
    {
      "text": "which is much lower\nthan all the other ones,",
      "start": 431.433,
      "duration": 2.204,
      "language": "en"
    },
    {
      "text": "so that's actually bad. So this is kind of a hand wavy approach,",
      "start": 433.637,
      "duration": 3.69399999999996,
      "language": "en"
    },
    {
      "text": "just kind of looking at\nthe scores and eyeballing which ones are good\nand which ones are bad.",
      "start": 437.331,
      "duration": 4.1229999999999905,
      "language": "en"
    },
    {
      "text": "But to actually write\nalgorithms about these things",
      "start": 441.454,
      "duration": 2.156,
      "language": "en"
    },
    {
      "text": "and to actually to determine\nautomatically which W",
      "start": 443.61,
      "duration": 2.454,
      "language": "en"
    },
    {
      "text": "will be best, we need some\nway to quantify the badness",
      "start": 446.064,
      "duration": 3.596,
      "language": "en"
    },
    {
      "text": "of any particular W.",
      "start": 449.66,
      "duration": 2.172,
      "language": "en"
    },
    {
      "text": "And that's this function\nthat takes in a W,",
      "start": 451.832,
      "duration": 3.994,
      "language": "en"
    },
    {
      "text": "looks at the scores and then\ntells us how bad quantitatively",
      "start": 455.826,
      "duration": 3.457,
      "language": "en"
    },
    {
      "text": "is that W, is something that\nwe'll call a loss function.",
      "start": 459.283,
      "duration": 3.504,
      "language": "en"
    },
    {
      "text": "And in this lecture we'll\nsee a couple examples",
      "start": 462.787,
      "duration": 2.68,
      "language": "en"
    },
    {
      "text": "of different loss functions\nthat you can use for this image",
      "start": 465.467,
      "duration": 2.626,
      "language": "en"
    },
    {
      "text": "classification problem.",
      "start": 468.093,
      "duration": 2.489,
      "language": "en"
    },
    {
      "text": "So then once we've got this\nidea of a loss function,",
      "start": 470.582,
      "duration": 2.901,
      "language": "en"
    },
    {
      "text": "this allows us to quantify\nfor any given value of W,",
      "start": 473.483,
      "duration": 4.049,
      "language": "en"
    },
    {
      "text": "how good or bad is it? But then we actually need to find",
      "start": 477.532,
      "duration": 3.302000000000021,
      "language": "en"
    },
    {
      "text": "and come up with an efficient procedure for searching through the\nspace of all possible Ws",
      "start": 480.834,
      "duration": 4.73599999999999,
      "language": "en"
    },
    {
      "text": "and actually come up with\nwhat is the correct value",
      "start": 485.57,
      "duration": 3.364,
      "language": "en"
    },
    {
      "text": "of W that is the least bad,",
      "start": 488.934,
      "duration": 2.554,
      "language": "en"
    },
    {
      "text": "and this process will be\nan optimization procedure",
      "start": 491.488,
      "duration": 2.172,
      "language": "en"
    },
    {
      "text": "and we'll talk more about\nthat in this lecture.",
      "start": 493.66,
      "duration": 3.416,
      "language": "en"
    },
    {
      "text": "So I'm going to shrink\nthis example a little bit",
      "start": 497.076,
      "duration": 2.015,
      "language": "en"
    },
    {
      "text": "because 10 classes is\na little bit unwieldy.",
      "start": 499.091,
      "duration": 2.712,
      "language": "en"
    },
    {
      "text": "So we'll kind of work with\nthis tiny toy data set",
      "start": 501.803,
      "duration": 2.928,
      "language": "en"
    },
    {
      "text": "of three examples and\nthree classes going forward",
      "start": 504.731,
      "duration": 2.82,
      "language": "en"
    },
    {
      "text": "in this lecture.",
      "start": 507.551,
      "duration": 2.135,
      "language": "en"
    },
    {
      "text": "So again, in this example, the\ncat is maybe not so correctly",
      "start": 509.686,
      "duration": 3.953,
      "language": "en"
    },
    {
      "text": "classified, the car is correctly\nclassified, and the frog,",
      "start": 513.639,
      "duration": 4.768,
      "language": "en"
    },
    {
      "text": "this setting of W got this\nfrog image totally wrong,",
      "start": 518.407,
      "duration": 2.913,
      "language": "en"
    },
    {
      "text": "because the frog score is\nmuch lower than others.",
      "start": 521.32,
      "duration": 3.905,
      "language": "en"
    },
    {
      "text": "So to formalize this a little\nbit, usually when we talk",
      "start": 525.225,
      "duration": 2.539,
      "language": "en"
    },
    {
      "text": "about a loss function, we imagine that we have some training\ndata set of xs and ys,",
      "start": 527.764,
      "duration": 5.905999999999949,
      "language": "en"
    },
    {
      "text": "usually N examples of these\nwhere the xs are the inputs",
      "start": 533.67,
      "duration": 3.326,
      "language": "en"
    },
    {
      "text": "to the algorithm in the\nimage classification case,",
      "start": 536.996,
      "duration": 3.008,
      "language": "en"
    },
    {
      "text": "the xs would be the actually\npixel values of your images,",
      "start": 540.004,
      "duration": 3.858,
      "language": "en"
    },
    {
      "text": "and the ys will be the things\nyou want your algorithm",
      "start": 543.862,
      "duration": 2.345,
      "language": "en"
    },
    {
      "text": "to predict, we usually call\nthese the labels or the targets.",
      "start": 546.207,
      "duration": 3.523,
      "language": "en"
    },
    {
      "text": "So in the case of image classification,",
      "start": 549.73,
      "duration": 2.052,
      "language": "en"
    },
    {
      "text": "remember we're trying\nto categorize each image",
      "start": 551.782,
      "duration": 2.758,
      "language": "en"
    },
    {
      "text": "for CIFAR-10 to one of 10 categories,",
      "start": 554.54,
      "duration": 3.057,
      "language": "en"
    },
    {
      "text": "so the label y here will be an integer",
      "start": 557.597,
      "duration": 2.204,
      "language": "en"
    },
    {
      "text": "between one and 10 or\nmaybe between zero and nine",
      "start": 559.801,
      "duration": 3.147,
      "language": "en"
    },
    {
      "text": "depending on what programming\nlanguage you're using,",
      "start": 562.948,
      "duration": 2.266,
      "language": "en"
    },
    {
      "text": "but it'll be an integer telling you what is the correct category\nfor each one of those images x.",
      "start": 565.214,
      "duration": 5.855999999999881,
      "language": "en"
    },
    {
      "text": "And now our loss function\nwill denote L_i to denote the,",
      "start": 571.07,
      "duration": 4.214,
      "language": "en"
    },
    {
      "text": "so then we have this prediction function x",
      "start": 575.284,
      "duration": 2.409,
      "language": "en"
    },
    {
      "text": "which takes in our example\nx and our weight matrix W",
      "start": 577.693,
      "duration": 4.076,
      "language": "en"
    },
    {
      "text": "and makes some prediction for y, in the case of image classification",
      "start": 581.769,
      "duration": 3.466000000000008,
      "language": "en"
    },
    {
      "text": "these will be our 10 numbers.",
      "start": 585.235,
      "duration": 2.011,
      "language": "en"
    },
    {
      "text": "Then we'll define some loss function L_i",
      "start": 587.246,
      "duration": 3.492,
      "language": "en"
    },
    {
      "text": "which will take in the predicted scores",
      "start": 590.738,
      "duration": 2.662,
      "language": "en"
    },
    {
      "text": "coming out of the function f together with the true target or label Y",
      "start": 593.4,
      "duration": 4.203999999999951,
      "language": "en"
    },
    {
      "text": "and give us some quantitative\nvalue for how bad",
      "start": 597.604,
      "duration": 2.508,
      "language": "en"
    },
    {
      "text": "those predictions are for\nthat training example.",
      "start": 600.112,
      "duration": 3.198,
      "language": "en"
    },
    {
      "text": "And now the final loss L will\nbe the average of these losses",
      "start": 603.31,
      "duration": 3.617,
      "language": "en"
    },
    {
      "text": "summed over the entire data\nset over each of the N examples",
      "start": 606.927,
      "duration": 2.849,
      "language": "en"
    },
    {
      "text": "in our data set. So this is actually a\nvery general formulation,",
      "start": 609.776,
      "duration": 4.456000000000017,
      "language": "en"
    },
    {
      "text": "and actually extends even\nbeyond image classification.",
      "start": 614.232,
      "duration": 2.989,
      "language": "en"
    },
    {
      "text": "Kind of as we move forward\nand see other tasks,",
      "start": 617.221,
      "duration": 2.597,
      "language": "en"
    },
    {
      "text": "other examples of tasks and deep learning,",
      "start": 619.818,
      "duration": 2.305,
      "language": "en"
    },
    {
      "text": "the kind of generic setup\nis that for any task",
      "start": 622.123,
      "duration": 3.103,
      "language": "en"
    },
    {
      "text": "you have some xs and ys\nand you want to write down",
      "start": 625.226,
      "duration": 2.413,
      "language": "en"
    },
    {
      "text": "some loss function that\nquantifies exactly how happy",
      "start": 627.639,
      "duration": 3.21,
      "language": "en"
    },
    {
      "text": "you are with your particular\nparameter settings W",
      "start": 630.849,
      "duration": 3.486,
      "language": "en"
    },
    {
      "text": "and then you'll eventually\nsearch over the space of W",
      "start": 634.335,
      "duration": 2.28,
      "language": "en"
    },
    {
      "text": "to find the W that minimizes\nthe loss on your training data.",
      "start": 636.615,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "So as a first example of\na concrete loss function",
      "start": 641.881,
      "duration": 4.449,
      "language": "en"
    },
    {
      "text": "that is a nice thing to work\nwith in image classification,",
      "start": 646.33,
      "duration": 3.792,
      "language": "en"
    },
    {
      "text": "we'll talk about the multi-class SVM loss.",
      "start": 650.122,
      "duration": 3.433,
      "language": "en"
    },
    {
      "text": "You may have seen the binary\nSVM, our support vector",
      "start": 653.555,
      "duration": 3.514,
      "language": "en"
    },
    {
      "text": "machine in CS 229 and the multiclass SVM",
      "start": 657.069,
      "duration": 3.333,
      "language": "en"
    },
    {
      "text": "is a generalization of that\nto handle multiple classes.",
      "start": 661.462,
      "duration": 4.601,
      "language": "en"
    },
    {
      "text": "In the binary SVM case as\nyou may have seen in 229,",
      "start": 666.063,
      "duration": 4.184,
      "language": "en"
    },
    {
      "text": "you only had two classes, each example x",
      "start": 670.247,
      "duration": 2.35,
      "language": "en"
    },
    {
      "text": "was going to be classified\nas either positive or negative example,",
      "start": 672.597,
      "duration": 3.3619999999999663,
      "language": "en"
    },
    {
      "text": "but now we have 10 categories,\nso we need to generalize",
      "start": 675.959,
      "duration": 2.456,
      "language": "en"
    },
    {
      "text": "this notion to handle multiple classes.",
      "start": 678.415,
      "duration": 3.147,
      "language": "en"
    },
    {
      "text": "So this loss function has kind\nof a funny functional form,",
      "start": 681.562,
      "duration": 4.092,
      "language": "en"
    },
    {
      "text": "so we'll walk through it in a bit more, in quite a bit of detail over\nthe next couple of slides.",
      "start": 685.654,
      "duration": 4.947000000000003,
      "language": "en"
    },
    {
      "text": "But what this is saying\nis that the loss L_i",
      "start": 690.601,
      "duration": 3.293,
      "language": "en"
    },
    {
      "text": "for any individual example,\nthe way we'll compute it",
      "start": 693.894,
      "duration": 2.49,
      "language": "en"
    },
    {
      "text": "is we're going to perform a sum\nover all of the categories, Y,",
      "start": 696.384,
      "duration": 4.714,
      "language": "en"
    },
    {
      "text": "except for the true category, Y_i,",
      "start": 701.098,
      "duration": 3.075,
      "language": "en"
    },
    {
      "text": "so we're going to sum over\nall the incorrect categories,",
      "start": 704.173,
      "duration": 2.831,
      "language": "en"
    },
    {
      "text": "and then we're going to compare the score",
      "start": 707.004,
      "duration": 2.238,
      "language": "en"
    },
    {
      "text": "of the correct category, and the score of the incorrect category,",
      "start": 709.242,
      "duration": 4.067000000000121,
      "language": "en"
    },
    {
      "text": "and now if the score\nfor the correct category",
      "start": 713.309,
      "duration": 2.57,
      "language": "en"
    },
    {
      "text": "is greater than the score\nof the incorrect category,",
      "start": 715.879,
      "duration": 4.51,
      "language": "en"
    },
    {
      "text": "greater than the incorrect\nscore by some safety margin",
      "start": 720.389,
      "duration": 4.225,
      "language": "en"
    },
    {
      "text": "that we set to one, if that's\nthe case that means that",
      "start": 724.614,
      "duration": 3.335,
      "language": "en"
    },
    {
      "text": "the true score is much, or the\nscore for the true category",
      "start": 727.949,
      "duration": 4.191,
      "language": "en"
    },
    {
      "text": "is if it's much larger than\nany of the false categories,",
      "start": 732.14,
      "duration": 3.321,
      "language": "en"
    },
    {
      "text": "then we'll get a loss of zero.",
      "start": 735.461,
      "duration": 3.002,
      "language": "en"
    },
    {
      "text": "And we'll sum this up over all\nof the incorrect categories",
      "start": 738.463,
      "duration": 4.04,
      "language": "en"
    },
    {
      "text": "for our image and this\nwill give us our final loss",
      "start": 742.503,
      "duration": 2.751,
      "language": "en"
    },
    {
      "text": "for this one example in the data set.",
      "start": 745.254,
      "duration": 2.701,
      "language": "en"
    },
    {
      "text": "And again we'll take\nthe average of this loss",
      "start": 747.955,
      "duration": 2.556,
      "language": "en"
    },
    {
      "text": "over the whole training data set.",
      "start": 750.511,
      "duration": 2.283,
      "language": "en"
    },
    {
      "text": "So this kind of like if then\nstatement, like if the true",
      "start": 752.794,
      "duration": 4.943,
      "language": "en"
    },
    {
      "text": "class score is much\nlarger than the others,",
      "start": 757.737,
      "duration": 4.326,
      "language": "en"
    },
    {
      "text": "this kind of if then\nformulation we often compactify",
      "start": 762.063,
      "duration": 3.897,
      "language": "en"
    },
    {
      "text": "into this single max of zero\nS_j minus S_Yi plus one thing,",
      "start": 765.96,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "but I always find that notation\na little bit confusing,",
      "start": 771.296,
      "duration": 2.553,
      "language": "en"
    },
    {
      "text": "and it always helps me to write it out in this\nsort of case based notation",
      "start": 773.849,
      "duration": 3.629000000000019,
      "language": "en"
    },
    {
      "text": "to figure out exactly\nwhat the two cases are and what's going on.",
      "start": 777.478,
      "duration": 4.2420000000000755,
      "language": "en"
    },
    {
      "text": "And by the way, this\nstyle of loss function",
      "start": 781.72,
      "duration": 2.869,
      "language": "en"
    },
    {
      "text": "where we take max of zero\nand some other quantity",
      "start": 784.589,
      "duration": 3.016,
      "language": "en"
    },
    {
      "text": "is often referred to as\nsome type of a hinge loss,",
      "start": 787.605,
      "duration": 3.322,
      "language": "en"
    },
    {
      "text": "and this name comes from\nthe shape of the graph",
      "start": 790.927,
      "duration": 3.075,
      "language": "en"
    },
    {
      "text": "when you go and plot it, so here the x axis corresponds to the S_Yi,",
      "start": 794.002,
      "duration": 4.925000000000068,
      "language": "en"
    },
    {
      "text": "that is the score of the\ntrue class for some training",
      "start": 799.903,
      "duration": 3.172,
      "language": "en"
    },
    {
      "text": "example, and now the y axis is the loss,",
      "start": 803.075,
      "duration": 3.078,
      "language": "en"
    },
    {
      "text": "and you can see that as the\nscore for the true category",
      "start": 806.153,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "for this example increases, then the loss will go down linearly",
      "start": 811.323,
      "duration": 3.6510000000000673,
      "language": "en"
    },
    {
      "text": "until we get to above this safety margin,",
      "start": 814.974,
      "duration": 3.631,
      "language": "en"
    },
    {
      "text": "after which the loss will be zero",
      "start": 818.605,
      "duration": 2.43,
      "language": "en"
    },
    {
      "text": "because we've already correctly\nclassified this example.",
      "start": 821.035,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "So let's, oh, question?",
      "start": 826.35,
      "duration": 2.286,
      "language": "en"
    },
    {
      "text": "- [Student] Sorry, in terms of notation",
      "start": 828.636,
      "duration": 2.628,
      "language": "en"
    },
    {
      "text": "what is S underscore Yi? Is that your right score?",
      "start": 831.264,
      "duration": 4.706000000000017,
      "language": "en"
    },
    {
      "text": "- Yeah, so the question is in terms of notation,\nwhat is S and what is SYI",
      "start": 835.97,
      "duration": 4.926999999999907,
      "language": "en"
    },
    {
      "text": "in particular, so the Ss\nare the predicted scores",
      "start": 840.897,
      "duration": 3.809,
      "language": "en"
    },
    {
      "text": "for the classes that are\ncoming out of the classifier.",
      "start": 844.706,
      "duration": 3.34,
      "language": "en"
    },
    {
      "text": "So if one is the cat class and\ntwo is the dog class then S1",
      "start": 848.046,
      "duration": 3.688,
      "language": "en"
    },
    {
      "text": "and S2 would be the cat and\ndog scores respectively.",
      "start": 851.734,
      "duration": 3.008,
      "language": "en"
    },
    {
      "text": "And remember we said that Yi\nwas the category of the ground",
      "start": 854.742,
      "duration": 3.492,
      "language": "en"
    },
    {
      "text": "truth label for the example\nwhich is some integer.",
      "start": 858.234,
      "duration": 3.622,
      "language": "en"
    },
    {
      "text": "So then S sub Y sub i, sorry\nfor the double subscript,",
      "start": 861.856,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "that corresponds to the\nscore of the true class",
      "start": 866.857,
      "duration": 2.909,
      "language": "en"
    },
    {
      "text": "for the i-th example in the training set.",
      "start": 869.766,
      "duration": 3.717,
      "language": "en"
    },
    {
      "text": "Question? - [Student] So what\nexactly is this computing?",
      "start": 873.483,
      "duration": 3.2380000000000564,
      "language": "en"
    },
    {
      "text": "- Yeah the question is what\nexactly is this computing here?",
      "start": 876.721,
      "duration": 2.756,
      "language": "en"
    },
    {
      "text": "It's a little bit funny, I\nthink it will become more clear",
      "start": 879.477,
      "duration": 2.948,
      "language": "en"
    },
    {
      "text": "when we walk through an explicit\nexample, but in some sense",
      "start": 882.425,
      "duration": 2.965,
      "language": "en"
    },
    {
      "text": "what this loss is saying is\nthat we are happy if the true",
      "start": 885.39,
      "duration": 4.085,
      "language": "en"
    },
    {
      "text": "score is much higher than\nall the other scores.",
      "start": 889.475,
      "duration": 3.309,
      "language": "en"
    },
    {
      "text": "It needs to be higher\nthan all the other scores",
      "start": 892.784,
      "duration": 2.609,
      "language": "en"
    },
    {
      "text": "by some safety margin,\nand if the true score",
      "start": 895.393,
      "duration": 3.771,
      "language": "en"
    },
    {
      "text": "is not high enough, greater\nthan any of the other scores,",
      "start": 899.164,
      "duration": 3.528,
      "language": "en"
    },
    {
      "text": "then we will incur some\nloss and that would be bad.",
      "start": 902.692,
      "duration": 4.642,
      "language": "en"
    },
    {
      "text": "So this might make a little bit more sense",
      "start": 907.334,
      "duration": 2.123,
      "language": "en"
    },
    {
      "text": "if we walk through an explicit example for this tiny three example data set.",
      "start": 909.457,
      "duration": 4.566000000000031,
      "language": "en"
    },
    {
      "text": "So here remember I've sort\nof removed the case space",
      "start": 914.023,
      "duration": 2.873,
      "language": "en"
    },
    {
      "text": "notation and just switching\nback to the zero one notation,",
      "start": 916.896,
      "duration": 3.417,
      "language": "en"
    },
    {
      "text": "and now if we look at, if we think about computing\nthis multi-class SVM loss",
      "start": 920.313,
      "duration": 5.030999999999949,
      "language": "en"
    },
    {
      "text": "for just this first training example on the left, then remember\nwe're going to loop over",
      "start": 925.344,
      "duration": 4.213999999999942,
      "language": "en"
    },
    {
      "text": "all of the incorrect\nclasses, so for this example,",
      "start": 929.558,
      "duration": 3.155,
      "language": "en"
    },
    {
      "text": "cat is the correct class, so\nwe're going to loop over the car",
      "start": 932.713,
      "duration": 3.009,
      "language": "en"
    },
    {
      "text": "and frog classes, and now for\ncar, we're going to compare the,",
      "start": 935.722,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "we're going to look at the car\nscore, 5.1, minus the cat score,",
      "start": 941.888,
      "duration": 3.527,
      "language": "en"
    },
    {
      "text": "3.2 plus one, when we're\ncomparing cat and car we expect",
      "start": 945.415,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "to incur some loss here because\nthe car score is greater",
      "start": 950.769,
      "duration": 3.158,
      "language": "en"
    },
    {
      "text": "than the cat score which is bad.",
      "start": 953.927,
      "duration": 2.007,
      "language": "en"
    },
    {
      "text": "So for this one class,\nfor this one example,",
      "start": 955.934,
      "duration": 4.019,
      "language": "en"
    },
    {
      "text": "we'll incur a loss of 2.9,",
      "start": 959.953,
      "duration": 2.18,
      "language": "en"
    },
    {
      "text": "and then when we go and\ncompare the cat score",
      "start": 962.133,
      "duration": 2.054,
      "language": "en"
    },
    {
      "text": "and the frog score we see that cat is 3.2,",
      "start": 964.187,
      "duration": 3.125,
      "language": "en"
    },
    {
      "text": "frog is minus 1.7, so cat is more than one greater than frog,",
      "start": 967.312,
      "duration": 5.048999999999978,
      "language": "en"
    },
    {
      "text": "which means that between these two classes",
      "start": 972.361,
      "duration": 3.167,
      "language": "en"
    },
    {
      "text": "we incur zero loss. So then the multiclass SVM\nloss for this training example",
      "start": 975.528,
      "duration": 6.019999999999982,
      "language": "en"
    },
    {
      "text": "will be the sum of the losses\nacross each of these pairs",
      "start": 981.548,
      "duration": 2.346,
      "language": "en"
    },
    {
      "text": "of classes, which will be\n2.9 plus zero which is 2.9.",
      "start": 983.894,
      "duration": 4.018,
      "language": "en"
    },
    {
      "text": "Which is sort of saying that\n2.9 is a quantitative measure",
      "start": 987.912,
      "duration": 3.368,
      "language": "en"
    },
    {
      "text": "of how much our classifier screwed up on this one training example.",
      "start": 991.28,
      "duration": 4.087000000000103,
      "language": "en"
    },
    {
      "text": "And then if we repeat this procedure for this next car image, then\nagain the true class is car,",
      "start": 996.595,
      "duration": 5.778999999999996,
      "language": "en"
    },
    {
      "text": "so we're going to iterate\nover all the other categories",
      "start": 1002.374,
      "duration": 2.477,
      "language": "en"
    },
    {
      "text": "when we compare the car and the cat score,",
      "start": 1004.851,
      "duration": 3.154,
      "language": "en"
    },
    {
      "text": "we see that car is more\nthan one greater than cat",
      "start": 1008.005,
      "duration": 2.889,
      "language": "en"
    },
    {
      "text": "so we get no loss here. When we compare car and frog, we again see",
      "start": 1010.894,
      "duration": 4.125,
      "language": "en"
    },
    {
      "text": "that the car score is more\nthan one greater than frog,",
      "start": 1015.019,
      "duration": 3.338,
      "language": "en"
    },
    {
      "text": "so we get again no loss\nhere, and our total loss",
      "start": 1018.357,
      "duration": 2.427,
      "language": "en"
    },
    {
      "text": "for this training example is zero.",
      "start": 1020.784,
      "duration": 3.009,
      "language": "en"
    },
    {
      "text": "And now I think you hopefully\nget the picture by now, but,",
      "start": 1023.793,
      "duration": 2.772,
      "language": "en"
    },
    {
      "text": "if you go look at frog, now\nfrog, we again compare frog",
      "start": 1026.565,
      "duration": 3.286,
      "language": "en"
    },
    {
      "text": "and cat, incur quite a lot of\nloss because the frog score",
      "start": 1029.851,
      "duration": 2.715,
      "language": "en"
    },
    {
      "text": "is very low, compare frog\nand car, incur a lot of loss",
      "start": 1032.566,
      "duration": 3.13,
      "language": "en"
    },
    {
      "text": "because the score is very low,\nand then our loss for this",
      "start": 1035.696,
      "duration": 3.468,
      "language": "en"
    },
    {
      "text": "example is 12.9. And then our final loss\nfor the entire data set",
      "start": 1039.164,
      "duration": 5.494000000000142,
      "language": "en"
    },
    {
      "text": "is the average of these losses across the different examples,",
      "start": 1044.658,
      "duration": 2.7210000000000036,
      "language": "en"
    },
    {
      "text": "so when you sum those out\nit comes to about 5.3.",
      "start": 1047.379,
      "duration": 2.698,
      "language": "en"
    },
    {
      "text": "So then it's sort of, this\nis our quantitative measure",
      "start": 1050.077,
      "duration": 2.253,
      "language": "en"
    },
    {
      "text": "that our classifier is\n5.3 bad on this data set.",
      "start": 1052.33,
      "duration": 3.399,
      "language": "en"
    },
    {
      "text": "Is there a question? - [Student] How do you\nchoose the plus one?",
      "start": 1055.729,
      "duration": 4.222999999999956,
      "language": "en"
    },
    {
      "text": "- Yeah, the question is how\ndo you choose the plus one?",
      "start": 1059.952,
      "duration": 2.768,
      "language": "en"
    },
    {
      "text": "That's actually a really great question, it seems like kind of an\narbitrary choice here,",
      "start": 1062.72,
      "duration": 4.741999999999962,
      "language": "en"
    },
    {
      "text": "it's the only constant that\nappears in the loss function",
      "start": 1067.462,
      "duration": 2.116,
      "language": "en"
    },
    {
      "text": "and that seems to offend\nyour aesthetic sensibilities",
      "start": 1069.578,
      "duration": 2.172,
      "language": "en"
    },
    {
      "text": "a bit maybe. But it turns out that this is somewhat",
      "start": 1071.75,
      "duration": 2.900000000000091,
      "language": "en"
    },
    {
      "text": "of an arbitrary choice,\nbecause we don't actually care",
      "start": 1074.65,
      "duration": 4.019,
      "language": "en"
    },
    {
      "text": "about the absolute values of the scores",
      "start": 1078.669,
      "duration": 2.436,
      "language": "en"
    },
    {
      "text": "in this loss function, we only care",
      "start": 1081.105,
      "duration": 2.49,
      "language": "en"
    },
    {
      "text": "about the relative differences\nbetween the scores.",
      "start": 1083.595,
      "duration": 2.594,
      "language": "en"
    },
    {
      "text": "We only care that the correct score is much greater than the incorrect scores.",
      "start": 1086.189,
      "duration": 3.5550000000000637,
      "language": "en"
    },
    {
      "text": "So in fact if you imagine\nscaling up your whole W",
      "start": 1089.744,
      "duration": 2.596,
      "language": "en"
    },
    {
      "text": "up or down, then it kind\nof rescales all the scores",
      "start": 1092.34,
      "duration": 3.202,
      "language": "en"
    },
    {
      "text": "correspondingly and if you kind\nof work through the details",
      "start": 1095.542,
      "duration": 2.797,
      "language": "en"
    },
    {
      "text": "and there's a detailed derivation\nof this in the course notes",
      "start": 1098.339,
      "duration": 3.269,
      "language": "en"
    },
    {
      "text": "online, you find this choice\nof one actually doesn't matter.",
      "start": 1101.608,
      "duration": 3.554,
      "language": "en"
    },
    {
      "text": "That this free parameter\nof one kind of washes out",
      "start": 1105.162,
      "duration": 3.295,
      "language": "en"
    },
    {
      "text": "and is canceled with this scale, like the overall setting\nof the scale in W.",
      "start": 1108.457,
      "duration": 4.968000000000075,
      "language": "en"
    },
    {
      "text": "And again, check the course\nnotes for a bit more detail",
      "start": 1113.425,
      "duration": 2.031,
      "language": "en"
    },
    {
      "text": "on that. So then I think it's\nkind of useful to think",
      "start": 1115.456,
      "duration": 5.718000000000075,
      "language": "en"
    },
    {
      "text": "about a couple different\nquestions to try to understand",
      "start": 1121.174,
      "duration": 2.145,
      "language": "en"
    },
    {
      "text": "intuitively what this loss is doing.",
      "start": 1123.319,
      "duration": 2.855,
      "language": "en"
    },
    {
      "text": "So the first question is what's\ngoing to happen to the loss",
      "start": 1126.174,
      "duration": 3.341,
      "language": "en"
    },
    {
      "text": "if we change the scores of the\ncar image just a little bit?",
      "start": 1129.515,
      "duration": 4.634,
      "language": "en"
    },
    {
      "text": "Any ideas? Everyone's too scared to ask a question?",
      "start": 1134.149,
      "duration": 6.507000000000062,
      "language": "en"
    },
    {
      "text": "Answer? [student speaking faintly]",
      "start": 1140.656,
      "duration": 4.416000000000167,
      "language": "en"
    },
    {
      "text": "- Yeah, so the answer is\nthat if we jiggle the scores",
      "start": 1147.783,
      "duration": 3.03,
      "language": "en"
    },
    {
      "text": "for this car image a little\nbit, the loss will not change.",
      "start": 1150.813,
      "duration": 3.32,
      "language": "en"
    },
    {
      "text": "So the SVM loss, remember,\nthe only thing it cares",
      "start": 1154.133,
      "duration": 2.293,
      "language": "en"
    },
    {
      "text": "about is getting the correct\nscore to be greater than one",
      "start": 1156.426,
      "duration": 3.447,
      "language": "en"
    },
    {
      "text": "more than the incorrect\nscores, but in this case,",
      "start": 1159.873,
      "duration": 2.845,
      "language": "en"
    },
    {
      "text": "the car score is already quite\na bit larger than the others,",
      "start": 1162.718,
      "duration": 3.588,
      "language": "en"
    },
    {
      "text": "so if the scores for this\nclass changed for this example",
      "start": 1166.306,
      "duration": 2.542,
      "language": "en"
    },
    {
      "text": "changed just a little\nbit, this margin of one",
      "start": 1168.848,
      "duration": 2.617,
      "language": "en"
    },
    {
      "text": "will still be retained and\nthe loss will not change,",
      "start": 1171.465,
      "duration": 2.605,
      "language": "en"
    },
    {
      "text": "we'll still get zero loss.",
      "start": 1174.07,
      "duration": 2.167,
      "language": "en"
    },
    {
      "text": "The next question, what's\nthe min and max possible loss",
      "start": 1177.67,
      "duration": 2.447,
      "language": "en"
    },
    {
      "text": "for SVM? [student speaking faintly]",
      "start": 1180.117,
      "duration": 5.128000000000156,
      "language": "en"
    },
    {
      "text": "Oh I hear some murmurs. So the minimum loss is zero,\nbecause if you can imagine that",
      "start": 1185.245,
      "duration": 4.929000000000087,
      "language": "en"
    },
    {
      "text": "across all the classes, if\nour correct score was much",
      "start": 1190.174,
      "duration": 2.644,
      "language": "en"
    },
    {
      "text": "larger then we'll incur zero\nloss across all the classes",
      "start": 1192.818,
      "duration": 3.515,
      "language": "en"
    },
    {
      "text": "and it will be zero, and if you think back to this\nhinge loss plot that we had,",
      "start": 1196.333,
      "duration": 5.1189999999999145,
      "language": "en"
    },
    {
      "text": "then you can see that if the correct score",
      "start": 1201.452,
      "duration": 2.843,
      "language": "en"
    },
    {
      "text": "goes very, very negative,\nthen we could incur",
      "start": 1204.295,
      "duration": 2.697,
      "language": "en"
    },
    {
      "text": "potentially infinite loss. So the min is zero and\nthe max is infinity.",
      "start": 1206.992,
      "duration": 5.346000000000004,
      "language": "en"
    },
    {
      "text": "Another question, sort of when\nyou initialize these things",
      "start": 1212.338,
      "duration": 2.889,
      "language": "en"
    },
    {
      "text": "and start training from scratch, usually you kind of initialize W",
      "start": 1215.227,
      "duration": 3.5879999999999654,
      "language": "en"
    },
    {
      "text": "with some small random values,\nas a result your scores",
      "start": 1218.815,
      "duration": 3.227,
      "language": "en"
    },
    {
      "text": "tend to be sort of small\nuniform random values",
      "start": 1222.042,
      "duration": 2.7,
      "language": "en"
    },
    {
      "text": "at the beginning of training. And then the question is\nthat if all of your Ss,",
      "start": 1224.742,
      "duration": 3.984000000000151,
      "language": "en"
    },
    {
      "text": "if all of the scores\nare approximately zero",
      "start": 1228.726,
      "duration": 2.138,
      "language": "en"
    },
    {
      "text": "and approximately equal, then what kind of loss do you expect",
      "start": 1230.864,
      "duration": 2.7509999999999764,
      "language": "en"
    },
    {
      "text": "when you're using multiclass SVM?",
      "start": 1233.615,
      "duration": 2.926,
      "language": "en"
    },
    {
      "text": "- [Student] Number of classes minus one. - Yeah, so the answer is\nnumber of classes minus one,",
      "start": 1236.541,
      "duration": 6.70699999999988,
      "language": "en"
    },
    {
      "text": "because remember that\nif we're looping over",
      "start": 1243.248,
      "duration": 3.423,
      "language": "en"
    },
    {
      "text": "all of the incorrect classes,\nso we're looping over",
      "start": 1246.671,
      "duration": 2.354,
      "language": "en"
    },
    {
      "text": "C minus one classes, within\neach of those classes",
      "start": 1249.025,
      "duration": 3.264,
      "language": "en"
    },
    {
      "text": "the two Ss will be about the same,",
      "start": 1252.289,
      "duration": 2.249,
      "language": "en"
    },
    {
      "text": "so we'll get a loss of one because of the margin and\nwe'll get C minus one.",
      "start": 1254.538,
      "duration": 3.798000000000002,
      "language": "en"
    },
    {
      "text": "So this is actually kind\nof useful because when you,",
      "start": 1258.336,
      "duration": 2.823,
      "language": "en"
    },
    {
      "text": "this is a useful debugging strategy when you're using these things,",
      "start": 1261.159,
      "duration": 2.883999999999787,
      "language": "en"
    },
    {
      "text": "that when you start off training, you should think about what\nyou expect your loss to be,",
      "start": 1264.043,
      "duration": 4.839000000000169,
      "language": "en"
    },
    {
      "text": "and if the loss you actually\nsee at the start of training",
      "start": 1268.882,
      "duration": 2.849,
      "language": "en"
    },
    {
      "text": "at that first iteration is\nnot equal to C minus one",
      "start": 1271.731,
      "duration": 2.853,
      "language": "en"
    },
    {
      "text": "in this case, that means you probably have\na bug and you should go check",
      "start": 1274.584,
      "duration": 2.56899999999996,
      "language": "en"
    },
    {
      "text": "your code, so this is actually\nkind of a useful thing",
      "start": 1277.153,
      "duration": 2.302,
      "language": "en"
    },
    {
      "text": "to be checking in practice.",
      "start": 1279.455,
      "duration": 2.25,
      "language": "en"
    },
    {
      "text": "Another question, what happens\nif, so I said we're summing",
      "start": 1282.686,
      "duration": 3.366,
      "language": "en"
    },
    {
      "text": "an SVM over the incorrect\nclasses, what happens if the sum",
      "start": 1286.052,
      "duration": 4.758,
      "language": "en"
    },
    {
      "text": "is also over the correct class if we just go over everything?",
      "start": 1290.81,
      "duration": 4.313000000000102,
      "language": "en"
    },
    {
      "text": "- [Student] The loss increases by one. - Yeah, so the answer is that\nthe loss increases by one.",
      "start": 1295.123,
      "duration": 5.002999999999929,
      "language": "en"
    },
    {
      "text": "And I think the reason\nthat we do this in practice",
      "start": 1300.126,
      "duration": 2.603,
      "language": "en"
    },
    {
      "text": "is because normally loss of\nzero is kind of, has this nice",
      "start": 1302.729,
      "duration": 3.123,
      "language": "en"
    },
    {
      "text": "interpretation that\nyou're not losing at all,",
      "start": 1305.852,
      "duration": 2.304,
      "language": "en"
    },
    {
      "text": "so that's nice, so I think your answers",
      "start": 1308.156,
      "duration": 4.007,
      "language": "en"
    },
    {
      "text": "wouldn't really change, you would end up finding\nthe same classifier",
      "start": 1312.163,
      "duration": 3.1490000000001146,
      "language": "en"
    },
    {
      "text": "if you actually looped\nover all the categories, but if just by conventions\nwe omit the correct class",
      "start": 1315.312,
      "duration": 5.467000000000098,
      "language": "en"
    },
    {
      "text": "so that our minimum loss is zero.",
      "start": 1320.779,
      "duration": 2.75,
      "language": "en"
    },
    {
      "text": "So another question, what if we used mean instead of sum here?",
      "start": 1325.731,
      "duration": 3.076999999999998,
      "language": "en"
    },
    {
      "text": "- [Student] Doesn't change. - Yeah, the answer is\nthat it doesn't change.",
      "start": 1330.743,
      "duration": 3.133000000000038,
      "language": "en"
    },
    {
      "text": "So the number of classes is\ngoing to be fixed ahead of time",
      "start": 1333.876,
      "duration": 2.712,
      "language": "en"
    },
    {
      "text": "when we select our data set,\nso that's just rescaling",
      "start": 1336.588,
      "duration": 2.287,
      "language": "en"
    },
    {
      "text": "the whole loss function by a constant,",
      "start": 1338.875,
      "duration": 2.425,
      "language": "en"
    },
    {
      "text": "so it doesn't really matter,\nit'll sort of wash out with all the other scale things",
      "start": 1341.3,
      "duration": 3.4909999999999854,
      "language": "en"
    },
    {
      "text": "because we don't actually\ncare about the true values of the scores, or the\ntrue value of the loss",
      "start": 1344.791,
      "duration": 4.3400000000001455,
      "language": "en"
    },
    {
      "text": "for that matter. So now here's another\nexample, what if we change",
      "start": 1349.131,
      "duration": 4.378999999999905,
      "language": "en"
    },
    {
      "text": "this loss formulation and we\nactually added a square term",
      "start": 1353.51,
      "duration": 3.225,
      "language": "en"
    },
    {
      "text": "on top of this max? Would this end up being the same problem",
      "start": 1356.735,
      "duration": 4.1310000000000855,
      "language": "en"
    },
    {
      "text": "or would this be a different\nclassification algorithm?",
      "start": 1360.866,
      "duration": 2.902,
      "language": "en"
    },
    {
      "text": "- [Student] Different. - Yes, this would be different.",
      "start": 1363.768,
      "duration": 2.2450000000001182,
      "language": "en"
    },
    {
      "text": "So here the idea is that\nwe're kind of changing",
      "start": 1366.013,
      "duration": 2.083,
      "language": "en"
    },
    {
      "text": "the trade-offs between good and badness in kind of a nonlinear way,",
      "start": 1368.096,
      "duration": 3.606999999999971,
      "language": "en"
    },
    {
      "text": "so this would end up actually computing a different loss function.",
      "start": 1371.703,
      "duration": 3.3609999999998763,
      "language": "en"
    },
    {
      "text": "This idea of a squared hinge\nloss actually does get used",
      "start": 1375.064,
      "duration": 3.032,
      "language": "en"
    },
    {
      "text": "sometimes in practice, so\nthat's kind of another trick",
      "start": 1378.096,
      "duration": 2.619,
      "language": "en"
    },
    {
      "text": "to have in your bag when you're making up your own loss functions\nfor your own problems.",
      "start": 1380.715,
      "duration": 5.151000000000067,
      "language": "en"
    },
    {
      "text": "So now you'll end up,\noh, was there a question?",
      "start": 1385.866,
      "duration": 2.765,
      "language": "en"
    },
    {
      "text": "- [Student] Why would\nyou use a squared loss instead of a non-squared loss?",
      "start": 1388.631,
      "duration": 3.7649999999998727,
      "language": "en"
    },
    {
      "text": "- Yeah, so the question is\nwhy would you ever consider",
      "start": 1392.396,
      "duration": 2.422,
      "language": "en"
    },
    {
      "text": "using a squared loss instead\nof a non-squared loss?",
      "start": 1394.818,
      "duration": 2.742,
      "language": "en"
    },
    {
      "text": "And the whole point of a loss function is to kind of quantify how\nbad are different mistakes.",
      "start": 1397.56,
      "duration": 5.520999999999958,
      "language": "en"
    },
    {
      "text": "And if the classifier is making\ndifferent sorts of mistakes,",
      "start": 1403.081,
      "duration": 2.862,
      "language": "en"
    },
    {
      "text": "how do we weigh off the\ndifferent trade-offs between different types\nof mistakes the classifier",
      "start": 1405.943,
      "duration": 3.7970000000000255,
      "language": "en"
    },
    {
      "text": "might make? So if you're using a squared loss,",
      "start": 1409.74,
      "duration": 3.3250000000000455,
      "language": "en"
    },
    {
      "text": "that sort of says that things\nthat are very, very bad",
      "start": 1413.065,
      "duration": 4.106,
      "language": "en"
    },
    {
      "text": "are now going to be squared bad so that's like really, really bad,",
      "start": 1417.171,
      "duration": 2.8209999999999127,
      "language": "en"
    },
    {
      "text": "like we don't want anything that's totally\ncatastrophically misclassified,",
      "start": 1419.992,
      "duration": 4.43100000000004,
      "language": "en"
    },
    {
      "text": "whereas if you're using this hinge loss,",
      "start": 1424.423,
      "duration": 3.698,
      "language": "en"
    },
    {
      "text": "we don't actually care between\nbeing a little bit wrong",
      "start": 1428.121,
      "duration": 2.235,
      "language": "en"
    },
    {
      "text": "and being a lot wrong, being\na lot wrong kind of like,",
      "start": 1430.356,
      "duration": 2.997,
      "language": "en"
    },
    {
      "text": "if an example is a lot\nwrong, and we increase it",
      "start": 1433.353,
      "duration": 2.787,
      "language": "en"
    },
    {
      "text": "and make it a little bit less wrong, that's kind of the same\ngoodness as an example",
      "start": 1436.14,
      "duration": 4.268000000000029,
      "language": "en"
    },
    {
      "text": "which was only a little bit\nwrong and then increasing it",
      "start": 1440.408,
      "duration": 2.995,
      "language": "en"
    },
    {
      "text": "to be a little bit more right. So that's a little bit hand wavy,",
      "start": 1443.403,
      "duration": 3.6649999999999636,
      "language": "en"
    },
    {
      "text": "but this idea of using\na linear versus a square",
      "start": 1447.068,
      "duration": 2.517,
      "language": "en"
    },
    {
      "text": "is a way to quantify how much we care",
      "start": 1449.585,
      "duration": 2.413,
      "language": "en"
    },
    {
      "text": "about different categories of errors.",
      "start": 1451.998,
      "duration": 2.399,
      "language": "en"
    },
    {
      "text": "And this is definitely something\nthat you should think about when you're actually applying\nthese things in practice,",
      "start": 1454.397,
      "duration": 4.341000000000122,
      "language": "en"
    },
    {
      "text": "because the loss function is the way",
      "start": 1458.738,
      "duration": 2.307,
      "language": "en"
    },
    {
      "text": "that you tell your algorithm\nwhat types of errors",
      "start": 1461.045,
      "duration": 2.525,
      "language": "en"
    },
    {
      "text": "you care about and what types of errors it should trade off against.",
      "start": 1463.57,
      "duration": 3.497000000000071,
      "language": "en"
    },
    {
      "text": "So that's actually super\nimportant in practice depending on your application.",
      "start": 1467.067,
      "duration": 4.1400000000001,
      "language": "en"
    },
    {
      "text": "So here's just a little snippet\nof sort of vectorized code",
      "start": 1473.115,
      "duration": 2.702,
      "language": "en"
    },
    {
      "text": "in numpy, and you'll end up implementing",
      "start": 1475.817,
      "duration": 2.549,
      "language": "en"
    },
    {
      "text": "something like this for\nthe first assignment,",
      "start": 1478.366,
      "duration": 3.396,
      "language": "en"
    },
    {
      "text": "but this kind of gives you\nthe sense that this sum",
      "start": 1481.762,
      "duration": 2.631,
      "language": "en"
    },
    {
      "text": "is actually like pretty easy to implement in numpy, it\nonly takes a couple lines",
      "start": 1484.393,
      "duration": 3.2049999999999272,
      "language": "en"
    },
    {
      "text": "of vectorized code. And you can see in practice,\nlike one nice trick",
      "start": 1487.598,
      "duration": 4.127999999999929,
      "language": "en"
    },
    {
      "text": "is that we can actually go in\nhere and zero out the margins",
      "start": 1491.726,
      "duration": 4.978,
      "language": "en"
    },
    {
      "text": "corresponding to the correct class, and that makes it easy to then just,",
      "start": 1496.704,
      "duration": 4.846000000000004,
      "language": "en"
    },
    {
      "text": "that's sort of one nice\nvectorized trick to skip,",
      "start": 1501.55,
      "duration": 3.595,
      "language": "en"
    },
    {
      "text": "iterate over all but one class. You just kind of zero out\nthe one you want to skip",
      "start": 1505.145,
      "duration": 3.5459999999998217,
      "language": "en"
    },
    {
      "text": "and then compute the sum\nanyway, so that's a nice trick you might consider\nusing on the assignment.",
      "start": 1508.691,
      "duration": 5.423999999999978,
      "language": "en"
    },
    {
      "text": "So now, another question\nabout this loss function.",
      "start": 1514.115,
      "duration": 3.791,
      "language": "en"
    },
    {
      "text": "Suppose that you were\nlucky enough to find a W",
      "start": 1517.906,
      "duration": 2.333,
      "language": "en"
    },
    {
      "text": "that has loss of zero,\nyou're not losing at all,",
      "start": 1520.239,
      "duration": 2.19,
      "language": "en"
    },
    {
      "text": "you're totally winning, this loss function is crushing it,",
      "start": 1522.429,
      "duration": 2.509999999999991,
      "language": "en"
    },
    {
      "text": "but then there's a\nquestion, is this W unique",
      "start": 1524.939,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "or were there other Ws that could also have achieved zero loss?",
      "start": 1528.619,
      "duration": 4.57100000000014,
      "language": "en"
    },
    {
      "text": "- [Student] There are other Ws. - Answer, yeah, so there\nare definitely other Ws.",
      "start": 1534.051,
      "duration": 4.192999999999984,
      "language": "en"
    },
    {
      "text": "And in particular, because\nwe talked a little bit",
      "start": 1538.244,
      "duration": 2.554,
      "language": "en"
    },
    {
      "text": "about this thing of scaling\nthe whole problem up or down",
      "start": 1540.798,
      "duration": 3.978,
      "language": "en"
    },
    {
      "text": "depending on W, so you\ncould actually take W",
      "start": 1544.776,
      "duration": 2.733,
      "language": "en"
    },
    {
      "text": "multiplied by two and this\ndoubled W (Is it quad U now?",
      "start": 1547.509,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "I don't know.) [laughing]",
      "start": 1552.784,
      "duration": 2.1259999999999764,
      "language": "en"
    },
    {
      "text": "This would also achieve zero loss.",
      "start": 1554.91,
      "duration": 2.491,
      "language": "en"
    },
    {
      "text": "So as a concrete example of this, you can go back to your favorite example",
      "start": 1557.401,
      "duration": 3.464999999999918,
      "language": "en"
    },
    {
      "text": "and maybe work through the numbers a little bit later,",
      "start": 1560.866,
      "duration": 2.2849999999998545,
      "language": "en"
    },
    {
      "text": "but if you're taking W and we double W,",
      "start": 1563.151,
      "duration": 2.958,
      "language": "en"
    },
    {
      "text": "then the margins between the\ncorrect and incorrect scores",
      "start": 1566.109,
      "duration": 3.82,
      "language": "en"
    },
    {
      "text": "will also double. So that means that if all these margins",
      "start": 1569.929,
      "duration": 3.060999999999922,
      "language": "en"
    },
    {
      "text": "were already greater than\none, and we doubled them,",
      "start": 1572.99,
      "duration": 2.331,
      "language": "en"
    },
    {
      "text": "they're still going to\nbe greater than one, so you'll still have zero loss.",
      "start": 1575.321,
      "duration": 4.33600000000024,
      "language": "en"
    },
    {
      "text": "And this is kind of interesting,",
      "start": 1580.98,
      "duration": 2.002,
      "language": "en"
    },
    {
      "text": "because if our loss function",
      "start": 1582.982,
      "duration": 2.39,
      "language": "en"
    },
    {
      "text": "is the way that we tell our\nclassifier which W we want",
      "start": 1585.372,
      "duration": 2.856,
      "language": "en"
    },
    {
      "text": "and which W we care about, this is a little bit weird,",
      "start": 1588.228,
      "duration": 2.9049999999999727,
      "language": "en"
    },
    {
      "text": "now there's this inconsistency and how is the classifier to choose",
      "start": 1591.133,
      "duration": 4.864000000000033,
      "language": "en"
    },
    {
      "text": "between these different versions of W that all achieve zero loss?",
      "start": 1595.997,
      "duration": 3.9149999999999636,
      "language": "en"
    },
    {
      "text": "And that's because what we've done here",
      "start": 1600.899,
      "duration": 2.111,
      "language": "en"
    },
    {
      "text": "is written down only a\nloss in terms of the data,",
      "start": 1603.01,
      "duration": 3.093,
      "language": "en"
    },
    {
      "text": "and we've only told our classifier",
      "start": 1606.103,
      "duration": 2.833,
      "language": "en"
    },
    {
      "text": "that it should try to find the W that fits the training data.",
      "start": 1609.782,
      "duration": 3.257000000000062,
      "language": "en"
    },
    {
      "text": "But really in practice,\nwe don't actually care that much about fitting the training data,",
      "start": 1613.039,
      "duration": 4.225999999999885,
      "language": "en"
    },
    {
      "text": "the whole point of machine learning is that we use the training\ndata to find some classifier",
      "start": 1617.265,
      "duration": 5.087999999999738,
      "language": "en"
    },
    {
      "text": "and then we'll apply\nthat thing on test data.",
      "start": 1622.353,
      "duration": 2.669,
      "language": "en"
    },
    {
      "text": "So we don't really care about\nthe training data performance,",
      "start": 1625.022,
      "duration": 2.636,
      "language": "en"
    },
    {
      "text": "we really care about the performance of this classifier on test data.",
      "start": 1627.658,
      "duration": 4.070000000000164,
      "language": "en"
    },
    {
      "text": "So as a result, if the only thing we're telling our classifier to do",
      "start": 1631.728,
      "duration": 3.742999999999938,
      "language": "en"
    },
    {
      "text": "is fit the training data, then we can lead ourselves",
      "start": 1635.471,
      "duration": 3.382000000000062,
      "language": "en"
    },
    {
      "text": "into some of these weird\nsituations sometimes,",
      "start": 1638.853,
      "duration": 2.254,
      "language": "en"
    },
    {
      "text": "where the classifier might\nhave unintuitive behavior.",
      "start": 1641.107,
      "duration": 3.772,
      "language": "en"
    },
    {
      "text": "So a concrete, canonical example\nof this sort of thing,",
      "start": 1644.879,
      "duration": 3.707,
      "language": "en"
    },
    {
      "text": "by the way, this is not\nlinear classification anymore,",
      "start": 1648.586,
      "duration": 2.199,
      "language": "en"
    },
    {
      "text": "this is a little bit of a more general machine learning concept,",
      "start": 1650.785,
      "duration": 2.9329999999999927,
      "language": "en"
    },
    {
      "text": "is that suppose we have this\ndata set of blue points,",
      "start": 1653.718,
      "duration": 2.897,
      "language": "en"
    },
    {
      "text": "and we're going to fit some\ncurve to the training data,",
      "start": 1656.615,
      "duration": 2.853,
      "language": "en"
    },
    {
      "text": "the blue points, then if the only thing we've\ntold our classifier to do",
      "start": 1659.468,
      "duration": 4.114999999999782,
      "language": "en"
    },
    {
      "text": "is to try and fit the training data, it might go in and have very wiggly curves",
      "start": 1663.583,
      "duration": 3.66800000000012,
      "language": "en"
    },
    {
      "text": "to try to perfectly classify all of the training data points.",
      "start": 1667.251,
      "duration": 3.19600000000014,
      "language": "en"
    },
    {
      "text": "But this is bad, because\nwe don't actually care",
      "start": 1670.447,
      "duration": 2.588,
      "language": "en"
    },
    {
      "text": "about this performance, we care about the\nperformance on the test data.",
      "start": 1673.035,
      "duration": 4.093999999999824,
      "language": "en"
    },
    {
      "text": "So now if we have some new data come in",
      "start": 1677.129,
      "duration": 2.05,
      "language": "en"
    },
    {
      "text": "that sort of follows the same trend,",
      "start": 1679.179,
      "duration": 2.331,
      "language": "en"
    },
    {
      "text": "then this very wiggly blue line is going to be totally wrong.",
      "start": 1681.51,
      "duration": 3.1620000000000346,
      "language": "en"
    },
    {
      "text": "And in fact, what we\nprobably would have preferred the classifier to do was maybe predict",
      "start": 1684.672,
      "duration": 3.933999999999969,
      "language": "en"
    },
    {
      "text": "this straight green line, rather than this very complex wiggly line",
      "start": 1688.606,
      "duration": 3.8769999999999527,
      "language": "en"
    },
    {
      "text": "to perfectly fit all the training data.",
      "start": 1692.483,
      "duration": 3.488,
      "language": "en"
    },
    {
      "text": "And this is a core fundamental problem",
      "start": 1695.971,
      "duration": 2.65,
      "language": "en"
    },
    {
      "text": "in machine learning, and the way we usually solve it,",
      "start": 1698.621,
      "duration": 2.8409999999998945,
      "language": "en"
    },
    {
      "text": "is this concept of regularization.",
      "start": 1701.462,
      "duration": 2.154,
      "language": "en"
    },
    {
      "text": "So here we're going to\nadd an additional term",
      "start": 1703.616,
      "duration": 2.343,
      "language": "en"
    },
    {
      "text": "to the loss function. In addition to the data loss,",
      "start": 1705.959,
      "duration": 2.6729999999997744,
      "language": "en"
    },
    {
      "text": "which will tell our\nclassifier that it should fit",
      "start": 1708.632,
      "duration": 2.423,
      "language": "en"
    },
    {
      "text": "the training data,\nwe'll also typically add",
      "start": 1711.055,
      "duration": 2.193,
      "language": "en"
    },
    {
      "text": "another term to the loss function called a regularization term,",
      "start": 1713.248,
      "duration": 3.6089999999999236,
      "language": "en"
    },
    {
      "text": "which encourages the model\nto somehow pick a simpler W,",
      "start": 1716.857,
      "duration": 4.634,
      "language": "en"
    },
    {
      "text": "where the concept of simple kind of depends on the task and the model.",
      "start": 1721.491,
      "duration": 5.300999999999931,
      "language": "en"
    },
    {
      "text": "There's this whole idea of Occam's Razor, which is this fundamental\nidea in scientific discovery",
      "start": 1728.725,
      "duration": 4.943000000000211,
      "language": "en"
    },
    {
      "text": "more broadly, which is that\nif you have many different",
      "start": 1733.668,
      "duration": 2.706,
      "language": "en"
    },
    {
      "text": "competing hypotheses, that could explain",
      "start": 1736.374,
      "duration": 2.139,
      "language": "en"
    },
    {
      "text": "your observations, you should generally\nprefer the simpler one,",
      "start": 1738.513,
      "duration": 3.326000000000249,
      "language": "en"
    },
    {
      "text": "because that's the explanation\nthat is more likely",
      "start": 1741.839,
      "duration": 2.36,
      "language": "en"
    },
    {
      "text": "to generalize to new\nobservations in the future.",
      "start": 1744.199,
      "duration": 3.402,
      "language": "en"
    },
    {
      "text": "And the way we\noperationalize this intuition in machine learning is\ntypically through some explicit",
      "start": 1747.601,
      "duration": 4.1759999999997035,
      "language": "en"
    },
    {
      "text": "regularization penalty that's often written down as R.",
      "start": 1751.777,
      "duration": 4.1440000000000055,
      "language": "en"
    },
    {
      "text": "So then your standard loss function",
      "start": 1757.112,
      "duration": 2.375,
      "language": "en"
    },
    {
      "text": "usually has these two terms, a data loss and a regularization loss,",
      "start": 1759.487,
      "duration": 3.730000000000018,
      "language": "en"
    },
    {
      "text": "and there's some\nhyper-parameter here, lambda,",
      "start": 1763.217,
      "duration": 2.713,
      "language": "en"
    },
    {
      "text": "that trades off between the two.",
      "start": 1765.93,
      "duration": 2.07,
      "language": "en"
    },
    {
      "text": "And we talked about hyper-parameters and cross-validation in the last lecture,",
      "start": 1768.0,
      "duration": 3.854000000000042,
      "language": "en"
    },
    {
      "text": "so this regularization\nhyper-parameter lambda",
      "start": 1771.854,
      "duration": 2.766,
      "language": "en"
    },
    {
      "text": "will be one of the more important ones that you'll need to tune when training",
      "start": 1774.62,
      "duration": 3.4600000000000364,
      "language": "en"
    },
    {
      "text": "these models in practice.",
      "start": 1778.08,
      "duration": 2.083,
      "language": "en"
    },
    {
      "text": "Question? - [Student] What does that lambda R W term",
      "start": 1781.029,
      "duration": 4.4500000000000455,
      "language": "en"
    },
    {
      "text": "have to do with [speaking faintly].",
      "start": 1785.479,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "- Yeah, so the question is, what's the connection\nbetween this lambda R W term",
      "start": 1791.485,
      "duration": 3.165000000000191,
      "language": "en"
    },
    {
      "text": "and actually forcing this wiggly line to become a straight green line?",
      "start": 1794.65,
      "duration": 4.221999999999753,
      "language": "en"
    },
    {
      "text": "I didn't want to go through\nthe derivation on this because I thought it would\nlead us too far astray,",
      "start": 1800.712,
      "duration": 3.63799999999992,
      "language": "en"
    },
    {
      "text": "but you can imagine, maybe you're doing a regression problem,",
      "start": 1804.35,
      "duration": 2.759999999999991,
      "language": "en"
    },
    {
      "text": "in terms of different\npolynomial basis functions,",
      "start": 1807.11,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "and if you're adding\nthis regression penalty,",
      "start": 1810.63,
      "duration": 4.091,
      "language": "en"
    },
    {
      "text": "maybe the model has access to polynomials of very high degree, but\nthrough this regression term",
      "start": 1814.721,
      "duration": 4.394999999999982,
      "language": "en"
    },
    {
      "text": "you could encourage the\nmodel to prefer polynomials",
      "start": 1819.116,
      "duration": 2.399,
      "language": "en"
    },
    {
      "text": "of lower degree, if they\nfit the data properly,",
      "start": 1821.515,
      "duration": 2.934,
      "language": "en"
    },
    {
      "text": "or if they fit the data relatively well. So you could imagine\nthere's two ways to do this,",
      "start": 1824.449,
      "duration": 5.371999999999844,
      "language": "en"
    },
    {
      "text": "either you can constrain your model class to just not contain the more powerful,",
      "start": 1829.821,
      "duration": 3.8759999999999764,
      "language": "en"
    },
    {
      "text": "more complex models, or you\ncan add this soft penalty",
      "start": 1833.697,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "where the model still has\naccess to more complex models,",
      "start": 1837.137,
      "duration": 4.509,
      "language": "en"
    },
    {
      "text": "maybe high degree\npolynomials in this case,",
      "start": 1841.646,
      "duration": 2.266,
      "language": "en"
    },
    {
      "text": "but you add this soft constraint saying that if you want to\nuse these more complex models,",
      "start": 1843.912,
      "duration": 4.808999999999969,
      "language": "en"
    },
    {
      "text": "you need to overcome this penalty for using their complexity.",
      "start": 1848.721,
      "duration": 3.394999999999982,
      "language": "en"
    },
    {
      "text": "So that's the connection here, that is not quite linear classification,",
      "start": 1852.116,
      "duration": 3.966000000000122,
      "language": "en"
    },
    {
      "text": "this is the picture that\nmany people have in mind",
      "start": 1856.082,
      "duration": 2.576,
      "language": "en"
    },
    {
      "text": "when they think about\nregularization at least.",
      "start": 1858.658,
      "duration": 3.833,
      "language": "en"
    },
    {
      "text": "So there's actually a\nlot of different types of regularization that\nget used in practice.",
      "start": 1863.531,
      "duration": 4.185999999999922,
      "language": "en"
    },
    {
      "text": "The most common one is\nprobably L2 regularization,",
      "start": 1867.717,
      "duration": 2.957,
      "language": "en"
    },
    {
      "text": "or weight decay. But there's a lot of other\nones that you might see.",
      "start": 1870.674,
      "duration": 4.030999999999949,
      "language": "en"
    },
    {
      "text": "This L2 regularization is\njust the euclidean norm",
      "start": 1874.705,
      "duration": 4.068,
      "language": "en"
    },
    {
      "text": "of this weight vector W, or sometimes the squared norm.",
      "start": 1878.773,
      "duration": 3.9190000000000964,
      "language": "en"
    },
    {
      "text": "Or sometimes half the squared norm",
      "start": 1882.692,
      "duration": 2.129,
      "language": "en"
    },
    {
      "text": "because it makes your derivatives work out a little bit nicer.",
      "start": 1884.821,
      "duration": 2.7170000000000982,
      "language": "en"
    },
    {
      "text": "But the idea of L2 regularization",
      "start": 1887.538,
      "duration": 2.094,
      "language": "en"
    },
    {
      "text": "is you're just penalizing\nthe euclidean norm of this weight vector.",
      "start": 1889.632,
      "duration": 3.8179999999999836,
      "language": "en"
    },
    {
      "text": "You might also sometimes\nsee L1 regularization,",
      "start": 1893.45,
      "duration": 2.943,
      "language": "en"
    },
    {
      "text": "where we're penalizing the\nL1 norm of the weight vector,",
      "start": 1896.393,
      "duration": 3.364,
      "language": "en"
    },
    {
      "text": "and the L1 regularization\nhas some nice properties",
      "start": 1899.757,
      "duration": 3.568,
      "language": "en"
    },
    {
      "text": "like encouraging sparsity\nin this matrix W.",
      "start": 1903.325,
      "duration": 3.876,
      "language": "en"
    },
    {
      "text": "Some other things you might see would be this elastic net regularization,",
      "start": 1907.201,
      "duration": 3.4139999999999873,
      "language": "en"
    },
    {
      "text": "which is some combination of L1 and L2.",
      "start": 1910.615,
      "duration": 2.929,
      "language": "en"
    },
    {
      "text": "You sometimes see max norm regularization,",
      "start": 1913.544,
      "duration": 3.093,
      "language": "en"
    },
    {
      "text": "penalizing the max norm\nrather than the L1 or L2 norm.",
      "start": 1916.637,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "But these sorts of regularizations are things that you see\nnot just in deep learning,",
      "start": 1921.919,
      "duration": 4.673000000000002,
      "language": "en"
    },
    {
      "text": "but across many areas of machine learning and even optimization more broadly.",
      "start": 1926.592,
      "duration": 5.204999999999927,
      "language": "en"
    },
    {
      "text": "In some later lectures, we'll also see",
      "start": 1931.797,
      "duration": 2.094,
      "language": "en"
    },
    {
      "text": "some types of regularization\nthat are more specific",
      "start": 1933.891,
      "duration": 2.635,
      "language": "en"
    },
    {
      "text": "to deep learning. For example dropout, we'll\nsee in a couple lectures,",
      "start": 1936.526,
      "duration": 3.8759999999999764,
      "language": "en"
    },
    {
      "text": "or batch normalization, stochastic depth,",
      "start": 1940.402,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "these things get kind of\ncrazy in recent years.",
      "start": 1943.682,
      "duration": 2.275,
      "language": "en"
    },
    {
      "text": "But the whole idea of regularization is just any thing that\nyou do to your model,",
      "start": 1945.957,
      "duration": 4.171999999999798,
      "language": "en"
    },
    {
      "text": "that sort of penalizes somehow\nthe complexity of the model,",
      "start": 1950.129,
      "duration": 3.228,
      "language": "en"
    },
    {
      "text": "rather than explicitly trying\nto fit the training data.",
      "start": 1953.357,
      "duration": 4.504,
      "language": "en"
    },
    {
      "text": "Question? [student speaking faintly]",
      "start": 1957.861,
      "duration": 4.8279999999999745,
      "language": "en"
    },
    {
      "text": "Yeah, so the question is, how does the L2 regularization\nmeasure the complexity",
      "start": 1965.658,
      "duration": 3.7460000000000946,
      "language": "en"
    },
    {
      "text": "of the model? Thankfully we have an\nexample of that right here,",
      "start": 1969.404,
      "duration": 3.43100000000004,
      "language": "en"
    },
    {
      "text": "maybe we can walk through.",
      "start": 1972.835,
      "duration": 2.167,
      "language": "en"
    },
    {
      "text": "So here we maybe have\nsome training example, x,",
      "start": 1976.237,
      "duration": 2.342,
      "language": "en"
    },
    {
      "text": "and there's two different\nWs that we're considering.",
      "start": 1978.579,
      "duration": 2.279,
      "language": "en"
    },
    {
      "text": "So x is just this vector of four ones,",
      "start": 1980.858,
      "duration": 2.615,
      "language": "en"
    },
    {
      "text": "and we're considering these\ntwo different possibilities",
      "start": 1983.473,
      "duration": 3.651,
      "language": "en"
    },
    {
      "text": "for W. One is a one in the\nfirst, one is a single one",
      "start": 1987.124,
      "duration": 2.6539999999999964,
      "language": "en"
    },
    {
      "text": "and three zeros, and the other has this 0.25 spread across",
      "start": 1989.778,
      "duration": 3.5519999999999072,
      "language": "en"
    },
    {
      "text": "the four different entries. And now, when we're doing\nlinear classification,",
      "start": 1993.33,
      "duration": 3.3680000000001655,
      "language": "en"
    },
    {
      "text": "we're really taking dot products between our x and our W.",
      "start": 1996.698,
      "duration": 3.80399999999986,
      "language": "en"
    },
    {
      "text": "So in terms of linear classification,",
      "start": 2000.502,
      "duration": 2.831,
      "language": "en"
    },
    {
      "text": "these two Ws are the same,",
      "start": 2003.333,
      "duration": 2.214,
      "language": "en"
    },
    {
      "text": "because they give the same result when dot producted with x.",
      "start": 2005.547,
      "duration": 3.5549999999998363,
      "language": "en"
    },
    {
      "text": "But now the question is, if you look at these two examples,",
      "start": 2009.102,
      "duration": 2.99799999999982,
      "language": "en"
    },
    {
      "text": "which one would L2 regression prefer?",
      "start": 2012.1,
      "duration": 3.083,
      "language": "en"
    },
    {
      "text": "Yeah, so L2 regression would prefer W2,",
      "start": 2016.852,
      "duration": 3.248,
      "language": "en"
    },
    {
      "text": "because it has a smaller norm. So the answer is that the L2 regression",
      "start": 2020.1,
      "duration": 5.554000000000087,
      "language": "en"
    },
    {
      "text": "measures complexity of the classifier",
      "start": 2025.654,
      "duration": 2.163,
      "language": "en"
    },
    {
      "text": "in this relatively coarse way,",
      "start": 2027.817,
      "duration": 2.423,
      "language": "en"
    },
    {
      "text": "where the idea is that,",
      "start": 2030.24,
      "duration": 2.204,
      "language": "en"
    },
    {
      "text": "remember the Ws in linear classification",
      "start": 2032.444,
      "duration": 2.913,
      "language": "en"
    },
    {
      "text": "had this interpretation of how much",
      "start": 2035.357,
      "duration": 2.358,
      "language": "en"
    },
    {
      "text": "does this value of the vector x",
      "start": 2037.715,
      "duration": 2.636,
      "language": "en"
    },
    {
      "text": "correspond to this output class?",
      "start": 2040.351,
      "duration": 2.369,
      "language": "en"
    },
    {
      "text": "So L2 regularization is saying",
      "start": 2042.72,
      "duration": 2.282,
      "language": "en"
    },
    {
      "text": "that it prefers to spread that influence",
      "start": 2045.002,
      "duration": 2.043,
      "language": "en"
    },
    {
      "text": "across all the different values in x.",
      "start": 2047.045,
      "duration": 2.836,
      "language": "en"
    },
    {
      "text": "Maybe this might be more robust, in case you come up with xs that vary,",
      "start": 2049.881,
      "duration": 5.503999999999905,
      "language": "en"
    },
    {
      "text": "then our decisions are spread out",
      "start": 2055.385,
      "duration": 2.103,
      "language": "en"
    },
    {
      "text": "and depend on the entire x vector, rather than depending\nonly on certain elements",
      "start": 2057.488,
      "duration": 3.518000000000029,
      "language": "en"
    },
    {
      "text": "of the x vector. And by the way, L1 regularization",
      "start": 2061.006,
      "duration": 3.7400000000002365,
      "language": "en"
    },
    {
      "text": "has this opposite interpretation.",
      "start": 2064.746,
      "duration": 2.893,
      "language": "en"
    },
    {
      "text": "So actually if we were\nusing L1 regularization,",
      "start": 2067.639,
      "duration": 2.518,
      "language": "en"
    },
    {
      "text": "then we would actually prefer W1 over W2,",
      "start": 2070.157,
      "duration": 3.589,
      "language": "en"
    },
    {
      "text": "because L1 regularization\nhas this different notion",
      "start": 2073.746,
      "duration": 2.649,
      "language": "en"
    },
    {
      "text": "of complexity, saying that\nmaybe the model is less complex,",
      "start": 2076.395,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "maybe we measure model\ncomplexity by the number of zeros",
      "start": 2082.369,
      "duration": 3.298,
      "language": "en"
    },
    {
      "text": "in the weight vector, so the question of how\ndo we measure complexity",
      "start": 2085.667,
      "duration": 4.466000000000349,
      "language": "en"
    },
    {
      "text": "and how does L2 measure complexity?",
      "start": 2090.133,
      "duration": 2.585,
      "language": "en"
    },
    {
      "text": "They're kind of problem dependent.",
      "start": 2092.718,
      "duration": 2.279,
      "language": "en"
    },
    {
      "text": "And you have to think about\nfor your particular setup,",
      "start": 2094.997,
      "duration": 2.929,
      "language": "en"
    },
    {
      "text": "for your particular model and data, how do you think that\ncomplexity should be measured",
      "start": 2097.926,
      "duration": 4.628000000000156,
      "language": "en"
    },
    {
      "text": "on this task? Question?",
      "start": 2102.554,
      "duration": 3.0340000000001055,
      "language": "en"
    },
    {
      "text": "- [Student] So why would L1 prefer W1?",
      "start": 2105.588,
      "duration": 2.252,
      "language": "en"
    },
    {
      "text": "Don't they sum to the same one?",
      "start": 2107.84,
      "duration": 2.089,
      "language": "en"
    },
    {
      "text": "- Oh yes, you're right. So in this case, L1 is actually the same",
      "start": 2109.929,
      "duration": 3.201000000000022,
      "language": "en"
    },
    {
      "text": "between these two. But you could construct\na similar example to this",
      "start": 2113.13,
      "duration": 5.687999999999647,
      "language": "en"
    },
    {
      "text": "where W1 would be preferred\nby L1 regularization.",
      "start": 2118.818,
      "duration": 3.528,
      "language": "en"
    },
    {
      "text": "I guess the general intuition behind L1",
      "start": 2122.346,
      "duration": 3.097,
      "language": "en"
    },
    {
      "text": "is that it generally\nprefers sparse solutions,",
      "start": 2125.443,
      "duration": 2.265,
      "language": "en"
    },
    {
      "text": "that it drives all your\nentries of W to zero",
      "start": 2127.708,
      "duration": 3.65,
      "language": "en"
    },
    {
      "text": "for most of the entries,\nexcept for a couple where it's allowed to deviate from zero.",
      "start": 2131.358,
      "duration": 4.457999999999629,
      "language": "en"
    },
    {
      "text": "The way of measuring complexity for L1",
      "start": 2135.816,
      "duration": 2.929,
      "language": "en"
    },
    {
      "text": "is maybe the number of non-zero entries,",
      "start": 2138.745,
      "duration": 2.246,
      "language": "en"
    },
    {
      "text": "and then for L2, it thinks\nthat things that spread the W",
      "start": 2140.991,
      "duration": 3.486,
      "language": "en"
    },
    {
      "text": "across all the values are less complex.",
      "start": 2144.477,
      "duration": 2.005,
      "language": "en"
    },
    {
      "text": "So it depends on your data,\ndepends on your problem.",
      "start": 2146.482,
      "duration": 3.238,
      "language": "en"
    },
    {
      "text": "Oh and by the way, if\nyou're a hardcore Bayesian,",
      "start": 2149.72,
      "duration": 2.673,
      "language": "en"
    },
    {
      "text": "then using L2 regularization\nhas this nice interpretation",
      "start": 2152.393,
      "duration": 2.991,
      "language": "en"
    },
    {
      "text": "of MAP inference under a Gaussian prior",
      "start": 2155.384,
      "duration": 2.622,
      "language": "en"
    },
    {
      "text": "on the parameter vector. I think there was a\nhomework problem about that",
      "start": 2158.006,
      "duration": 3.22400000000016,
      "language": "en"
    },
    {
      "text": "in 229, but we won't talk about that",
      "start": 2161.23,
      "duration": 2.066,
      "language": "en"
    },
    {
      "text": "for the rest of the quarter.",
      "start": 2163.296,
      "duration": 2.847,
      "language": "en"
    },
    {
      "text": "That's sort of my long, deep dive",
      "start": 2166.143,
      "duration": 2.762,
      "language": "en"
    },
    {
      "text": "into the multi-class SVM loss.",
      "start": 2168.905,
      "duration": 3.295,
      "language": "en"
    },
    {
      "text": "Question? - [Student] Yeah, so I'm still confused",
      "start": 2172.2,
      "duration": 2.999000000000251,
      "language": "en"
    },
    {
      "text": "about what the kind of stuff I need to do",
      "start": 2175.199,
      "duration": 3.247,
      "language": "en"
    },
    {
      "text": "when the linear versus polynomial thing,",
      "start": 2178.446,
      "duration": 3.333,
      "language": "en"
    },
    {
      "text": "because the use of this loss function",
      "start": 2185.39,
      "duration": 3.083,
      "language": "en"
    },
    {
      "text": "isn't going to change the\nfact that you're just doing,",
      "start": 2189.807,
      "duration": 3.104,
      "language": "en"
    },
    {
      "text": "you're looking at a\nlinear classifier, right? - Yeah, so the question is that,",
      "start": 2192.911,
      "duration": 4.793999999999869,
      "language": "en"
    },
    {
      "text": "adding a regularization is not going to change\nthe hypothesis class.",
      "start": 2197.705,
      "duration": 2.893000000000029,
      "language": "en"
    },
    {
      "text": "This is not going to change us\naway from a linear classifier.",
      "start": 2200.598,
      "duration": 4.238,
      "language": "en"
    },
    {
      "text": "The idea is that maybe this example of this polynomial regression",
      "start": 2204.836,
      "duration": 3.455000000000382,
      "language": "en"
    },
    {
      "text": "is definitely not linear regression.",
      "start": 2208.291,
      "duration": 2.547,
      "language": "en"
    },
    {
      "text": "That could be seen as linear regression",
      "start": 2210.838,
      "duration": 2.136,
      "language": "en"
    },
    {
      "text": "on top of a polynomial\nexpansion of the input,",
      "start": 2212.974,
      "duration": 4.652,
      "language": "en"
    },
    {
      "text": "and in which case, this\nregression sort of says",
      "start": 2217.626,
      "duration": 2.886,
      "language": "en"
    },
    {
      "text": "that you're not allowed\nto use as many polynomial",
      "start": 2220.512,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "coefficients as maybe you should have.",
      "start": 2224.032,
      "duration": 2.57,
      "language": "en"
    },
    {
      "text": "Right, so you can imagine this is like, when you're doing polynomial regression,",
      "start": 2226.602,
      "duration": 3.4880000000002838,
      "language": "en"
    },
    {
      "text": "you can write out a polynomial as f of x",
      "start": 2230.09,
      "duration": 2.472,
      "language": "en"
    },
    {
      "text": "equals A zero plus A one\nx plus A two x squared",
      "start": 2232.562,
      "duration": 4.425,
      "language": "en"
    },
    {
      "text": "plus A three x whatever, in that case your parameters, your Ws,",
      "start": 2236.987,
      "duration": 4.155999999999949,
      "language": "en"
    },
    {
      "text": "would be these As, in which case,",
      "start": 2241.143,
      "duration": 2.75,
      "language": "en"
    },
    {
      "text": "penalizing the W could force it towards lower degree polynomials.",
      "start": 2245.011,
      "duration": 3.979000000000269,
      "language": "en"
    },
    {
      "text": "Except in the case of\npolynomial regression, you don't actually want to parameterize",
      "start": 2248.99,
      "duration": 3.300999999999931,
      "language": "en"
    },
    {
      "text": "in terms of As, there's\nsome other paramterization",
      "start": 2252.291,
      "duration": 2.035,
      "language": "en"
    },
    {
      "text": "that you want to use, but that's the general idea,",
      "start": 2254.326,
      "duration": 2.838000000000193,
      "language": "en"
    },
    {
      "text": "that you're sort of penalizing\nthe parameters of the model",
      "start": 2257.164,
      "duration": 2.507,
      "language": "en"
    },
    {
      "text": "to force it towards the simpler hypotheses",
      "start": 2259.671,
      "duration": 2.997,
      "language": "en"
    },
    {
      "text": "within your hypothesis class.",
      "start": 2262.668,
      "duration": 2.417,
      "language": "en"
    },
    {
      "text": "And maybe we can take this offline if that's still a bit confusing.",
      "start": 2266.029,
      "duration": 5.110999999999876,
      "language": "en"
    },
    {
      "text": "So then we've sort of seen\nthis multi-class SVM loss,",
      "start": 2271.14,
      "duration": 4.249,
      "language": "en"
    },
    {
      "text": "and just by the way as a side note, this is one extension or\ngeneralization of the SVM loss",
      "start": 2275.389,
      "duration": 5.92699999999968,
      "language": "en"
    },
    {
      "text": "to multiple classes, there's actually a couple\ndifferent formulations",
      "start": 2282.724,
      "duration": 2.4229999999997744,
      "language": "en"
    },
    {
      "text": "that you can see around in literature,",
      "start": 2285.147,
      "duration": 2.249,
      "language": "en"
    },
    {
      "text": "but I mean, my intuition is\nthat they all tend to work",
      "start": 2287.396,
      "duration": 3.252,
      "language": "en"
    },
    {
      "text": "similarly in practice, at least in the context of deep learning.",
      "start": 2290.648,
      "duration": 3.9649999999996908,
      "language": "en"
    },
    {
      "text": "So we'll stick with this\none particular formulation",
      "start": 2294.613,
      "duration": 2.636,
      "language": "en"
    },
    {
      "text": "of the multi-class SVM loss in this class.",
      "start": 2297.249,
      "duration": 3.5,
      "language": "en"
    },
    {
      "text": "But of course there's many\ndifferent loss functions",
      "start": 2301.861,
      "duration": 2.084,
      "language": "en"
    },
    {
      "text": "you might imagine.",
      "start": 2303.945,
      "duration": 2.013,
      "language": "en"
    },
    {
      "text": "And another really popular choice,",
      "start": 2305.958,
      "duration": 2.112,
      "language": "en"
    },
    {
      "text": "in addition to the multi-class SVM loss,",
      "start": 2308.07,
      "duration": 3.333,
      "language": "en"
    },
    {
      "text": "another really popular\nchoice in deep learning is this multinomial logistic regression,",
      "start": 2312.561,
      "duration": 4.507999999999811,
      "language": "en"
    },
    {
      "text": "or a softmax loss. And this one is probably\nactually a bit more common",
      "start": 2317.069,
      "duration": 5.028999999999996,
      "language": "en"
    },
    {
      "text": "in the context of deep learning, but I decided to present\nthis second for some reason.",
      "start": 2322.098,
      "duration": 6.829000000000178,
      "language": "en"
    },
    {
      "text": "So remember in the context\nof the multi-class SVM loss,",
      "start": 2328.927,
      "duration": 3.615,
      "language": "en"
    },
    {
      "text": "we didn't actually have an interpretation for those scores.",
      "start": 2332.542,
      "duration": 3.354000000000269,
      "language": "en"
    },
    {
      "text": "Remember, when we're\ndoing some classification, our model F, spits our these 10 numbers,",
      "start": 2335.896,
      "duration": 5.427999999999429,
      "language": "en"
    },
    {
      "text": "which are our scores for the classes,",
      "start": 2341.324,
      "duration": 2.146,
      "language": "en"
    },
    {
      "text": "and for the multi-class SVM,",
      "start": 2343.47,
      "duration": 2.011,
      "language": "en"
    },
    {
      "text": "we didn't actually give much\ninterpretation to those scores.",
      "start": 2345.481,
      "duration": 3.106,
      "language": "en"
    },
    {
      "text": "We just said that we want the true score, the score of the correct class",
      "start": 2348.587,
      "duration": 3.324999999999818,
      "language": "en"
    },
    {
      "text": "to be greater than the incorrect classes,",
      "start": 2351.912,
      "duration": 2.385,
      "language": "en"
    },
    {
      "text": "and beyond that we don't really\nsay what those scores mean.",
      "start": 2354.297,
      "duration": 4.215,
      "language": "en"
    },
    {
      "text": "But now, for the multinomial\nlogistic regression",
      "start": 2358.512,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "loss function, we actually\nwill endow those scores",
      "start": 2364.058,
      "duration": 2.367,
      "language": "en"
    },
    {
      "text": "with some additional meaning.",
      "start": 2366.425,
      "duration": 2.043,
      "language": "en"
    },
    {
      "text": "And in particular we're\ngoing to use those scores",
      "start": 2368.468,
      "duration": 2.047,
      "language": "en"
    },
    {
      "text": "to compute a probability distribution",
      "start": 2370.515,
      "duration": 2.549,
      "language": "en"
    },
    {
      "text": "over our classes. So we use this so-called softmax function",
      "start": 2373.064,
      "duration": 5.059999999999945,
      "language": "en"
    },
    {
      "text": "where we take all of our scores,",
      "start": 2378.124,
      "duration": 2.451,
      "language": "en"
    },
    {
      "text": "we exponentiate them so that\nnow they become positive,",
      "start": 2380.575,
      "duration": 3.417,
      "language": "en"
    },
    {
      "text": "then we re-normalize them by\nthe sum of those exponents",
      "start": 2383.992,
      "duration": 3.348,
      "language": "en"
    },
    {
      "text": "so now after we send our scores",
      "start": 2387.34,
      "duration": 2.6,
      "language": "en"
    },
    {
      "text": "through this softmax function, now we end up with this\nprobability distribution,",
      "start": 2389.94,
      "duration": 3.913000000000011,
      "language": "en"
    },
    {
      "text": "where now we have\nprobabilities over our classes,",
      "start": 2393.853,
      "duration": 2.739,
      "language": "en"
    },
    {
      "text": "where each probability\nis between zero and one,",
      "start": 2396.592,
      "duration": 2.401,
      "language": "en"
    },
    {
      "text": "and the sum of probabilities\nacross all classes",
      "start": 2398.993,
      "duration": 3.177,
      "language": "en"
    },
    {
      "text": "sum to one. And now the interpretation\nis that we want,",
      "start": 2402.17,
      "duration": 5.798999999999978,
      "language": "en"
    },
    {
      "text": "there's this computed\nprobability distribution",
      "start": 2407.969,
      "duration": 2.944,
      "language": "en"
    },
    {
      "text": "that's implied by our scores, and we want to compare\nthis with the target",
      "start": 2410.913,
      "duration": 4.537000000000262,
      "language": "en"
    },
    {
      "text": "or true probability distribution.",
      "start": 2415.45,
      "duration": 2.488,
      "language": "en"
    },
    {
      "text": "So if we know that the thing is a cat,",
      "start": 2417.938,
      "duration": 2.028,
      "language": "en"
    },
    {
      "text": "then the target probability distribution",
      "start": 2419.966,
      "duration": 2.917,
      "language": "en"
    },
    {
      "text": "would put all of the\nprobability mass on cat,",
      "start": 2422.883,
      "duration": 2.652,
      "language": "en"
    },
    {
      "text": "so we would have probability\nof cat equals one,",
      "start": 2425.535,
      "duration": 2.169,
      "language": "en"
    },
    {
      "text": "and zero probability for\nall the other classes.",
      "start": 2427.704,
      "duration": 2.85,
      "language": "en"
    },
    {
      "text": "So now what we want to do is encourage our computed probability distribution",
      "start": 2430.554,
      "duration": 3.7739999999998872,
      "language": "en"
    },
    {
      "text": "that's coming out of this softmax function",
      "start": 2434.328,
      "duration": 2.046,
      "language": "en"
    },
    {
      "text": "to match this target\nprobability distribution",
      "start": 2436.374,
      "duration": 2.802,
      "language": "en"
    },
    {
      "text": "that has all the mass\non the correct class.",
      "start": 2439.176,
      "duration": 2.295,
      "language": "en"
    },
    {
      "text": "And the way that we do this, I mean, you can do this\nequation in many ways,",
      "start": 2441.471,
      "duration": 4.5099999999997635,
      "language": "en"
    },
    {
      "text": "you can do this as a KL divergence between the target",
      "start": 2445.981,
      "duration": 3.101999999999407,
      "language": "en"
    },
    {
      "text": "and the computed probability distribution,",
      "start": 2449.083,
      "duration": 2.819,
      "language": "en"
    },
    {
      "text": "you can do this as a\nmaximum likelihood estimate,",
      "start": 2451.902,
      "duration": 2.119,
      "language": "en"
    },
    {
      "text": "but at the end of the day, what we really want is\nthat the probability",
      "start": 2454.021,
      "duration": 3.2529999999997017,
      "language": "en"
    },
    {
      "text": "of the true class is\nhigh and as close to one.",
      "start": 2457.274,
      "duration": 4.365,
      "language": "en"
    },
    {
      "text": "So then our loss will\nnow be the negative log",
      "start": 2461.639,
      "duration": 3.176,
      "language": "en"
    },
    {
      "text": "of the probability of the true class.",
      "start": 2464.815,
      "duration": 2.374,
      "language": "en"
    },
    {
      "text": "This is confusing 'cause\nwe're putting this through multiple different things,",
      "start": 2467.189,
      "duration": 3.318000000000211,
      "language": "en"
    },
    {
      "text": "but remember we wanted the probability",
      "start": 2470.507,
      "duration": 2.038,
      "language": "en"
    },
    {
      "text": "to be close to one, so now log is a monotonic\nfunction, it goes like this,",
      "start": 2472.545,
      "duration": 5.326000000000022,
      "language": "en"
    },
    {
      "text": "and it turns out mathematically, it's easier to maximize log",
      "start": 2477.871,
      "duration": 3.768999999999778,
      "language": "en"
    },
    {
      "text": "than it is to maximize\nthe raw probability,",
      "start": 2481.64,
      "duration": 2.437,
      "language": "en"
    },
    {
      "text": "so we stick with log.",
      "start": 2484.077,
      "duration": 2.327,
      "language": "en"
    },
    {
      "text": "And now log is monotonic, so if we maximize log P of correct class,",
      "start": 2486.404,
      "duration": 4.640000000000327,
      "language": "en"
    },
    {
      "text": "that means we want that to be high,",
      "start": 2491.044,
      "duration": 2.355,
      "language": "en"
    },
    {
      "text": "but loss functions measure\nbadness not goodness",
      "start": 2493.399,
      "duration": 3.425,
      "language": "en"
    },
    {
      "text": "so we need to put in the minus one to make it go the right way.",
      "start": 2496.824,
      "duration": 4.027000000000044,
      "language": "en"
    },
    {
      "text": "So now our loss function for SVM",
      "start": 2500.851,
      "duration": 2.263,
      "language": "en"
    },
    {
      "text": "is going to be the minus\nlog of the probability",
      "start": 2503.114,
      "duration": 2.334,
      "language": "en"
    },
    {
      "text": "of the true class. Yeah, so that's the summary here,",
      "start": 2505.448,
      "duration": 6.673999999999978,
      "language": "en"
    },
    {
      "text": "is that we take our scores,\nwe run through the softmax,",
      "start": 2512.122,
      "duration": 2.46,
      "language": "en"
    },
    {
      "text": "and now our loss is this\nminus log of the probability",
      "start": 2514.582,
      "duration": 2.293,
      "language": "en"
    },
    {
      "text": "of the true class. Okay, so then if you look\nat what this looks like",
      "start": 2516.875,
      "duration": 7.667999999999665,
      "language": "en"
    },
    {
      "text": "on a concrete example, then we go back to our\nfavorite beautiful cat",
      "start": 2524.543,
      "duration": 4.005999999999858,
      "language": "en"
    },
    {
      "text": "with our three examples and\nwe've got these three scores",
      "start": 2528.549,
      "duration": 2.885,
      "language": "en"
    },
    {
      "text": "that are coming out of\nour linear classifier,",
      "start": 2531.434,
      "duration": 3.852,
      "language": "en"
    },
    {
      "text": "and these scores are exactly\nthe way that they were in the context of the SVM loss.",
      "start": 2535.286,
      "duration": 4.226000000000113,
      "language": "en"
    },
    {
      "text": "But now, rather than taking these scores",
      "start": 2539.512,
      "duration": 2.114,
      "language": "en"
    },
    {
      "text": "and putting them directly\ninto our loss function, we're going to take them\nall and exponentiate them",
      "start": 2541.626,
      "duration": 4.596000000000004,
      "language": "en"
    },
    {
      "text": "so that they're all positive, and then we'll normalize them to make sure",
      "start": 2546.222,
      "duration": 3.6729999999997744,
      "language": "en"
    },
    {
      "text": "that they all sum to one. And now our loss will be the minus log",
      "start": 2549.895,
      "duration": 4.692999999999756,
      "language": "en"
    },
    {
      "text": "of the true class score.",
      "start": 2554.588,
      "duration": 2.0,
      "language": "en"
    },
    {
      "text": "So that's the softmax loss,",
      "start": 2557.443,
      "duration": 2.25,
      "language": "en"
    },
    {
      "text": "also called multinomial\nlogistic regression.",
      "start": 2560.956,
      "duration": 3.667,
      "language": "en"
    },
    {
      "text": "So now we asked several questions to try to gain intuition about\nthe multi-class SVM loss,",
      "start": 2566.296,
      "duration": 5.253999999999905,
      "language": "en"
    },
    {
      "text": "and it's useful to think about\nsome of the same questions",
      "start": 2571.55,
      "duration": 3.028,
      "language": "en"
    },
    {
      "text": "to contrast with the softmax loss.",
      "start": 2574.578,
      "duration": 3.582,
      "language": "en"
    },
    {
      "text": "So then the question is, what's the min and max\nvalue of the softmax loss?",
      "start": 2578.16,
      "duration": 5.337000000000444,
      "language": "en"
    },
    {
      "text": "Okay, maybe not so sure, there's too many logs and sums and stuff",
      "start": 2585.784,
      "duration": 3.31899999999996,
      "language": "en"
    },
    {
      "text": "going on in here. So the answer is that the min loss is zero",
      "start": 2589.103,
      "duration": 5.455999999999676,
      "language": "en"
    },
    {
      "text": "and the max loss is infinity. And the way that you can see this,",
      "start": 2594.559,
      "duration": 4.503999999999905,
      "language": "en"
    },
    {
      "text": "the probability distribution that we want is one on the correct class,\nzero on the incorrect classes,",
      "start": 2600.222,
      "duration": 5.045000000000073,
      "language": "en"
    },
    {
      "text": "the way that we do that is, so if that were the case,",
      "start": 2605.267,
      "duration": 2.731999999999971,
      "language": "en"
    },
    {
      "text": "then this thing inside the\nlog would end up being one,",
      "start": 2607.999,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "because it's the log\nprobability of the true class,",
      "start": 2614.462,
      "duration": 3.088,
      "language": "en"
    },
    {
      "text": "then log of one is zero, minus\nlog of one is still zero.",
      "start": 2617.55,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "So that means that if we\ngot the thing totally right,",
      "start": 2622.693,
      "duration": 2.108,
      "language": "en"
    },
    {
      "text": "then our loss would be zero.",
      "start": 2624.801,
      "duration": 2.514,
      "language": "en"
    },
    {
      "text": "But by the way, in order to\nget the thing totally right,",
      "start": 2627.315,
      "duration": 3.734,
      "language": "en"
    },
    {
      "text": "what would our scores have to look like?",
      "start": 2631.049,
      "duration": 3.333,
      "language": "en"
    },
    {
      "text": "Murmuring, murmuring. So the scores would actually\nhave to go quite extreme,",
      "start": 2636.763,
      "duration": 4.1720000000000255,
      "language": "en"
    },
    {
      "text": "like towards infinity. So because we actually\nhave this exponentiation,",
      "start": 2640.935,
      "duration": 4.248999999999796,
      "language": "en"
    },
    {
      "text": "this normalization, the only way we can actually get a\nprobability distribution of one",
      "start": 2645.184,
      "duration": 4.644999999999982,
      "language": "en"
    },
    {
      "text": "and zero, is actually\nputting an infinite score",
      "start": 2649.829,
      "duration": 2.941,
      "language": "en"
    },
    {
      "text": "for the correct class,\nand minus infinity score",
      "start": 2652.77,
      "duration": 4.036,
      "language": "en"
    },
    {
      "text": "for all the incorrect classes. And computers don't do\nso well with infinities,",
      "start": 2656.806,
      "duration": 4.6460000000001855,
      "language": "en"
    },
    {
      "text": "so in practice, you'll\nnever get to zero loss on this thing with finite precision.",
      "start": 2661.452,
      "duration": 3.456000000000131,
      "language": "en"
    },
    {
      "text": "But you still have this interpretation that zero is the theoretical\nminimum loss here.",
      "start": 2664.908,
      "duration": 5.114999999999782,
      "language": "en"
    },
    {
      "text": "And the maximum loss is unbounded.",
      "start": 2670.023,
      "duration": 2.384,
      "language": "en"
    },
    {
      "text": "So suppose that if we\nhad zero probability mass",
      "start": 2672.407,
      "duration": 3.573,
      "language": "en"
    },
    {
      "text": "on the correct class, then\nyou would have minus log",
      "start": 2675.98,
      "duration": 4.303,
      "language": "en"
    },
    {
      "text": "of zero, log of zero is minus infinity,",
      "start": 2680.283,
      "duration": 3.16,
      "language": "en"
    },
    {
      "text": "so minus log of zero\nwould be plus infinity,",
      "start": 2683.443,
      "duration": 3.64,
      "language": "en"
    },
    {
      "text": "so that's really bad. But again, you'll never really get here",
      "start": 2687.083,
      "duration": 2.788000000000011,
      "language": "en"
    },
    {
      "text": "because the only way you can\nactually get this probability",
      "start": 2689.871,
      "duration": 4.677,
      "language": "en"
    },
    {
      "text": "to be zero, is if e to the\ncorrect class score is zero,",
      "start": 2694.548,
      "duration": 4.815,
      "language": "en"
    },
    {
      "text": "and that can only happen\nif that correct class score is minus infinity.",
      "start": 2699.363,
      "duration": 3.0670000000000073,
      "language": "en"
    },
    {
      "text": "So again, you'll never\nactually get to these minimum,",
      "start": 2702.43,
      "duration": 2.404,
      "language": "en"
    },
    {
      "text": "maximum values with finite precision.",
      "start": 2704.834,
      "duration": 3.083,
      "language": "en"
    },
    {
      "text": "So then, remember we had this debugging,",
      "start": 2709.663,
      "duration": 2.169,
      "language": "en"
    },
    {
      "text": "sanity check question in the\ncontext of the multi-class SVM,",
      "start": 2711.832,
      "duration": 3.308,
      "language": "en"
    },
    {
      "text": "and we can ask the same for the softmax.",
      "start": 2715.14,
      "duration": 2.061,
      "language": "en"
    },
    {
      "text": "If all the Ss are small and about zero,",
      "start": 2717.201,
      "duration": 2.737,
      "language": "en"
    },
    {
      "text": "then what is the loss here?",
      "start": 2719.938,
      "duration": 2.149,
      "language": "en"
    },
    {
      "text": "Yeah, answer? - [Student] Minus log one over C.",
      "start": 2722.087,
      "duration": 3.076000000000022,
      "language": "en"
    },
    {
      "text": "- So minus log of one over C?",
      "start": 2725.163,
      "duration": 2.682,
      "language": "en"
    },
    {
      "text": "I think that's, yeah, so then it'd be minus log of one over C,",
      "start": 2727.845,
      "duration": 6.307000000000244,
      "language": "en"
    },
    {
      "text": "because log can flip the thing so then it's just log of C.",
      "start": 2734.152,
      "duration": 3.173999999999978,
      "language": "en"
    },
    {
      "text": "Yeah, so it's just log of C. And again, this is a nice debugging thing,",
      "start": 2737.326,
      "duration": 3.382999999999811,
      "language": "en"
    },
    {
      "text": "if you're training a model\nwith this softmax loss,",
      "start": 2740.709,
      "duration": 2.002,
      "language": "en"
    },
    {
      "text": "you should check at the first iteration.",
      "start": 2742.711,
      "duration": 2.066,
      "language": "en"
    },
    {
      "text": "If it's not log C, then\nsomething's gone wrong.",
      "start": 2744.777,
      "duration": 3.917,
      "language": "en"
    },
    {
      "text": "So then we can compare and\ncontrast these two loss functions",
      "start": 2750.851,
      "duration": 3.206,
      "language": "en"
    },
    {
      "text": "a bit. In terms of linear classification,",
      "start": 2754.057,
      "duration": 2.854000000000269,
      "language": "en"
    },
    {
      "text": "this setup looks the same. We've got this W matrix\nthat gets multiplied",
      "start": 2756.911,
      "duration": 3.1349999999997635,
      "language": "en"
    },
    {
      "text": "against our input to produce\nthis specter of scores,",
      "start": 2760.046,
      "duration": 2.826,
      "language": "en"
    },
    {
      "text": "and now the difference\nbetween the two loss functions is how we choose to interpret those scores",
      "start": 2762.872,
      "duration": 4.36200000000008,
      "language": "en"
    },
    {
      "text": "to quantitatively measure\nthe badness afterwards.",
      "start": 2767.234,
      "duration": 2.893,
      "language": "en"
    },
    {
      "text": "So for SVM, we were going to\ngo in and look at the margins",
      "start": 2770.127,
      "duration": 2.235,
      "language": "en"
    },
    {
      "text": "between the scores of the correct class",
      "start": 2772.362,
      "duration": 3.425,
      "language": "en"
    },
    {
      "text": "and the scores of the incorrect class,",
      "start": 2775.787,
      "duration": 2.151,
      "language": "en"
    },
    {
      "text": "whereas for this softmax\nor cross-entropy loss,",
      "start": 2777.938,
      "duration": 3.118,
      "language": "en"
    },
    {
      "text": "we're going to go and compute\na probability distribution",
      "start": 2781.056,
      "duration": 2.447,
      "language": "en"
    },
    {
      "text": "and then look at the minus log probability",
      "start": 2783.503,
      "duration": 2.277,
      "language": "en"
    },
    {
      "text": "of the correct class. So sometimes if you look at,",
      "start": 2785.78,
      "duration": 4.016000000000076,
      "language": "en"
    },
    {
      "text": "in terms of, nevermind,\nI'll skip that point.",
      "start": 2790.998,
      "duration": 3.018,
      "language": "en"
    },
    {
      "text": "[laughing] So another question that's interesting",
      "start": 2794.016,
      "duration": 3.1419999999998254,
      "language": "en"
    },
    {
      "text": "when contrasting these two\nloss functions is thinking,",
      "start": 2797.158,
      "duration": 4.496,
      "language": "en"
    },
    {
      "text": "suppose that I've got this example point,",
      "start": 2801.654,
      "duration": 3.387,
      "language": "en"
    },
    {
      "text": "and if you change its scores, so assume that we've got\nthree scores for this,",
      "start": 2805.041,
      "duration": 5.733999999999924,
      "language": "en"
    },
    {
      "text": "ignore the part on the bottom. But remember if we go back to this example",
      "start": 2813.659,
      "duration": 3.699000000000069,
      "language": "en"
    },
    {
      "text": "where in the multi-class SVM loss,",
      "start": 2817.358,
      "duration": 3.256,
      "language": "en"
    },
    {
      "text": "when we had the car, and the\ncar score was much better",
      "start": 2820.614,
      "duration": 4.33,
      "language": "en"
    },
    {
      "text": "than all the incorrect classes,",
      "start": 2824.944,
      "duration": 2.149,
      "language": "en"
    },
    {
      "text": "then jiggling the scores\nfor that car image",
      "start": 2827.093,
      "duration": 2.253,
      "language": "en"
    },
    {
      "text": "didn't change the\nmulti-class SVM loss at all,",
      "start": 2829.346,
      "duration": 2.7,
      "language": "en"
    },
    {
      "text": "because the only thing that the SVM loss cared about was getting that correct score",
      "start": 2832.046,
      "duration": 4.036000000000058,
      "language": "en"
    },
    {
      "text": "to be greater than a margin\nabove the incorrect scores.",
      "start": 2836.082,
      "duration": 3.077,
      "language": "en"
    },
    {
      "text": "But now the softmax loss\nis actually quite different",
      "start": 2839.159,
      "duration": 2.033,
      "language": "en"
    },
    {
      "text": "in this respect. The softmax loss actually\nalways wants to drive",
      "start": 2841.192,
      "duration": 3.781999999999698,
      "language": "en"
    },
    {
      "text": "that probability mass all the way to one.",
      "start": 2844.974,
      "duration": 2.264,
      "language": "en"
    },
    {
      "text": "So even if you're giving very high score",
      "start": 2847.238,
      "duration": 3.333,
      "language": "en"
    },
    {
      "text": "to the correct class, and very low score to all the incorrect classes,",
      "start": 2851.943,
      "duration": 3.1549999999997453,
      "language": "en"
    },
    {
      "text": "softmax will want you to pile\nmore and more probability mass",
      "start": 2855.098,
      "duration": 2.554,
      "language": "en"
    },
    {
      "text": "on the correct class, and\ncontinue to push the score",
      "start": 2857.652,
      "duration": 3.192,
      "language": "en"
    },
    {
      "text": "of that correct class up towards infinity,",
      "start": 2860.844,
      "duration": 2.306,
      "language": "en"
    },
    {
      "text": "and the score of the incorrect classes down towards minus infinity.",
      "start": 2863.15,
      "duration": 3.788000000000011,
      "language": "en"
    },
    {
      "text": "So that's the interesting difference between these two loss\nfunctions in practice.",
      "start": 2866.938,
      "duration": 3.8299999999999272,
      "language": "en"
    },
    {
      "text": "That SVM, it'll get this\ndata point over the bar",
      "start": 2870.768,
      "duration": 3.562,
      "language": "en"
    },
    {
      "text": "to be correctly classified\nand then just give up,",
      "start": 2874.33,
      "duration": 2.39,
      "language": "en"
    },
    {
      "text": "it doesn't care about\nthat data point any more. Whereas softmax will just always\ntry to continually improve",
      "start": 2876.72,
      "duration": 4.376000000000204,
      "language": "en"
    },
    {
      "text": "every single data point\nto get better and better and better and better.",
      "start": 2881.096,
      "duration": 3.5419999999999163,
      "language": "en"
    },
    {
      "text": "So that's an interesting difference between these two functions.",
      "start": 2884.638,
      "duration": 3.5399999999999636,
      "language": "en"
    },
    {
      "text": "In practice, I think it tends\nnot to make a huge difference",
      "start": 2888.178,
      "duration": 2.596,
      "language": "en"
    },
    {
      "text": "which one you choose, they tend to perform",
      "start": 2890.774,
      "duration": 2.266,
      "language": "en"
    },
    {
      "text": "pretty similarly across, at least a lot of deep\nlearning applications.",
      "start": 2893.04,
      "duration": 3.7260000000001128,
      "language": "en"
    },
    {
      "text": "But it is very useful to keep\nsome of these differences",
      "start": 2896.766,
      "duration": 3.171,
      "language": "en"
    },
    {
      "text": "in mind. Yeah, so to recap where\nwe've come to from here,",
      "start": 2899.937,
      "duration": 7.03899999999976,
      "language": "en"
    },
    {
      "text": "is that we've got some\ndata set of xs and ys,",
      "start": 2906.976,
      "duration": 3.409,
      "language": "en"
    },
    {
      "text": "we use our linear classifier\nto get some score function,",
      "start": 2910.385,
      "duration": 3.433,
      "language": "en"
    },
    {
      "text": "to compute our scores\nS, from our inputs, x,",
      "start": 2913.818,
      "duration": 3.577,
      "language": "en"
    },
    {
      "text": "and then we'll use a loss function, maybe softmax or SVM or\nsome other loss function",
      "start": 2917.395,
      "duration": 4.53899999999976,
      "language": "en"
    },
    {
      "text": "to compute how quantitatively\nbad were our predictions",
      "start": 2921.934,
      "duration": 4.863,
      "language": "en"
    },
    {
      "text": "compared to this ground true targets, y.",
      "start": 2926.797,
      "duration": 2.957,
      "language": "en"
    },
    {
      "text": "And then we'll often\naugment this loss function",
      "start": 2929.754,
      "duration": 3.456,
      "language": "en"
    },
    {
      "text": "with a regularization term, that tries to trade off between\nfitting the training data",
      "start": 2933.21,
      "duration": 3.763999999999669,
      "language": "en"
    },
    {
      "text": "and preferring simpler models.",
      "start": 2936.974,
      "duration": 2.885,
      "language": "en"
    },
    {
      "text": "So this is a pretty generic overview",
      "start": 2939.859,
      "duration": 2.431,
      "language": "en"
    },
    {
      "text": "of a lot of what we call\nsupervised learning,",
      "start": 2942.29,
      "duration": 2.476,
      "language": "en"
    },
    {
      "text": "and what we'll see in deep\nlearning as we move forward,",
      "start": 2944.766,
      "duration": 3.099,
      "language": "en"
    },
    {
      "text": "is that generally you'll want\nto specify some function, f,",
      "start": 2947.865,
      "duration": 3.823,
      "language": "en"
    },
    {
      "text": "that could be very complex in structure, specify some loss function that determines",
      "start": 2951.688,
      "duration": 3.600999999999658,
      "language": "en"
    },
    {
      "text": "how well your algorithm is doing,",
      "start": 2955.289,
      "duration": 3.539,
      "language": "en"
    },
    {
      "text": "given any value of the parameters, some regularization term",
      "start": 2958.828,
      "duration": 3.025000000000091,
      "language": "en"
    },
    {
      "text": "for how to penalize model complexity",
      "start": 2961.853,
      "duration": 3.207,
      "language": "en"
    },
    {
      "text": "and then you combine these things together and try to find the W",
      "start": 2965.06,
      "duration": 3.3640000000000327,
      "language": "en"
    },
    {
      "text": "that minimizes this final loss function.",
      "start": 2968.424,
      "duration": 3.242,
      "language": "en"
    },
    {
      "text": "But then the question is, how do we actually go about doing that?",
      "start": 2971.666,
      "duration": 2.769999999999982,
      "language": "en"
    },
    {
      "text": "How do we actually find this\nW that minimizes the loss?",
      "start": 2974.436,
      "duration": 3.496,
      "language": "en"
    },
    {
      "text": "And that leads us to the\ntopic of optimization.",
      "start": 2977.932,
      "duration": 3.329,
      "language": "en"
    },
    {
      "text": "So when we're doing optimization,",
      "start": 2981.261,
      "duration": 2.572,
      "language": "en"
    },
    {
      "text": "I usually think of things\nin terms of walking",
      "start": 2983.833,
      "duration": 2.462,
      "language": "en"
    },
    {
      "text": "around some large valley. So the idea is that you're\nwalking around this large valley",
      "start": 2986.295,
      "duration": 6.456000000000131,
      "language": "en"
    },
    {
      "text": "with different mountains\nand valleys and streams",
      "start": 2992.751,
      "duration": 2.232,
      "language": "en"
    },
    {
      "text": "and stuff, and every\npoint on this landscape",
      "start": 2994.983,
      "duration": 2.72,
      "language": "en"
    },
    {
      "text": "corresponds to some setting\nof the parameters W.",
      "start": 2997.703,
      "duration": 3.826,
      "language": "en"
    },
    {
      "text": "And you're this little guy who's\nwalking around this valley,",
      "start": 3001.529,
      "duration": 2.325,
      "language": "en"
    },
    {
      "text": "and you're trying to find, and the height of each of these points,",
      "start": 3003.854,
      "duration": 3.162000000000262,
      "language": "en"
    },
    {
      "text": "sorry, is equal to the loss\nincurred by that setting of W.",
      "start": 3007.016,
      "duration": 4.512,
      "language": "en"
    },
    {
      "text": "And now your job as this little man",
      "start": 3011.528,
      "duration": 2.151,
      "language": "en"
    },
    {
      "text": "walking around this landscape, you need to somehow find\nthe bottom of this valley.",
      "start": 3013.679,
      "duration": 5.121000000000095,
      "language": "en"
    },
    {
      "text": "And this is kind of a\nhard problem in general.",
      "start": 3018.8,
      "duration": 2.517,
      "language": "en"
    },
    {
      "text": "You might think, maybe I'm really smart",
      "start": 3021.317,
      "duration": 2.334,
      "language": "en"
    },
    {
      "text": "and I can think really hard\nabout the analytic properties",
      "start": 3023.651,
      "duration": 2.372,
      "language": "en"
    },
    {
      "text": "of my loss function, my\nregularization all that,",
      "start": 3026.023,
      "duration": 2.156,
      "language": "en"
    },
    {
      "text": "maybe I can just write down the minimizer,",
      "start": 3028.179,
      "duration": 2.867,
      "language": "en"
    },
    {
      "text": "and that would sort of correspond\nto magically teleporting",
      "start": 3031.046,
      "duration": 2.843,
      "language": "en"
    },
    {
      "text": "all the way to the bottom of this valley.",
      "start": 3033.889,
      "duration": 2.42,
      "language": "en"
    },
    {
      "text": "But in practice, once your\nprediction function, f,",
      "start": 3036.309,
      "duration": 3.231,
      "language": "en"
    },
    {
      "text": "and your loss function\nand your regularizer, once these things get big and complex",
      "start": 3039.54,
      "duration": 3.7020000000002256,
      "language": "en"
    },
    {
      "text": "and using neural networks,",
      "start": 3043.242,
      "duration": 2.167,
      "language": "en"
    },
    {
      "text": "there's really not much\nhope in trying to write down an explicit analytic solution",
      "start": 3046.99,
      "duration": 3.5080000000002656,
      "language": "en"
    },
    {
      "text": "that takes you directly to the minima.",
      "start": 3050.498,
      "duration": 2.319,
      "language": "en"
    },
    {
      "text": "So in practice we tend to use various\ntypes of iterative methods",
      "start": 3052.817,
      "duration": 3.467999999999847,
      "language": "en"
    },
    {
      "text": "where we start with some solution and then gradually improve it over time.",
      "start": 3056.285,
      "duration": 5.039000000000215,
      "language": "en"
    },
    {
      "text": "So the very first, stupidest thing",
      "start": 3061.324,
      "duration": 2.833,
      "language": "en"
    },
    {
      "text": "that you might imagine is random search,",
      "start": 3065.327,
      "duration": 2.458,
      "language": "en"
    },
    {
      "text": "that will just take a bunch of Ws,",
      "start": 3067.785,
      "duration": 2.039,
      "language": "en"
    },
    {
      "text": "sampled randomly, and throw\nthem into our loss function",
      "start": 3069.824,
      "duration": 3.156,
      "language": "en"
    },
    {
      "text": "and see how well they do.",
      "start": 3072.98,
      "duration": 2.783,
      "language": "en"
    },
    {
      "text": "So spoiler alert, this is\na really bad algorithm,",
      "start": 3075.763,
      "duration": 2.453,
      "language": "en"
    },
    {
      "text": "you probably shouldn't use this, but at least it's one thing\nyou might imagine trying.",
      "start": 3078.216,
      "duration": 5.907000000000153,
      "language": "en"
    },
    {
      "text": "And we can actually do this, we can actually try to\ntrain a linear classifier",
      "start": 3084.123,
      "duration": 4.532999999999902,
      "language": "en"
    },
    {
      "text": "via random search, for CIFAR-10",
      "start": 3088.656,
      "duration": 2.957,
      "language": "en"
    },
    {
      "text": "and for this there's 10 classes,",
      "start": 3091.613,
      "duration": 3.339,
      "language": "en"
    },
    {
      "text": "so random chance is 10%, and if we did some\nnumber of random trials,",
      "start": 3094.952,
      "duration": 5.6159999999999854,
      "language": "en"
    },
    {
      "text": "we eventually found just\nthrough sheer dumb luck,",
      "start": 3100.568,
      "duration": 2.444,
      "language": "en"
    },
    {
      "text": "some setting of W that\ngot maybe 15% accuracy.",
      "start": 3103.012,
      "duration": 3.433,
      "language": "en"
    },
    {
      "text": "So it's better than random,",
      "start": 3106.445,
      "duration": 2.374,
      "language": "en"
    },
    {
      "text": "but state of the art is maybe 95%",
      "start": 3108.819,
      "duration": 2.219,
      "language": "en"
    },
    {
      "text": "so we've got a little\nbit of gap to close here.",
      "start": 3111.038,
      "duration": 3.593,
      "language": "en"
    },
    {
      "text": "So again, probably don't\nuse this in practice,",
      "start": 3114.631,
      "duration": 2.917,
      "language": "en"
    },
    {
      "text": "but you might imagine\nthat this is something you could potentially do.",
      "start": 3117.548,
      "duration": 3.929000000000542,
      "language": "en"
    },
    {
      "text": "So in practice, maybe a better strategy is actually using some\nof the local geometry",
      "start": 3121.477,
      "duration": 3.98700000000008,
      "language": "en"
    },
    {
      "text": "of this landscape. So if you're this little guy who's walking",
      "start": 3125.464,
      "duration": 2.949999999999818,
      "language": "en"
    },
    {
      "text": "around this landscape,",
      "start": 3128.414,
      "duration": 2.296,
      "language": "en"
    },
    {
      "text": "maybe you can't see directly the path",
      "start": 3130.71,
      "duration": 2.268,
      "language": "en"
    },
    {
      "text": "down to the bottom of the valley, but what you can do is feel with your foot",
      "start": 3132.978,
      "duration": 3.8529999999996107,
      "language": "en"
    },
    {
      "text": "and figure out what is the local geometry,",
      "start": 3136.831,
      "duration": 3.5,
      "language": "en"
    },
    {
      "text": "if I'm standing right here, which way will take me\na little bit downhill?",
      "start": 3141.497,
      "duration": 3.4479999999998654,
      "language": "en"
    },
    {
      "text": "So you can feel with your feet and feel where is the slope of the ground",
      "start": 3144.945,
      "duration": 3.9909999999999854,
      "language": "en"
    },
    {
      "text": "taking me down a little\nbit in this direction?",
      "start": 3148.936,
      "duration": 2.725,
      "language": "en"
    },
    {
      "text": "And you can take a step in that direction, and then you'll go down a little bit,",
      "start": 3151.661,
      "duration": 3.175999999999931,
      "language": "en"
    },
    {
      "text": "feel again with your feet to\nfigure out which way is down,",
      "start": 3154.837,
      "duration": 2.223,
      "language": "en"
    },
    {
      "text": "and then repeat over and over again and hope that you'll end up at the bottom",
      "start": 3157.06,
      "duration": 3.268999999999778,
      "language": "en"
    },
    {
      "text": "of the valley eventually. So this also seems like a\nrelatively simple algorithm,",
      "start": 3160.329,
      "duration": 5.766999999999825,
      "language": "en"
    },
    {
      "text": "but actually this one\ntends to work really well in practice if you get\nall the details right.",
      "start": 3166.096,
      "duration": 4.898999999999887,
      "language": "en"
    },
    {
      "text": "So this is generally the\nstrategy that we'll follow",
      "start": 3170.995,
      "duration": 2.014,
      "language": "en"
    },
    {
      "text": "when training these large neural networks and linear classifiers and other things.",
      "start": 3173.009,
      "duration": 4.81899999999996,
      "language": "en"
    },
    {
      "text": "So then, that was a little hand wavy, so what is slope?",
      "start": 3177.828,
      "duration": 2.923999999999978,
      "language": "en"
    },
    {
      "text": "If you remember back\nto your calculus class,",
      "start": 3180.752,
      "duration": 2.385,
      "language": "en"
    },
    {
      "text": "then at least in one dimension, the slope is the derivative\nof this function.",
      "start": 3183.137,
      "duration": 5.335999999999785,
      "language": "en"
    },
    {
      "text": "So if we've got some\none-dimensional function, f,",
      "start": 3188.473,
      "duration": 2.298,
      "language": "en"
    },
    {
      "text": "that takes in a scalar x,\nand then outputs the height",
      "start": 3190.771,
      "duration": 2.998,
      "language": "en"
    },
    {
      "text": "of some curve, then we\ncan compute the slope",
      "start": 3193.769,
      "duration": 3.491,
      "language": "en"
    },
    {
      "text": "or derivative at any point by imagining,",
      "start": 3197.26,
      "duration": 3.257,
      "language": "en"
    },
    {
      "text": "if we take a small step,\nh, in any direction,",
      "start": 3200.517,
      "duration": 3.75,
      "language": "en"
    },
    {
      "text": "take a small step, h, and\ncompare the difference in the function value over that step",
      "start": 3207.098,
      "duration": 3.5,
      "language": "en"
    },
    {
      "text": "and then drag the step size to zero, that will give us the\nslope of that function",
      "start": 3210.598,
      "duration": 3.438999999999851,
      "language": "en"
    },
    {
      "text": "at that point. And this generalizes quite naturally",
      "start": 3214.037,
      "duration": 2.8570000000004256,
      "language": "en"
    },
    {
      "text": "to multi-variable functions as well.",
      "start": 3216.894,
      "duration": 2.239,
      "language": "en"
    },
    {
      "text": "So in practice, our x\nis maybe not a scalar",
      "start": 3219.133,
      "duration": 3.044,
      "language": "en"
    },
    {
      "text": "but a whole vector, 'cause remember, x\nmight be a whole vector,",
      "start": 3222.177,
      "duration": 5.067999999999756,
      "language": "en"
    },
    {
      "text": "so we need to generalize this notion to multi-variable things.",
      "start": 3228.332,
      "duration": 4.4090000000001055,
      "language": "en"
    },
    {
      "text": "And the generalization that\nwe use of the derivative",
      "start": 3232.741,
      "duration": 2.717,
      "language": "en"
    },
    {
      "text": "in the multi-variable\nsetting is the gradient,",
      "start": 3235.458,
      "duration": 3.238,
      "language": "en"
    },
    {
      "text": "so the gradient is a vector\nof partial derivatives.",
      "start": 3238.696,
      "duration": 3.272,
      "language": "en"
    },
    {
      "text": "So the gradient will\nhave the same shape as x,",
      "start": 3241.968,
      "duration": 3.241,
      "language": "en"
    },
    {
      "text": "and each element of the\ngradient will tell us",
      "start": 3245.209,
      "duration": 3.064,
      "language": "en"
    },
    {
      "text": "what is the slope of the function f,",
      "start": 3248.273,
      "duration": 2.122,
      "language": "en"
    },
    {
      "text": "if we move in that coordinate direction.",
      "start": 3250.395,
      "duration": 2.796,
      "language": "en"
    },
    {
      "text": "And the gradient turns out to have these very nice properties,",
      "start": 3253.191,
      "duration": 4.425000000000182,
      "language": "en"
    },
    {
      "text": "so the gradient is now a\nvector of partial derivatives,",
      "start": 3259.173,
      "duration": 2.663,
      "language": "en"
    },
    {
      "text": "but it points in the\ndirection of greatest increase",
      "start": 3261.836,
      "duration": 2.192,
      "language": "en"
    },
    {
      "text": "of the function and correspondingly,",
      "start": 3264.028,
      "duration": 2.429,
      "language": "en"
    },
    {
      "text": "if you look at the negative\ngradient direction, that gives you the direction\nof greatest decrease",
      "start": 3266.457,
      "duration": 4.112999999999829,
      "language": "en"
    },
    {
      "text": "of the function. And more generally, if you want to know,",
      "start": 3270.57,
      "duration": 4.4399999999996,
      "language": "en"
    },
    {
      "text": "what is the slope of my\nlandscape in any direction?",
      "start": 3275.01,
      "duration": 3.208,
      "language": "en"
    },
    {
      "text": "Then that's equal to the\ndot product of the gradient with the unit vector\ndescribing that direction.",
      "start": 3278.218,
      "duration": 5.275000000000091,
      "language": "en"
    },
    {
      "text": "So this gradient is super important, because it gives you this\nlinear, first-order approximation",
      "start": 3283.493,
      "duration": 5.026000000000295,
      "language": "en"
    },
    {
      "text": "to your function at your current point.",
      "start": 3288.519,
      "duration": 2.479,
      "language": "en"
    },
    {
      "text": "So in practice, a lot of deep learning is about computing\ngradients of your functions",
      "start": 3290.998,
      "duration": 3.462999999999738,
      "language": "en"
    },
    {
      "text": "and then using those gradients\nto iteratively update",
      "start": 3294.461,
      "duration": 2.629,
      "language": "en"
    },
    {
      "text": "your parameter vector. So one naive way that you might imagine",
      "start": 3297.09,
      "duration": 5.904999999999745,
      "language": "en"
    },
    {
      "text": "actually evaluating this\ngradient on a computer,",
      "start": 3302.995,
      "duration": 2.617,
      "language": "en"
    },
    {
      "text": "is using the method of finite differences,",
      "start": 3305.612,
      "duration": 2.143,
      "language": "en"
    },
    {
      "text": "going back to the limit\ndefinition of gradient.",
      "start": 3307.755,
      "duration": 2.533,
      "language": "en"
    },
    {
      "text": "So here on the left, we\nimagine that our current W",
      "start": 3310.288,
      "duration": 3.133,
      "language": "en"
    },
    {
      "text": "is this parameter vector that maybe gives us some\ncurrent loss of maybe 1.25",
      "start": 3313.421,
      "duration": 4.811000000000149,
      "language": "en"
    },
    {
      "text": "and our goal is to\ncompute the gradient, dW,",
      "start": 3318.232,
      "duration": 3.695,
      "language": "en"
    },
    {
      "text": "which will be a vector\nof the same shape as W,",
      "start": 3321.927,
      "duration": 2.795,
      "language": "en"
    },
    {
      "text": "and each slot in that\ngradient will tell us",
      "start": 3324.722,
      "duration": 2.099,
      "language": "en"
    },
    {
      "text": "how much will the loss\nchange is we move a tiny,",
      "start": 3326.821,
      "duration": 3.029,
      "language": "en"
    },
    {
      "text": "infinitesimal amount in\nthat coordinate direction.",
      "start": 3329.85,
      "duration": 2.684,
      "language": "en"
    },
    {
      "text": "So one thing you might imagine is just computing these\nfinite differences,",
      "start": 3332.534,
      "duration": 4.007000000000062,
      "language": "en"
    },
    {
      "text": "that we have our W, we\nmight try to increment",
      "start": 3336.541,
      "duration": 2.595,
      "language": "en"
    },
    {
      "text": "the first element of\nW by a small value, h,",
      "start": 3339.136,
      "duration": 3.566,
      "language": "en"
    },
    {
      "text": "and then re-compute the\nloss using our loss function",
      "start": 3342.702,
      "duration": 2.308,
      "language": "en"
    },
    {
      "text": "and our classifier and all that. And maybe in this setting,\nif we move a little bit",
      "start": 3345.01,
      "duration": 3.901999999999589,
      "language": "en"
    },
    {
      "text": "in the first dimension,\nthen our loss will decrease",
      "start": 3348.912,
      "duration": 2.68,
      "language": "en"
    },
    {
      "text": "a little bit from 1.2534 to 1.25322.",
      "start": 3351.592,
      "duration": 3.0,
      "language": "en"
    },
    {
      "text": "And then we can use this limit definition to come up with this finite\ndifferences approximation",
      "start": 3356.745,
      "duration": 5.41800000000012,
      "language": "en"
    },
    {
      "text": "to the gradient in this first dimension.",
      "start": 3362.163,
      "duration": 3.015,
      "language": "en"
    },
    {
      "text": "And now you can imagine\nrepeating this procedure in the second dimension,",
      "start": 3365.178,
      "duration": 3.417000000000371,
      "language": "en"
    },
    {
      "text": "where now we take the first dimension, set it back to the original value,",
      "start": 3368.595,
      "duration": 3.2339999999999236,
      "language": "en"
    },
    {
      "text": "and now increment the second\ndirection by a small step.",
      "start": 3371.829,
      "duration": 2.699,
      "language": "en"
    },
    {
      "text": "And again, we compute the loss and use this finite\ndifferences approximation",
      "start": 3374.528,
      "duration": 3.8650000000002365,
      "language": "en"
    },
    {
      "text": "to compute an approximation\nto the gradient in the second slot.",
      "start": 3378.393,
      "duration": 3.5720000000001164,
      "language": "en"
    },
    {
      "text": "And now repeat this again for the third, and on and on and on.",
      "start": 3381.965,
      "duration": 3.9699999999998,
      "language": "en"
    },
    {
      "text": "So this is actually a terrible idea",
      "start": 3385.935,
      "duration": 2.548,
      "language": "en"
    },
    {
      "text": "because it's super slow. So you might imagine that\ncomputing this function, f,",
      "start": 3388.483,
      "duration": 4.296999999999571,
      "language": "en"
    },
    {
      "text": "might actually be super\nslow if it's a large,",
      "start": 3392.78,
      "duration": 2.25,
      "language": "en"
    },
    {
      "text": "convolutional neural network. And this parameter vector, W,",
      "start": 3395.03,
      "duration": 4.138999999999669,
      "language": "en"
    },
    {
      "text": "probably will not have 10\nentries like it does here,",
      "start": 3399.169,
      "duration": 2.324,
      "language": "en"
    },
    {
      "text": "it might have tens of millions or even hundreds of millions\nfor some of these large,",
      "start": 3401.493,
      "duration": 3.65099999999984,
      "language": "en"
    },
    {
      "text": "complex deep learning models.",
      "start": 3405.144,
      "duration": 2.102,
      "language": "en"
    },
    {
      "text": "So in practice, you'll\nnever want to compute",
      "start": 3407.246,
      "duration": 2.036,
      "language": "en"
    },
    {
      "text": "your gradients for your\nfinite differences, 'cause you'd have to wait\nfor hundreds of millions",
      "start": 3409.282,
      "duration": 4.382000000000062,
      "language": "en"
    },
    {
      "text": "potentially of function evaluations to get a single gradient,\nand that would be super slow",
      "start": 3413.664,
      "duration": 4.043999999999869,
      "language": "en"
    },
    {
      "text": "and super bad. But thankfully we don't have to do that.",
      "start": 3417.708,
      "duration": 5.615999999999531,
      "language": "en"
    },
    {
      "text": "Hopefully you took a calculus course at some point in your lives,",
      "start": 3423.324,
      "duration": 2.7910000000001673,
      "language": "en"
    },
    {
      "text": "so you know that thanks to these guys,",
      "start": 3426.115,
      "duration": 2.831,
      "language": "en"
    },
    {
      "text": "we can just write down the\nexpression for our loss",
      "start": 3428.946,
      "duration": 3.06,
      "language": "en"
    },
    {
      "text": "and then use the magical\nhammer of calculus",
      "start": 3432.006,
      "duration": 2.452,
      "language": "en"
    },
    {
      "text": "to just write down an expression for what this gradient should be.",
      "start": 3434.458,
      "duration": 3.713999999999942,
      "language": "en"
    },
    {
      "text": "And this'll be way more efficient than trying to compute it analytically",
      "start": 3438.172,
      "duration": 2.8729999999995925,
      "language": "en"
    },
    {
      "text": "via finite differences. One, it'll be exact,",
      "start": 3441.045,
      "duration": 2.6840000000001965,
      "language": "en"
    },
    {
      "text": "and two, it'll be much faster\nsince we just need to compute",
      "start": 3443.729,
      "duration": 2.504,
      "language": "en"
    },
    {
      "text": "this single expression. So what this would look like is now,",
      "start": 3446.233,
      "duration": 5.971999999999753,
      "language": "en"
    },
    {
      "text": "if we go back to this\npicture of our current W,",
      "start": 3452.205,
      "duration": 2.108,
      "language": "en"
    },
    {
      "text": "rather than iterating over\nall the dimensions of W,",
      "start": 3454.313,
      "duration": 3.335,
      "language": "en"
    },
    {
      "text": "we'll figure out ahead of time what is the analytic\nexpression for the gradient,",
      "start": 3457.648,
      "duration": 3.8049999999998363,
      "language": "en"
    },
    {
      "text": "and then just write it down\nand go directly from the W",
      "start": 3461.453,
      "duration": 3.626,
      "language": "en"
    },
    {
      "text": "and compute the dW or\nthe gradient in one step.",
      "start": 3465.079,
      "duration": 3.058,
      "language": "en"
    },
    {
      "text": "And that will be much better in practice.",
      "start": 3468.137,
      "duration": 3.538,
      "language": "en"
    },
    {
      "text": "So in summary, this numerical gradient",
      "start": 3471.675,
      "duration": 2.971,
      "language": "en"
    },
    {
      "text": "is something that's\nsimple and makes sense,",
      "start": 3474.646,
      "duration": 2.892,
      "language": "en"
    },
    {
      "text": "but you won't really use it in practice.",
      "start": 3477.538,
      "duration": 2.007,
      "language": "en"
    },
    {
      "text": "In practice, you'll always\ntake an analytic gradient",
      "start": 3479.545,
      "duration": 3.049,
      "language": "en"
    },
    {
      "text": "and use that when actually performing\nthese gradient computations.",
      "start": 3482.594,
      "duration": 3.507000000000062,
      "language": "en"
    },
    {
      "text": "However, one interesting note is that these numeric gradients\nare actually a very useful",
      "start": 3486.101,
      "duration": 4.0590000000001965,
      "language": "en"
    },
    {
      "text": "debugging tool. Say you've written some code,",
      "start": 3490.16,
      "duration": 4.480999999999767,
      "language": "en"
    },
    {
      "text": "and you wrote some code\nthat computes the loss",
      "start": 3494.641,
      "duration": 2.328,
      "language": "en"
    },
    {
      "text": "and the gradient of the loss, then how do you debug this thing?",
      "start": 3496.969,
      "duration": 3.393000000000029,
      "language": "en"
    },
    {
      "text": "How do you make sure that\nthis analytic expression",
      "start": 3500.362,
      "duration": 2.347,
      "language": "en"
    },
    {
      "text": "that you derived and wrote down in code",
      "start": 3502.709,
      "duration": 2.176,
      "language": "en"
    },
    {
      "text": "is actually correct? So a really common debugging\nstrategy for these things",
      "start": 3504.885,
      "duration": 4.35799999999972,
      "language": "en"
    },
    {
      "text": "is to use the numeric gradient as a way,",
      "start": 3509.243,
      "duration": 2.716,
      "language": "en"
    },
    {
      "text": "as sort of a unit test to make sure that your analytic gradient was correct.",
      "start": 3511.959,
      "duration": 3.9820000000004256,
      "language": "en"
    },
    {
      "text": "Again, because this is\nsuper slow and inexact,",
      "start": 3515.941,
      "duration": 3.179,
      "language": "en"
    },
    {
      "text": "then when doing this\nnumeric gradient checking,",
      "start": 3519.12,
      "duration": 3.115,
      "language": "en"
    },
    {
      "text": "as it's called, you'll tend\nto scale down the parameter",
      "start": 3522.235,
      "duration": 2.304,
      "language": "en"
    },
    {
      "text": "of the problem so that it actually runs in a reasonable amount of time.",
      "start": 3524.539,
      "duration": 3.0159999999996217,
      "language": "en"
    },
    {
      "text": "But this ends up being a super\nuseful debugging strategy",
      "start": 3527.555,
      "duration": 2.621,
      "language": "en"
    },
    {
      "text": "when you're writing your\nown gradient computations.",
      "start": 3530.176,
      "duration": 2.345,
      "language": "en"
    },
    {
      "text": "So this is actually very\ncommonly used in practice,",
      "start": 3532.521,
      "duration": 2.391,
      "language": "en"
    },
    {
      "text": "and you'll do this on\nyour assignments as well.",
      "start": 3534.912,
      "duration": 4.498,
      "language": "en"
    },
    {
      "text": "So then once we know how\nto compute the gradient,",
      "start": 3539.41,
      "duration": 3.224,
      "language": "en"
    },
    {
      "text": "then it leads us to this\nsuper simple algorithm",
      "start": 3542.634,
      "duration": 2.713,
      "language": "en"
    },
    {
      "text": "that's like three lines, but\nturns out to be at the heart",
      "start": 3545.347,
      "duration": 2.443,
      "language": "en"
    },
    {
      "text": "of how we train even these very biggest,",
      "start": 3547.79,
      "duration": 2.49,
      "language": "en"
    },
    {
      "text": "most complex deep learning algorithms,",
      "start": 3550.28,
      "duration": 2.127,
      "language": "en"
    },
    {
      "text": "and that's gradient descent. So gradient descent is\nfirst we initialize our W",
      "start": 3552.407,
      "duration": 5.3840000000000146,
      "language": "en"
    },
    {
      "text": "as some random thing, then while true,",
      "start": 3557.791,
      "duration": 2.553,
      "language": "en"
    },
    {
      "text": "we'll compute our loss and our gradient",
      "start": 3560.344,
      "duration": 2.011,
      "language": "en"
    },
    {
      "text": "and then we'll update our weights",
      "start": 3562.355,
      "duration": 2.966,
      "language": "en"
    },
    {
      "text": "in the opposite of the gradient direction,",
      "start": 3565.321,
      "duration": 3.026,
      "language": "en"
    },
    {
      "text": "'cause remember that the gradient was pointing in the direction\nof greatest increase",
      "start": 3568.347,
      "duration": 3.162999999999556,
      "language": "en"
    },
    {
      "text": "of the function, so minus gradient points in the direction\nof greatest decrease,",
      "start": 3571.51,
      "duration": 3.336999999999989,
      "language": "en"
    },
    {
      "text": "so we'll take a small\nstep in the direction",
      "start": 3574.847,
      "duration": 2.68,
      "language": "en"
    },
    {
      "text": "of minus gradient, and\njust repeat this forever",
      "start": 3577.527,
      "duration": 2.535,
      "language": "en"
    },
    {
      "text": "and eventually your network will converge and you'll be very happy, hopefully.",
      "start": 3580.062,
      "duration": 3.993000000000393,
      "language": "en"
    },
    {
      "text": "But this step size is\nactually a hyper-parameter,",
      "start": 3584.055,
      "duration": 2.341,
      "language": "en"
    },
    {
      "text": "and this tells us that every\ntime we compute the gradient,",
      "start": 3586.396,
      "duration": 2.623,
      "language": "en"
    },
    {
      "text": "how far do we step in that direction.",
      "start": 3589.019,
      "duration": 2.623,
      "language": "en"
    },
    {
      "text": "And this step size, also\nsometimes called a learning rate,",
      "start": 3591.642,
      "duration": 2.76,
      "language": "en"
    },
    {
      "text": "is probably one of the\nsingle most important hyper-parameters that you need to set",
      "start": 3594.402,
      "duration": 3.77599999999984,
      "language": "en"
    },
    {
      "text": "when you're actually training\nthese things in practice.",
      "start": 3598.178,
      "duration": 2.655,
      "language": "en"
    },
    {
      "text": "Actually for me when I'm\ntraining these things,",
      "start": 3600.833,
      "duration": 2.084,
      "language": "en"
    },
    {
      "text": "trying to figure out this step size",
      "start": 3602.917,
      "duration": 2.083,
      "language": "en"
    },
    {
      "text": "or this learning rate, is\nthe first hyper-parameter",
      "start": 3605.0,
      "duration": 2.14,
      "language": "en"
    },
    {
      "text": "that I always check. Things like model size or\nregularization strength",
      "start": 3607.14,
      "duration": 4.609000000000378,
      "language": "en"
    },
    {
      "text": "I leave until a little bit later, and getting the learning\nrate or the step size correct",
      "start": 3611.749,
      "duration": 4.459000000000287,
      "language": "en"
    },
    {
      "text": "is the first thing that I\ntry to set at the beginning.",
      "start": 3616.208,
      "duration": 3.953,
      "language": "en"
    },
    {
      "text": "So pictorially what this looks like",
      "start": 3620.161,
      "duration": 3.654,
      "language": "en"
    },
    {
      "text": "here's a simple example in two dimensions.",
      "start": 3623.815,
      "duration": 2.197,
      "language": "en"
    },
    {
      "text": "So here we've got maybe this bowl",
      "start": 3626.012,
      "duration": 2.792,
      "language": "en"
    },
    {
      "text": "that's showing our loss function",
      "start": 3628.804,
      "duration": 2.047,
      "language": "en"
    },
    {
      "text": "where this red region in the center",
      "start": 3630.851,
      "duration": 3.584,
      "language": "en"
    },
    {
      "text": "is this region of low\nloss we want to get to",
      "start": 3634.435,
      "duration": 2.963,
      "language": "en"
    },
    {
      "text": "and these blue and green\nregions towards the edge",
      "start": 3637.398,
      "duration": 2.254,
      "language": "en"
    },
    {
      "text": "are higher loss that we want to avoid.",
      "start": 3639.652,
      "duration": 2.335,
      "language": "en"
    },
    {
      "text": "So now we're going to start of our W",
      "start": 3641.987,
      "duration": 2.017,
      "language": "en"
    },
    {
      "text": "at some random point in the space, and then we'll compute the\nnegative gradient direction,",
      "start": 3644.004,
      "duration": 4.332000000000335,
      "language": "en"
    },
    {
      "text": "which will hopefully\npoint us in the direction",
      "start": 3648.336,
      "duration": 2.144,
      "language": "en"
    },
    {
      "text": "of the minima eventually. And if we repeat this over and over again,",
      "start": 3650.48,
      "duration": 3.4909999999999854,
      "language": "en"
    },
    {
      "text": "we'll hopefully eventually\nget to the exact minima.",
      "start": 3653.971,
      "duration": 3.236,
      "language": "en"
    },
    {
      "text": "And what this looks like in practice is,",
      "start": 3657.207,
      "duration": 3.876,
      "language": "en"
    },
    {
      "text": "oh man, we've got this\nmouse problem again.",
      "start": 3661.083,
      "duration": 2.857,
      "language": "en"
    },
    {
      "text": "So what this looks like in practice is that if we repeat this\nthing over and over again,",
      "start": 3663.94,
      "duration": 6.109999999999673,
      "language": "en"
    },
    {
      "text": "then we will start off at some point",
      "start": 3670.05,
      "duration": 2.811,
      "language": "en"
    },
    {
      "text": "and eventually, taking tiny\ngradient steps each time,",
      "start": 3672.861,
      "duration": 3.205,
      "language": "en"
    },
    {
      "text": "you'll see that the parameter\nwill arc in toward the center,",
      "start": 3676.066,
      "duration": 3.955,
      "language": "en"
    },
    {
      "text": "this region of minima, and that's really what you want,",
      "start": 3680.021,
      "duration": 3.0149999999998727,
      "language": "en"
    },
    {
      "text": "because you want to get to low loss.",
      "start": 3683.036,
      "duration": 2.005,
      "language": "en"
    },
    {
      "text": "And by the way, as a bit of a teaser,",
      "start": 3685.041,
      "duration": 2.571,
      "language": "en"
    },
    {
      "text": "we saw in the previous slide, this example of very\nsimple gradient descent,",
      "start": 3687.612,
      "duration": 4.092999999999847,
      "language": "en"
    },
    {
      "text": "where at every step, we're\njust stepping in the direction of the gradient.",
      "start": 3691.705,
      "duration": 3.093000000000302,
      "language": "en"
    },
    {
      "text": "But in practice, over the\nnext couple of lectures, we'll see that there are\nslightly fancier step,",
      "start": 3694.798,
      "duration": 4.68100000000004,
      "language": "en"
    },
    {
      "text": "what they call these update rules,",
      "start": 3699.479,
      "duration": 2.206,
      "language": "en"
    },
    {
      "text": "where you can take slightly fancier things",
      "start": 3701.685,
      "duration": 2.713,
      "language": "en"
    },
    {
      "text": "to incorporate gradients\nacross multiple time steps",
      "start": 3704.398,
      "duration": 2.439,
      "language": "en"
    },
    {
      "text": "and stuff like that, that tend\nto work a little bit better",
      "start": 3706.837,
      "duration": 2.169,
      "language": "en"
    },
    {
      "text": "in practice and are\nused much more commonly",
      "start": 3709.006,
      "duration": 2.63,
      "language": "en"
    },
    {
      "text": "than this vanilla gradient descent when training these things in practice.",
      "start": 3711.636,
      "duration": 3.774000000000342,
      "language": "en"
    },
    {
      "text": "And then, as a bit of a preview, we can look at some of these\nslightly fancier methods",
      "start": 3715.41,
      "duration": 4.49100000000044,
      "language": "en"
    },
    {
      "text": "on optimizing the same problem. So again, the black will be\nthis same gradient computation,",
      "start": 3719.901,
      "duration": 5.599999999999909,
      "language": "en"
    },
    {
      "text": "and these, I forgot which color they are,",
      "start": 3725.501,
      "duration": 2.829,
      "language": "en"
    },
    {
      "text": "but these two other curves are using slightly fancier update rules",
      "start": 3728.33,
      "duration": 4.049999999999727,
      "language": "en"
    },
    {
      "text": "to decide exactly how to\nuse the gradient information",
      "start": 3732.38,
      "duration": 2.38,
      "language": "en"
    },
    {
      "text": "to make our next step. So one of these is gradient\ndescent with momentum,",
      "start": 3734.76,
      "duration": 6.490999999999531,
      "language": "en"
    },
    {
      "text": "the other is this Adam optimizer,",
      "start": 3741.251,
      "duration": 2.384,
      "language": "en"
    },
    {
      "text": "and we'll see more details about those later in the course.",
      "start": 3743.635,
      "duration": 2.5539999999996326,
      "language": "en"
    },
    {
      "text": "But the idea is that we have\nthis very basic algorithm",
      "start": 3746.189,
      "duration": 2.911,
      "language": "en"
    },
    {
      "text": "called gradient descent, where we use the gradient\nat every time step",
      "start": 3749.1,
      "duration": 3.243999999999687,
      "language": "en"
    },
    {
      "text": "to determine where to step next, and there exist different\nupdate rules which tell us",
      "start": 3752.344,
      "duration": 4.329999999999927,
      "language": "en"
    },
    {
      "text": "how exactly do we use\nthat gradient information.",
      "start": 3756.674,
      "duration": 2.685,
      "language": "en"
    },
    {
      "text": "But it's all the same basic algorithm of trying to go downhill\nat every time step.",
      "start": 3759.359,
      "duration": 5.498999999999796,
      "language": "en"
    },
    {
      "text": "But there's actually\none more little wrinkle that we should talk about.",
      "start": 3770.822,
      "duration": 3.1900000000000546,
      "language": "en"
    },
    {
      "text": "So remember that we\ndefined our loss function,",
      "start": 3774.012,
      "duration": 3.606,
      "language": "en"
    },
    {
      "text": "we defined a loss that computes how bad",
      "start": 3777.618,
      "duration": 2.538,
      "language": "en"
    },
    {
      "text": "is our classifier doing at\nany single training example,",
      "start": 3780.156,
      "duration": 2.881,
      "language": "en"
    },
    {
      "text": "and then we said that our\nfull loss over the data set",
      "start": 3783.037,
      "duration": 2.299,
      "language": "en"
    },
    {
      "text": "was going to be the average loss across the entire training set.",
      "start": 3785.336,
      "duration": 3.7780000000002474,
      "language": "en"
    },
    {
      "text": "But in practice, this N\ncould be very very large.",
      "start": 3789.114,
      "duration": 4.258,
      "language": "en"
    },
    {
      "text": "If we're using the image\nnet data set for example,",
      "start": 3793.372,
      "duration": 2.702,
      "language": "en"
    },
    {
      "text": "that we talked about in the first lecture, then N could be like 1.3 million,",
      "start": 3796.074,
      "duration": 3.924999999999727,
      "language": "en"
    },
    {
      "text": "so actually computing this loss",
      "start": 3799.999,
      "duration": 2.204,
      "language": "en"
    },
    {
      "text": "could be actually very expensive and require computing perhaps\nmillions of evaluations",
      "start": 3802.203,
      "duration": 4.677999999999884,
      "language": "en"
    },
    {
      "text": "of this function.",
      "start": 3806.881,
      "duration": 2.125,
      "language": "en"
    },
    {
      "text": "So that could be really slow. And actually, because the\ngradient is a linear operator,",
      "start": 3809.006,
      "duration": 3.8880000000003747,
      "language": "en"
    },
    {
      "text": "when you actually try\nto compute the gradient",
      "start": 3812.894,
      "duration": 2.015,
      "language": "en"
    },
    {
      "text": "of this expression, you see\nthat the gradient of our loss",
      "start": 3814.909,
      "duration": 2.925,
      "language": "en"
    },
    {
      "text": "is now the sum of the\ngradient of the losses",
      "start": 3817.834,
      "duration": 2.47,
      "language": "en"
    },
    {
      "text": "for each of the individual terms. So now if we want to\ncompute the gradient again,",
      "start": 3820.304,
      "duration": 4.230000000000018,
      "language": "en"
    },
    {
      "text": "it sort of requires us to iterate over the entire training data set",
      "start": 3824.534,
      "duration": 3.195999999999458,
      "language": "en"
    },
    {
      "text": "all N of these examples. So if our N was like a million,",
      "start": 3827.73,
      "duration": 3.4599999999995816,
      "language": "en"
    },
    {
      "text": "this would be super super slow, and we would have to wait\na very very long time",
      "start": 3831.19,
      "duration": 3.688000000000102,
      "language": "en"
    },
    {
      "text": "before we make any individual update to W.",
      "start": 3834.878,
      "duration": 2.9,
      "language": "en"
    },
    {
      "text": "So in practice, we tend to use what is called stochastic\ngradient descent,",
      "start": 3837.778,
      "duration": 3.75,
      "language": "en"
    },
    {
      "text": "where rather than computing\nthe loss and gradient",
      "start": 3841.528,
      "duration": 3.333,
      "language": "en"
    },
    {
      "text": "over the entire training set, instead at every iteration,\nwe sample some small set",
      "start": 3844.861,
      "duration": 4.876999999999953,
      "language": "en"
    },
    {
      "text": "of training examples, called a minibatch.",
      "start": 3849.738,
      "duration": 3.602,
      "language": "en"
    },
    {
      "text": "Usually this is a power\nof two by convention, like 32, 64, 128 are common numbers,",
      "start": 3853.34,
      "duration": 5.164999999999964,
      "language": "en"
    },
    {
      "text": "and then we'll use this small minibatch",
      "start": 3858.505,
      "duration": 2.182,
      "language": "en"
    },
    {
      "text": "to compute an estimate of the full sum,",
      "start": 3860.687,
      "duration": 2.596,
      "language": "en"
    },
    {
      "text": "and an estimate of the true gradient.",
      "start": 3863.283,
      "duration": 2.564,
      "language": "en"
    },
    {
      "text": "And now this is stochastic\nbecause you can view this",
      "start": 3865.847,
      "duration": 2.656,
      "language": "en"
    },
    {
      "text": "as maybe a Monte Carlo\nestimate of some expectation",
      "start": 3868.503,
      "duration": 4.142,
      "language": "en"
    },
    {
      "text": "of the true value. So now this makes our\nalgorithm slightly fancier,",
      "start": 3872.645,
      "duration": 5.472999999999956,
      "language": "en"
    },
    {
      "text": "but it's still only four lines. So now it's well true, sample\nsome random minibatch of data,",
      "start": 3878.118,
      "duration": 5.793999999999869,
      "language": "en"
    },
    {
      "text": "evaluate your loss and\ngradient on the minibatch,",
      "start": 3885.091,
      "duration": 2.391,
      "language": "en"
    },
    {
      "text": "and now make an update on your parameters",
      "start": 3887.482,
      "duration": 2.008,
      "language": "en"
    },
    {
      "text": "based on this estimate of the loss,",
      "start": 3889.49,
      "duration": 2.423,
      "language": "en"
    },
    {
      "text": "and this estimate of the gradient.",
      "start": 3891.913,
      "duration": 2.548,
      "language": "en"
    },
    {
      "text": "And again, we'll see\nslightly fancier update rules",
      "start": 3894.461,
      "duration": 3.108,
      "language": "en"
    },
    {
      "text": "of exactly how to integrate\nmultiple gradients",
      "start": 3897.569,
      "duration": 3.042,
      "language": "en"
    },
    {
      "text": "over time, but this is the\nbasic training algorithm",
      "start": 3900.611,
      "duration": 2.969,
      "language": "en"
    },
    {
      "text": "that we use for pretty much\nall deep neural networks",
      "start": 3903.58,
      "duration": 2.168,
      "language": "en"
    },
    {
      "text": "in practice. So we have another interactive web demo",
      "start": 3905.748,
      "duration": 5.287000000000262,
      "language": "en"
    },
    {
      "text": "actually playing around\nwith linear classifiers,",
      "start": 3911.035,
      "duration": 2.39,
      "language": "en"
    },
    {
      "text": "and training these things via\nstochastic gradient descent,",
      "start": 3913.425,
      "duration": 2.345,
      "language": "en"
    },
    {
      "text": "but given how miserable\nthe web demo was last time,",
      "start": 3915.77,
      "duration": 2.812,
      "language": "en"
    },
    {
      "text": "I'm not actually going to open the link.",
      "start": 3918.582,
      "duration": 2.34,
      "language": "en"
    },
    {
      "text": "Instead, I'll just play this video.",
      "start": 3920.922,
      "duration": 3.147,
      "language": "en"
    },
    {
      "text": "[laughing]",
      "start": 3924.069,
      "duration": 2.07,
      "language": "en"
    },
    {
      "text": "But I encourage you to go check this out and play with it online,",
      "start": 3926.139,
      "duration": 2.4099999999998545,
      "language": "en"
    },
    {
      "text": "because it actually helps\nto build some intuition about linear classifiers and training them",
      "start": 3928.549,
      "duration": 3.287000000000262,
      "language": "en"
    },
    {
      "text": "via gradient descent. So here you can see on the left,",
      "start": 3931.836,
      "duration": 3.8830000000002656,
      "language": "en"
    },
    {
      "text": "we've got this problem\nwhere we're categorizing",
      "start": 3935.719,
      "duration": 2.566,
      "language": "en"
    },
    {
      "text": "three different classes,",
      "start": 3938.285,
      "duration": 2.661,
      "language": "en"
    },
    {
      "text": "and we've got these\ngreen, blue and red points",
      "start": 3940.946,
      "duration": 2.405,
      "language": "en"
    },
    {
      "text": "that are our training samples\nfrom these three classes.",
      "start": 3943.351,
      "duration": 3.202,
      "language": "en"
    },
    {
      "text": "And now we've drawn\nthe decision boundaries",
      "start": 3946.553,
      "duration": 2.598,
      "language": "en"
    },
    {
      "text": "for these classes, which are\nthe colored background regions,",
      "start": 3949.151,
      "duration": 3.717,
      "language": "en"
    },
    {
      "text": "as well as these directions,",
      "start": 3952.868,
      "duration": 2.202,
      "language": "en"
    },
    {
      "text": "giving you the direction of\nincrease for the class scores",
      "start": 3955.07,
      "duration": 3.217,
      "language": "en"
    },
    {
      "text": "for each of these three classes. And now if you see, if\nyou actually go and play",
      "start": 3958.287,
      "duration": 5.621000000000095,
      "language": "en"
    },
    {
      "text": "with this thing online, you can see that we can\ngo in and adjust the Ws",
      "start": 3963.908,
      "duration": 4.471000000000004,
      "language": "en"
    },
    {
      "text": "and changing the values of the Ws will cause these decision\nboundaries to rotate.",
      "start": 3968.379,
      "duration": 4.597000000000207,
      "language": "en"
    },
    {
      "text": "If you change the biases,\nthen the decision boundaries",
      "start": 3972.976,
      "duration": 2.013,
      "language": "en"
    },
    {
      "text": "will not rotate, but will\ninstead move side to side",
      "start": 3974.989,
      "duration": 3.287,
      "language": "en"
    },
    {
      "text": "or up and down. Then we can actually make steps",
      "start": 3978.276,
      "duration": 2.5130000000003747,
      "language": "en"
    },
    {
      "text": "that are trying to update this loss, or you can change the step\nsize with this slider.",
      "start": 3980.789,
      "duration": 3.994999999999891,
      "language": "en"
    },
    {
      "text": "You can hit this button\nto actually run the thing.",
      "start": 3984.784,
      "duration": 2.025,
      "language": "en"
    },
    {
      "text": "So now with a big step size, we're running gradient descent right now,",
      "start": 3986.809,
      "duration": 3.0670000000000073,
      "language": "en"
    },
    {
      "text": "and these decision boundaries\nare flipping around and trying to fit the data.",
      "start": 3989.876,
      "duration": 3.7979999999997744,
      "language": "en"
    },
    {
      "text": "So it's doing okay now, but we can actually change\nthe loss function in real time",
      "start": 3995.353,
      "duration": 5.336999999999989,
      "language": "en"
    },
    {
      "text": "between these different SVM formulations and the different softmax.",
      "start": 4000.69,
      "duration": 3.6770000000001346,
      "language": "en"
    },
    {
      "text": "And you can see that as you flip between these different\nformulations of loss functions,",
      "start": 4004.367,
      "duration": 4.127999999999702,
      "language": "en"
    },
    {
      "text": "it's generally doing the same thing.",
      "start": 4008.495,
      "duration": 2.524,
      "language": "en"
    },
    {
      "text": "Our decision regions are\nmostly in the same place,",
      "start": 4011.019,
      "duration": 2.121,
      "language": "en"
    },
    {
      "text": "but exactly how they end\nup relative to each other",
      "start": 4013.14,
      "duration": 2.605,
      "language": "en"
    },
    {
      "text": "and exactly what the trade-offs are between categorizing\nthese different things",
      "start": 4015.745,
      "duration": 4.194000000000415,
      "language": "en"
    },
    {
      "text": "changes a little bit. So I really encourage you to go online",
      "start": 4019.939,
      "duration": 2.9980000000000473,
      "language": "en"
    },
    {
      "text": "and play with this thing to\ntry to get some intuition for what it actually looks like",
      "start": 4022.937,
      "duration": 3.562000000000353,
      "language": "en"
    },
    {
      "text": "to try to train these linear classifiers via gradient descent.",
      "start": 4026.499,
      "duration": 3.479000000000269,
      "language": "en"
    },
    {
      "text": "Now as an aside, I'd like\nto talk about another idea,",
      "start": 4033.143,
      "duration": 3.902,
      "language": "en"
    },
    {
      "text": "which is that of image features. So so far we've talked\nabout linear classifiers,",
      "start": 4037.045,
      "duration": 4.4229999999997744,
      "language": "en"
    },
    {
      "text": "which is just maybe taking\nour raw image pixels",
      "start": 4041.468,
      "duration": 2.364,
      "language": "en"
    },
    {
      "text": "and then feeding the raw pixels themselves",
      "start": 4043.832,
      "duration": 2.152,
      "language": "en"
    },
    {
      "text": "into our linear classifier.",
      "start": 4045.984,
      "duration": 2.25,
      "language": "en"
    },
    {
      "text": "But as we talked about\nin the last lecture,",
      "start": 4049.264,
      "duration": 2.611,
      "language": "en"
    },
    {
      "text": "this is maybe not such\na great thing to do,",
      "start": 4051.875,
      "duration": 2.268,
      "language": "en"
    },
    {
      "text": "because of things like\nmulti-modality and whatnot.",
      "start": 4054.143,
      "duration": 2.89,
      "language": "en"
    },
    {
      "text": "So in practice, actually\nfeeding raw pixel values",
      "start": 4057.033,
      "duration": 3.135,
      "language": "en"
    },
    {
      "text": "into linear classifiers\ntends to not work so well.",
      "start": 4060.168,
      "duration": 3.421,
      "language": "en"
    },
    {
      "text": "So it was actually common\nbefore the dominance",
      "start": 4063.589,
      "duration": 2.953,
      "language": "en"
    },
    {
      "text": "of deep neural networks, was instead to have\nthis two-stage approach,",
      "start": 4066.542,
      "duration": 3.850000000000364,
      "language": "en"
    },
    {
      "text": "where first, you would take your image and then compute various\nfeature representations",
      "start": 4070.392,
      "duration": 4.324000000000524,
      "language": "en"
    },
    {
      "text": "of that image, that are maybe computing different kinds of quantities\nrelating to the appearance",
      "start": 4074.716,
      "duration": 5.303000000000338,
      "language": "en"
    },
    {
      "text": "of the image, and then concatenate these\ndifferent feature vectors",
      "start": 4080.019,
      "duration": 2.8980000000001382,
      "language": "en"
    },
    {
      "text": "to give you some feature\nrepresentation of the image,",
      "start": 4082.917,
      "duration": 3.02,
      "language": "en"
    },
    {
      "text": "and now this feature\nrepresentation of the image would be fed into a linear classifier,",
      "start": 4085.937,
      "duration": 3.699000000000069,
      "language": "en"
    },
    {
      "text": "rather than feeding the\nraw pixels themselves into the classifier.",
      "start": 4089.636,
      "duration": 4.066000000000258,
      "language": "en"
    },
    {
      "text": "And the motivation here is that,",
      "start": 4093.702,
      "duration": 2.77,
      "language": "en"
    },
    {
      "text": "so imagine we have a\ntraining data set on the left",
      "start": 4096.472,
      "duration": 2.029,
      "language": "en"
    },
    {
      "text": "of these red points, and\nred points in the middle",
      "start": 4098.501,
      "duration": 2.492,
      "language": "en"
    },
    {
      "text": "and blue points around that.",
      "start": 4100.993,
      "duration": 2.051,
      "language": "en"
    },
    {
      "text": "And for this kind of data set, there's no way that we can\ndraw a linear decision boundary",
      "start": 4103.044,
      "duration": 4.197000000000116,
      "language": "en"
    },
    {
      "text": "to separate the red points\nfrom the blue points.",
      "start": 4107.241,
      "duration": 2.717,
      "language": "en"
    },
    {
      "text": "And we saw more examples of\nthis in the last lecture.",
      "start": 4109.958,
      "duration": 2.998,
      "language": "en"
    },
    {
      "text": "But if we use a clever feature transform,",
      "start": 4112.956,
      "duration": 2.303,
      "language": "en"
    },
    {
      "text": "in this case transforming\nto polar coordinates,",
      "start": 4115.259,
      "duration": 2.201,
      "language": "en"
    },
    {
      "text": "then now after we do\nthe feature transform,",
      "start": 4117.46,
      "duration": 2.419,
      "language": "en"
    },
    {
      "text": "then this complex data\nset actually might become",
      "start": 4119.879,
      "duration": 3.282,
      "language": "en"
    },
    {
      "text": "linearly separable, and actually could be classified correctly",
      "start": 4123.161,
      "duration": 2.798999999999978,
      "language": "en"
    },
    {
      "text": "by a linear classifier. And the whole trick here\nnow is to figure out",
      "start": 4125.96,
      "duration": 3.27599999999984,
      "language": "en"
    },
    {
      "text": "what is the right feature transform",
      "start": 4129.236,
      "duration": 2.598,
      "language": "en"
    },
    {
      "text": "that is computing the right quantities",
      "start": 4131.834,
      "duration": 2.163,
      "language": "en"
    },
    {
      "text": "for the problem that you care about. So for images, maybe\nconverting your pixels",
      "start": 4133.997,
      "duration": 4.819999999999709,
      "language": "en"
    },
    {
      "text": "to polar coordinates, doesn't make sense, but you actually can try to write down",
      "start": 4138.817,
      "duration": 3.4890000000004875,
      "language": "en"
    },
    {
      "text": "feature representations of images that might make sense,",
      "start": 4142.306,
      "duration": 3.243000000000393,
      "language": "en"
    },
    {
      "text": "and actually might help you out and might do better than\nputting in raw pixels",
      "start": 4145.549,
      "duration": 3.6369999999997162,
      "language": "en"
    },
    {
      "text": "into the classifier.",
      "start": 4149.186,
      "duration": 2.006,
      "language": "en"
    },
    {
      "text": "So one example of this kind\nof feature representation",
      "start": 4151.192,
      "duration": 2.765,
      "language": "en"
    },
    {
      "text": "that's super simple, is this\nidea of a color histogram.",
      "start": 4153.957,
      "duration": 3.186,
      "language": "en"
    },
    {
      "text": "So you'll take maybe each pixel,",
      "start": 4157.143,
      "duration": 2.183,
      "language": "en"
    },
    {
      "text": "you'll take this hue color spectrum",
      "start": 4159.326,
      "duration": 2.663,
      "language": "en"
    },
    {
      "text": "and divide it into buckets\nand then for every pixel,",
      "start": 4161.989,
      "duration": 2.797,
      "language": "en"
    },
    {
      "text": "you'll map it into one\nof those color buckets",
      "start": 4164.786,
      "duration": 2.44,
      "language": "en"
    },
    {
      "text": "and then count up how many pixels",
      "start": 4167.226,
      "duration": 2.11,
      "language": "en"
    },
    {
      "text": "fall into each of these different buckets.",
      "start": 4169.336,
      "duration": 2.627,
      "language": "en"
    },
    {
      "text": "So this tells you globally\nwhat colors are in the image.",
      "start": 4171.963,
      "duration": 3.476,
      "language": "en"
    },
    {
      "text": "Maybe if this example of a frog, this feature vector would tell us",
      "start": 4175.439,
      "duration": 2.8609999999998763,
      "language": "en"
    },
    {
      "text": "there's a lot of green stuff, and maybe not a lot of\npurple or red stuff.",
      "start": 4178.3,
      "duration": 3.438000000000102,
      "language": "en"
    },
    {
      "text": "And this is kind of a simple feature\nvector that you might see",
      "start": 4181.738,
      "duration": 2.105,
      "language": "en"
    },
    {
      "text": "in practice. Another common feature vector that we saw",
      "start": 4183.843,
      "duration": 4.677999999999884,
      "language": "en"
    },
    {
      "text": "before the rise of neural networks, or before the dominance of neural networks",
      "start": 4188.521,
      "duration": 3.26299999999992,
      "language": "en"
    },
    {
      "text": "was this histogram of oriented gradients.",
      "start": 4191.784,
      "duration": 2.236,
      "language": "en"
    },
    {
      "text": "So remember from the first lecture, that Hubel and Wiesel\nfound these oriented edges",
      "start": 4194.02,
      "duration": 4.609000000000378,
      "language": "en"
    },
    {
      "text": "are really important in\nthe human visual system,",
      "start": 4198.629,
      "duration": 2.217,
      "language": "en"
    },
    {
      "text": "and this histogram of oriented gradients",
      "start": 4200.846,
      "duration": 2.163,
      "language": "en"
    },
    {
      "text": "feature representation tries to capture",
      "start": 4203.009,
      "duration": 2.481,
      "language": "en"
    },
    {
      "text": "the same intuition and\nmeasure the local orientation",
      "start": 4205.49,
      "duration": 2.99,
      "language": "en"
    },
    {
      "text": "of edges on the image.",
      "start": 4208.48,
      "duration": 2.294,
      "language": "en"
    },
    {
      "text": "So what this thing is going to do, is take our image and then divide it",
      "start": 4210.774,
      "duration": 3.311999999999898,
      "language": "en"
    },
    {
      "text": "into these little eight\nby eight pixel regions.",
      "start": 4214.086,
      "duration": 3.068,
      "language": "en"
    },
    {
      "text": "And then within each of those\neight by eight pixel regions,",
      "start": 4217.154,
      "duration": 2.788,
      "language": "en"
    },
    {
      "text": "we'll compute what is the\ndominant edge direction",
      "start": 4219.942,
      "duration": 3.126,
      "language": "en"
    },
    {
      "text": "of each pixel, quantize\nthose edge directions",
      "start": 4223.068,
      "duration": 2.653,
      "language": "en"
    },
    {
      "text": "into several buckets and then\nwithin each of those regions,",
      "start": 4225.721,
      "duration": 2.855,
      "language": "en"
    },
    {
      "text": "compute a histogram over these\ndifferent edge orientations.",
      "start": 4228.576,
      "duration": 4.081,
      "language": "en"
    },
    {
      "text": "And now your full-feature vector will be these different\nbucketed histograms",
      "start": 4232.657,
      "duration": 3.9399999999996,
      "language": "en"
    },
    {
      "text": "of edge orientations across all the different\neight by eight regions",
      "start": 4236.597,
      "duration": 3.3239999999996144,
      "language": "en"
    },
    {
      "text": "in the image. So this is in some sense dual",
      "start": 4239.921,
      "duration": 4.3289999999997235,
      "language": "en"
    },
    {
      "text": "to the color histogram\nclassifier that we saw before.",
      "start": 4244.25,
      "duration": 3.579,
      "language": "en"
    },
    {
      "text": "So color histogram is\nsaying, globally, what colors",
      "start": 4247.829,
      "duration": 2.675,
      "language": "en"
    },
    {
      "text": "exist in the image, and this is saying, overall,\nwhat types of edge information",
      "start": 4250.504,
      "duration": 4.046999999999571,
      "language": "en"
    },
    {
      "text": "exist in the image. And even localized to\ndifferent parts of the image,",
      "start": 4254.551,
      "duration": 4.239999999998872,
      "language": "en"
    },
    {
      "text": "what types of edges exist\nin different regions.",
      "start": 4258.791,
      "duration": 3.2,
      "language": "en"
    },
    {
      "text": "So maybe for this frog on the left,",
      "start": 4261.991,
      "duration": 2.355,
      "language": "en"
    },
    {
      "text": "you can see he's sitting on a leaf, and these leaves have these\ndominant diagonal edges,",
      "start": 4264.346,
      "duration": 4.015000000000327,
      "language": "en"
    },
    {
      "text": "and if you visualize the\nhistogram of oriented gradient",
      "start": 4268.361,
      "duration": 2.919,
      "language": "en"
    },
    {
      "text": "features, then you can\nsee that in this region,",
      "start": 4271.28,
      "duration": 2.353,
      "language": "en"
    },
    {
      "text": "we've got a lot of diagonal edges, that this histogram of oriented gradient",
      "start": 4273.633,
      "duration": 3.394000000000233,
      "language": "en"
    },
    {
      "text": "feature representation's capturing.",
      "start": 4277.027,
      "duration": 3.113,
      "language": "en"
    },
    {
      "text": "So this was a super common\nfeature representation",
      "start": 4280.14,
      "duration": 2.169,
      "language": "en"
    },
    {
      "text": "and was used a lot for object recognition",
      "start": 4282.309,
      "duration": 2.026,
      "language": "en"
    },
    {
      "text": "actually not too long ago.",
      "start": 4284.335,
      "duration": 2.167,
      "language": "en"
    },
    {
      "text": "Another feature representation\nthat you might see out there",
      "start": 4287.373,
      "duration": 3.216,
      "language": "en"
    },
    {
      "text": "is this idea of bag of words.",
      "start": 4290.589,
      "duration": 3.021,
      "language": "en"
    },
    {
      "text": "So this is taking inspiration from natural language processing.",
      "start": 4293.61,
      "duration": 3.5450000000009823,
      "language": "en"
    },
    {
      "text": "So if you've got a paragraph, then a way that you might\nrepresent a paragraph",
      "start": 4297.155,
      "duration": 4.444000000000415,
      "language": "en"
    },
    {
      "text": "by a feature vector is\ncounting up the occurrences",
      "start": 4301.599,
      "duration": 2.599,
      "language": "en"
    },
    {
      "text": "of different words in that paragraph.",
      "start": 4304.198,
      "duration": 2.334,
      "language": "en"
    },
    {
      "text": "So we want to take that\nintuition and apply it to images in some way.",
      "start": 4306.532,
      "duration": 3.931999999999789,
      "language": "en"
    },
    {
      "text": "But the problem is that\nthere's no really simple,",
      "start": 4310.464,
      "duration": 2.044,
      "language": "en"
    },
    {
      "text": "straightforward analogy\nof words to images,",
      "start": 4312.508,
      "duration": 2.58,
      "language": "en"
    },
    {
      "text": "so we need to define our own vocabulary",
      "start": 4315.088,
      "duration": 2.344,
      "language": "en"
    },
    {
      "text": "of visual words. So we take this two-stage approach,",
      "start": 4317.432,
      "duration": 4.47400000000016,
      "language": "en"
    },
    {
      "text": "where first we'll get a bunch of images,",
      "start": 4321.906,
      "duration": 3.212,
      "language": "en"
    },
    {
      "text": "sample a whole bunch of tiny random crops",
      "start": 4325.118,
      "duration": 2.137,
      "language": "en"
    },
    {
      "text": "from those images and then cluster them using something like K means",
      "start": 4327.255,
      "duration": 3.268000000000029,
      "language": "en"
    },
    {
      "text": "to come up with these\ndifferent cluster centers",
      "start": 4330.523,
      "duration": 3.097,
      "language": "en"
    },
    {
      "text": "that are maybe representing\ndifferent types",
      "start": 4333.62,
      "duration": 2.369,
      "language": "en"
    },
    {
      "text": "of visual words in the images. So if you look at this\nexample on the right here,",
      "start": 4335.989,
      "duration": 4.152000000000953,
      "language": "en"
    },
    {
      "text": "this is a real example of clustering actually different image\npatches from images,",
      "start": 4340.141,
      "duration": 3.9940000000005966,
      "language": "en"
    },
    {
      "text": "and you can see that after\nthis clustering step,",
      "start": 4344.135,
      "duration": 2.292,
      "language": "en"
    },
    {
      "text": "our visual words capture\nthese different colors,",
      "start": 4346.427,
      "duration": 2.823,
      "language": "en"
    },
    {
      "text": "like red and blue and yellow,",
      "start": 4349.25,
      "duration": 2.102,
      "language": "en"
    },
    {
      "text": "as well as these different\ntypes of oriented edges",
      "start": 4351.352,
      "duration": 2.001,
      "language": "en"
    },
    {
      "text": "in different directions,",
      "start": 4353.353,
      "duration": 2.003,
      "language": "en"
    },
    {
      "text": "which is interesting that\nnow we're starting to see these oriented edges\ncome out from the data",
      "start": 4355.356,
      "duration": 4.3530000000000655,
      "language": "en"
    },
    {
      "text": "in a data-driven way. And now, once we've got\nthese set of visual words,",
      "start": 4359.709,
      "duration": 4.188000000000102,
      "language": "en"
    },
    {
      "text": "also called a codebook, then we can encode our\nimage by trying to say,",
      "start": 4363.897,
      "duration": 4.152000000000044,
      "language": "en"
    },
    {
      "text": "for each of these visual words, how much does this visual\nword occur in the image?",
      "start": 4368.049,
      "duration": 5.219000000000051,
      "language": "en"
    },
    {
      "text": "And now this gives us, again, some slightly different information",
      "start": 4373.268,
      "duration": 2.994999999999891,
      "language": "en"
    },
    {
      "text": "about what is the visual\nappearance of this image.",
      "start": 4376.263,
      "duration": 3.964,
      "language": "en"
    },
    {
      "text": "And actually this is a type\nof feature representation",
      "start": 4380.227,
      "duration": 2.697,
      "language": "en"
    },
    {
      "text": "that Fei-Fei worked on when\nshe was a grad student,",
      "start": 4382.924,
      "duration": 2.514,
      "language": "en"
    },
    {
      "text": "so this is something\nthat you saw in practice",
      "start": 4385.438,
      "duration": 2.917,
      "language": "en"
    },
    {
      "text": "not too long ago. So then as a bit of teaser,",
      "start": 4388.355,
      "duration": 5.4780000000000655,
      "language": "en"
    },
    {
      "text": "tying this all back together, the way that this image\nclassification pipeline",
      "start": 4395.751,
      "duration": 4.791999999999462,
      "language": "en"
    },
    {
      "text": "might have looked like, maybe about five to 10 years ago,",
      "start": 4400.543,
      "duration": 2.981999999999971,
      "language": "en"
    },
    {
      "text": "would be that you would take your image, and then compute these different\nfeature representations",
      "start": 4403.525,
      "duration": 3.9520000000002256,
      "language": "en"
    },
    {
      "text": "of your image, things like bag of words,",
      "start": 4407.477,
      "duration": 2.132,
      "language": "en"
    },
    {
      "text": "or histogram of orientated gradients,",
      "start": 4409.609,
      "duration": 2.364,
      "language": "en"
    },
    {
      "text": "concatenate a whole bunch\nof features together,",
      "start": 4411.973,
      "duration": 2.208,
      "language": "en"
    },
    {
      "text": "and then feed these feature extractors",
      "start": 4414.181,
      "duration": 2.138,
      "language": "en"
    },
    {
      "text": "down into some linear classifier.",
      "start": 4416.319,
      "duration": 3.071,
      "language": "en"
    },
    {
      "text": "I'm simplifying a little bit, the pipelines were a little\nbit more complex than that,",
      "start": 4419.39,
      "duration": 3.4279999999998836,
      "language": "en"
    },
    {
      "text": "but this is the general intuition.",
      "start": 4422.818,
      "duration": 2.558,
      "language": "en"
    },
    {
      "text": "And then the idea here was\nthat after you extracted",
      "start": 4425.376,
      "duration": 3.582,
      "language": "en"
    },
    {
      "text": "these features, this feature extractor",
      "start": 4428.958,
      "duration": 2.038,
      "language": "en"
    },
    {
      "text": "would be a fixed block\nthat would not be updated",
      "start": 4430.996,
      "duration": 2.13,
      "language": "en"
    },
    {
      "text": "during training. And during training,",
      "start": 4433.126,
      "duration": 2.069999999999709,
      "language": "en"
    },
    {
      "text": "you would only update\nthe linear classifier if it's working on top of features.",
      "start": 4435.196,
      "duration": 3.511000000000422,
      "language": "en"
    },
    {
      "text": "And actually, I would\nargue that once we move",
      "start": 4438.707,
      "duration": 2.065,
      "language": "en"
    },
    {
      "text": "to convolutional neural networks, and these deep neural networks,",
      "start": 4440.772,
      "duration": 3.1679999999996653,
      "language": "en"
    },
    {
      "text": "it actually doesn't look that different.",
      "start": 4443.94,
      "duration": 3.346,
      "language": "en"
    },
    {
      "text": "The only difference is that\nrather than writing down",
      "start": 4447.286,
      "duration": 2.165,
      "language": "en"
    },
    {
      "text": "the features ahead of time, we're going to learn the\nfeatures directly from the data.",
      "start": 4449.451,
      "duration": 4.036000000000058,
      "language": "en"
    },
    {
      "text": "So we'll take our raw pixels and feed them",
      "start": 4453.487,
      "duration": 3.229,
      "language": "en"
    },
    {
      "text": "into this to convolutional network, which will end up computing\nthrough many different layers",
      "start": 4456.716,
      "duration": 3.770999999999731,
      "language": "en"
    },
    {
      "text": "some type of feature representation driven by the data, and\nthen we'll actually train",
      "start": 4460.487,
      "duration": 3.771999999999025,
      "language": "en"
    },
    {
      "text": "this entire weights for\nthis entire network,",
      "start": 4464.259,
      "duration": 2.661,
      "language": "en"
    },
    {
      "text": "rather than just the\nweights of linear classifier on top.",
      "start": 4466.92,
      "duration": 2.6669999999994616,
      "language": "en"
    },
    {
      "text": "So, next time we'll really\nstart diving into this idea",
      "start": 4471.129,
      "duration": 2.641,
      "language": "en"
    },
    {
      "text": "in more detail, and we'll\nintroduce some neural networks,",
      "start": 4473.77,
      "duration": 3.161,
      "language": "en"
    },
    {
      "text": "and start talking about\nbackpropagation as well.",
      "start": 4476.931,
      "duration": 4.0,
      "language": "en"
    }
  ]
}