{
  "video_id": "d14TUNcbn1k",
  "created_at": "2025-07-26T20:38:29.708283",
  "segment_count": 1219,
  "metadata": {
    "video_id": "d14TUNcbn1k",
    "language": "en",
    "segment_count": 1219
  },
  "transcript": [
    {
      "text": "[students murmuring]",
      "start": 6.804,
      "duration": 3.806,
      "language": "en"
    },
    {
      "text": "- Okay, so good afternoon\neveryone, let's get started.",
      "start": 10.61,
      "duration": 4.765,
      "language": "en"
    },
    {
      "text": "So hi, so for those of\nyou who I haven't met yet,",
      "start": 15.375,
      "duration": 2.882,
      "language": "en"
    },
    {
      "text": "my name is Serena Yeung and I'm the third",
      "start": 18.257,
      "duration": 3.213,
      "language": "en"
    },
    {
      "text": "and final instructor for this class,",
      "start": 21.47,
      "duration": 3.383,
      "language": "en"
    },
    {
      "text": "and I'm also a PhD student\nin Fei-Fei's group.",
      "start": 24.853,
      "duration": 3.454,
      "language": "en"
    },
    {
      "text": "Okay, so today we're going\nto talk about backpropagation",
      "start": 28.307,
      "duration": 2.985,
      "language": "en"
    },
    {
      "text": "and neural networks, and so\nnow we're really starting",
      "start": 31.292,
      "duration": 2.551,
      "language": "en"
    },
    {
      "text": "to get to some of the core\nmaterial in this class.",
      "start": 33.843,
      "duration": 3.503,
      "language": "en"
    },
    {
      "text": "Before we begin, let's see, oh.",
      "start": 37.346,
      "duration": 2.583,
      "language": "en"
    },
    {
      "text": "So a few administrative details,",
      "start": 42.124,
      "duration": 2.042,
      "language": "en"
    },
    {
      "text": "so assignment one is due\nThursday, April 20th,",
      "start": 44.166,
      "duration": 3.436,
      "language": "en"
    },
    {
      "text": "so a reminder, we shifted\nthe date back by a little bit",
      "start": 47.602,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "and it's going to be due\n11:59 p.m. on Canvas.",
      "start": 51.522,
      "duration": 3.583,
      "language": "en"
    },
    {
      "text": "So you should start thinking\nabout your projects,",
      "start": 56.722,
      "duration": 2.594,
      "language": "en"
    },
    {
      "text": "there are TA specialties\nlisted on the Piazza website",
      "start": 59.316,
      "duration": 3.011,
      "language": "en"
    },
    {
      "text": "so if you have questions\nabout a specific project topic",
      "start": 62.327,
      "duration": 3.105,
      "language": "en"
    },
    {
      "text": "you're thinking about, you\ncan go and try and find",
      "start": 65.432,
      "duration": 3.526,
      "language": "en"
    },
    {
      "text": "the TAs that might be most relevant.",
      "start": 68.958,
      "duration": 3.724,
      "language": "en"
    },
    {
      "text": "And then also for Google Cloud,\nso all students are going",
      "start": 72.682,
      "duration": 3.228,
      "language": "en"
    },
    {
      "text": "to get $100 in credits\nto use for Google Cloud",
      "start": 75.91,
      "duration": 3.22,
      "language": "en"
    },
    {
      "text": "for their assignments and project,",
      "start": 79.13,
      "duration": 2.138,
      "language": "en"
    },
    {
      "text": "so you should be receiving an email for that this week, I think.",
      "start": 81.268,
      "duration": 3.016999999999996,
      "language": "en"
    },
    {
      "text": "A lot of you may have already, and then for those of you who haven't,\nthey're going to come,",
      "start": 84.285,
      "duration": 3.9420000000000073,
      "language": "en"
    },
    {
      "text": "should be by the end of this week.",
      "start": 88.227,
      "duration": 2.833,
      "language": "en"
    },
    {
      "text": "Okay so where we are, so\nfar we've talked about",
      "start": 92.544,
      "duration": 3.177,
      "language": "en"
    },
    {
      "text": "how to define a classifier\nusing a function f,",
      "start": 95.721,
      "duration": 3.486,
      "language": "en"
    },
    {
      "text": "parameterized by weights\nW, and this function f",
      "start": 99.207,
      "duration": 2.462,
      "language": "en"
    },
    {
      "text": "is going to take data x as input,\nand output a vector of scores",
      "start": 101.669,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "for each of the classes\nthat you want to classify.",
      "start": 108.027,
      "duration": 3.405,
      "language": "en"
    },
    {
      "text": "And so from here we can also define",
      "start": 111.432,
      "duration": 3.346,
      "language": "en"
    },
    {
      "text": "a loss function, so for\nexample, the SVM loss function",
      "start": 114.778,
      "duration": 2.536,
      "language": "en"
    },
    {
      "text": "that we've talked about\nwhich basically quantifies",
      "start": 117.314,
      "duration": 3.131,
      "language": "en"
    },
    {
      "text": "how happy or unhappy we are with the scores that we've produced, right,",
      "start": 120.445,
      "duration": 3.7220000000000084,
      "language": "en"
    },
    {
      "text": "and then we can use that to\ndefine a total loss term.",
      "start": 124.167,
      "duration": 3.56,
      "language": "en"
    },
    {
      "text": "So L here, which is a\ncombination of this data term,",
      "start": 127.727,
      "duration": 3.03,
      "language": "en"
    },
    {
      "text": "combined with a regularization\nterm that expresses",
      "start": 130.757,
      "duration": 3.86,
      "language": "en"
    },
    {
      "text": "how simple our model is,\nand we have a preference for simpler models, for\nbetter generalization.",
      "start": 134.617,
      "duration": 5.584000000000003,
      "language": "en"
    },
    {
      "text": "And so now we want to\nfind the parameters W",
      "start": 140.201,
      "duration": 3.094,
      "language": "en"
    },
    {
      "text": "that correspond to our lowest loss, right? We want to minimize the loss function,",
      "start": 143.295,
      "duration": 3.7970000000000255,
      "language": "en"
    },
    {
      "text": "and so to do that we want to find the gradient of L with respect to W.",
      "start": 147.092,
      "duration": 4.405000000000001,
      "language": "en"
    },
    {
      "text": "So last lecture we talked\nabout how we can do this",
      "start": 153.275,
      "duration": 2.554,
      "language": "en"
    },
    {
      "text": "using optimization, and we're going to iteratively take steps in the direction",
      "start": 155.829,
      "duration": 4.14700000000002,
      "language": "en"
    },
    {
      "text": "of steepest descent, which is\nthe negative of the gradient,",
      "start": 159.976,
      "duration": 2.841,
      "language": "en"
    },
    {
      "text": "in order to walk down this loss landscape",
      "start": 162.817,
      "duration": 2.002,
      "language": "en"
    },
    {
      "text": "and get to the point\nof lowest loss, right?",
      "start": 164.819,
      "duration": 2.472,
      "language": "en"
    },
    {
      "text": "And we saw how this gradient\ndescent can basically take",
      "start": 167.291,
      "duration": 4.656,
      "language": "en"
    },
    {
      "text": "this trajectory, looking\nlike this image on the right,",
      "start": 171.947,
      "duration": 2.823,
      "language": "en"
    },
    {
      "text": "getting to the bottom\nof your loss landscape.",
      "start": 174.77,
      "duration": 4.523,
      "language": "en"
    },
    {
      "text": "Oh! Okay, and so we also\ntalked about different ways",
      "start": 179.293,
      "duration": 5.082999999999998,
      "language": "en"
    },
    {
      "text": "for computing a gradient, right? We can compute this numerically",
      "start": 184.376,
      "duration": 4.161999999999978,
      "language": "en"
    },
    {
      "text": "using finite difference approximation",
      "start": 188.538,
      "duration": 2.158,
      "language": "en"
    },
    {
      "text": "which is slow and approximate,\nbut at the same time",
      "start": 190.696,
      "duration": 2.748,
      "language": "en"
    },
    {
      "text": "it's really easy to write out, you know you can always\nget the gradient this way.",
      "start": 193.444,
      "duration": 4.14500000000001,
      "language": "en"
    },
    {
      "text": "We also talked about how to\nuse the analytic gradient",
      "start": 197.589,
      "duration": 3.475,
      "language": "en"
    },
    {
      "text": "and computing this is, it's fast",
      "start": 201.064,
      "duration": 2.049,
      "language": "en"
    },
    {
      "text": "and exact once you've\ngotten the expression for",
      "start": 203.113,
      "duration": 2.114,
      "language": "en"
    },
    {
      "text": "the analytic gradient, but\nat the same time you have",
      "start": 205.227,
      "duration": 2.272,
      "language": "en"
    },
    {
      "text": "to do all the math and the\ncalculus to derive this,",
      "start": 207.499,
      "duration": 2.171,
      "language": "en"
    },
    {
      "text": "so it's also, you know, easy\nto make mistakes, right?",
      "start": 209.67,
      "duration": 3.064,
      "language": "en"
    },
    {
      "text": "So in practice what we want\nto do is we want to derive",
      "start": 212.734,
      "duration": 2.191,
      "language": "en"
    },
    {
      "text": "the analytic gradient and use this,",
      "start": 214.925,
      "duration": 2.695,
      "language": "en"
    },
    {
      "text": "but at the same time check\nour implementation using",
      "start": 217.62,
      "duration": 2.217,
      "language": "en"
    },
    {
      "text": "the numerical gradient to make sure that we've gotten all of our math right.",
      "start": 219.837,
      "duration": 4.763000000000005,
      "language": "en"
    },
    {
      "text": "So today we're going to\ntalk about how to compute",
      "start": 226.032,
      "duration": 2.486,
      "language": "en"
    },
    {
      "text": "the analytic gradient for\narbitrarily complex functions,",
      "start": 228.518,
      "duration": 3.743,
      "language": "en"
    },
    {
      "text": "using a framework that I'm going\nto call computational graphs.",
      "start": 232.261,
      "duration": 3.563,
      "language": "en"
    },
    {
      "text": "And so basically what a\ncomputational graph is,",
      "start": 235.824,
      "duration": 2.682,
      "language": "en"
    },
    {
      "text": "is that we can use this\nkind of graph in order",
      "start": 238.506,
      "duration": 2.049,
      "language": "en"
    },
    {
      "text": "to represent any function,\nwhere the nodes of the graph",
      "start": 240.555,
      "duration": 3.455,
      "language": "en"
    },
    {
      "text": "are steps of computation\nthat we go through.",
      "start": 244.01,
      "duration": 2.601,
      "language": "en"
    },
    {
      "text": "So for example, in this example, the linear classifier\nthat we've talked about,",
      "start": 246.611,
      "duration": 4.695000000000022,
      "language": "en"
    },
    {
      "text": "the inputs here are x and W, right,",
      "start": 251.306,
      "duration": 3.431,
      "language": "en"
    },
    {
      "text": "and then this multiplication\nnode represents",
      "start": 254.737,
      "duration": 3.444,
      "language": "en"
    },
    {
      "text": "the matrix multiplier,\nthe multiplication of",
      "start": 258.181,
      "duration": 3.141,
      "language": "en"
    },
    {
      "text": "the parameters W with\nour data x that we have,",
      "start": 261.322,
      "duration": 3.716,
      "language": "en"
    },
    {
      "text": "outputting our vector of scores.",
      "start": 265.038,
      "duration": 2.44,
      "language": "en"
    },
    {
      "text": "And then we have another\ncomputational node which represents our hinge loss, right,",
      "start": 267.478,
      "duration": 4.46999999999997,
      "language": "en"
    },
    {
      "text": "computing our data loss term, Li.",
      "start": 271.948,
      "duration": 3.165,
      "language": "en"
    },
    {
      "text": "And we also have this\nregularization term at",
      "start": 275.113,
      "duration": 3.085,
      "language": "en"
    },
    {
      "text": "the bottom right, so this node which computes our regularization term,",
      "start": 278.198,
      "duration": 4.7660000000000196,
      "language": "en"
    },
    {
      "text": "and then our total loss\nhere at the end, L, is the sum of the regularization\nterm and the data term.",
      "start": 282.964,
      "duration": 6.079999999999984,
      "language": "en"
    },
    {
      "text": "And the advantage is\nthat once we can express a function using a computational graph,",
      "start": 290.624,
      "duration": 4.050999999999988,
      "language": "en"
    },
    {
      "text": "then we can use a technique\nthat we call backpropagation",
      "start": 294.675,
      "duration": 3.428,
      "language": "en"
    },
    {
      "text": "which is going to recursively\nuse the chain rule",
      "start": 298.103,
      "duration": 2.158,
      "language": "en"
    },
    {
      "text": "in order to compute the gradient with respect to every variable\nin the computational graph,",
      "start": 300.261,
      "duration": 5.3069999999999595,
      "language": "en"
    },
    {
      "text": "and so we're going to\nsee how this is done.",
      "start": 305.568,
      "duration": 4.262,
      "language": "en"
    },
    {
      "text": "And this becomes very\nuseful when we start working with really complex functions,",
      "start": 309.83,
      "duration": 3.583000000000027,
      "language": "en"
    },
    {
      "text": "so for example,\nconvolutional neural networks",
      "start": 313.413,
      "duration": 2.089,
      "language": "en"
    },
    {
      "text": "that we're going to talk\nabout later in this class.",
      "start": 315.502,
      "duration": 2.334,
      "language": "en"
    },
    {
      "text": "We have here the input image at the top,",
      "start": 317.836,
      "duration": 3.064,
      "language": "en"
    },
    {
      "text": "we have our loss at the bottom, and the input has to\ngo through many layers",
      "start": 320.9,
      "duration": 3.1229999999999905,
      "language": "en"
    },
    {
      "text": "of transformations in order to get all",
      "start": 324.023,
      "duration": 2.246,
      "language": "en"
    },
    {
      "text": "the way down to the loss function.",
      "start": 326.269,
      "duration": 2.833,
      "language": "en"
    },
    {
      "text": "And this can get even\ncrazier with things like,",
      "start": 330.896,
      "duration": 2.269,
      "language": "en"
    },
    {
      "text": "the, you know, like a\nneural turing machine,",
      "start": 333.165,
      "duration": 2.282,
      "language": "en"
    },
    {
      "text": "which is another kind\nof deep learning model,",
      "start": 335.447,
      "duration": 2.422,
      "language": "en"
    },
    {
      "text": "and in this case you can see\nthat the computational graph",
      "start": 337.869,
      "duration": 2.046,
      "language": "en"
    },
    {
      "text": "for this is really insane, and especially,",
      "start": 339.915,
      "duration": 3.498,
      "language": "en"
    },
    {
      "text": "we end up, you know,\nunrolling this over time.",
      "start": 343.413,
      "duration": 2.484,
      "language": "en"
    },
    {
      "text": "It's basically completely impractical if you want to compute the gradients",
      "start": 345.897,
      "duration": 4.004000000000019,
      "language": "en"
    },
    {
      "text": "for any of these intermediate variables.",
      "start": 349.901,
      "duration": 3.333,
      "language": "en"
    },
    {
      "text": "Okay, so how does backpropagation work?",
      "start": 355.56,
      "duration": 3.579,
      "language": "en"
    },
    {
      "text": "So we're going to start\noff with a simple example,",
      "start": 359.139,
      "duration": 2.699,
      "language": "en"
    },
    {
      "text": "where again, our goal is\nthat we have a function. So in this case, f of x, y, z",
      "start": 361.838,
      "duration": 4.389999999999986,
      "language": "en"
    },
    {
      "text": "equals x plus y times z,",
      "start": 366.228,
      "duration": 2.386,
      "language": "en"
    },
    {
      "text": "and we want to find the\ngradients of the output of",
      "start": 368.614,
      "duration": 2.132,
      "language": "en"
    },
    {
      "text": "the function with respect\nto any of the variables.",
      "start": 370.746,
      "duration": 2.826,
      "language": "en"
    },
    {
      "text": "So the first step, always, is we want to take our function f, and we want",
      "start": 373.572,
      "duration": 3.757000000000005,
      "language": "en"
    },
    {
      "text": "to represent it using\na computational graph.",
      "start": 377.329,
      "duration": 3.294,
      "language": "en"
    },
    {
      "text": "Right, so here our computational\ngraph is on the right,",
      "start": 380.623,
      "duration": 2.716,
      "language": "en"
    },
    {
      "text": "and you can see that we have our,",
      "start": 383.339,
      "duration": 2.618,
      "language": "en"
    },
    {
      "text": "first we have the plus node, so x plus y, and then we have this\nmultiplication node, right,",
      "start": 385.957,
      "duration": 4.086999999999989,
      "language": "en"
    },
    {
      "text": "for the second computation\nthat we're doing.",
      "start": 390.044,
      "duration": 4.016,
      "language": "en"
    },
    {
      "text": "And then, now we're going\nto do a forward pass",
      "start": 394.06,
      "duration": 2.141,
      "language": "en"
    },
    {
      "text": "of this network, so given the values of",
      "start": 396.201,
      "duration": 2.531,
      "language": "en"
    },
    {
      "text": "the variables that we have, so here,",
      "start": 398.732,
      "duration": 2.05,
      "language": "en"
    },
    {
      "text": "x equals negative two, y equals five",
      "start": 400.782,
      "duration": 2.162,
      "language": "en"
    },
    {
      "text": "and z equals negative four,\nI'm going to fill these all in",
      "start": 402.944,
      "duration": 3.307,
      "language": "en"
    },
    {
      "text": "in our computational graph,\nand then here we can compute",
      "start": 406.251,
      "duration": 3.166,
      "language": "en"
    },
    {
      "text": "an intermediate value,\nso x plus y gives three,",
      "start": 409.417,
      "duration": 3.548,
      "language": "en"
    },
    {
      "text": "and then finally we pass it through again,",
      "start": 412.965,
      "duration": 2.08,
      "language": "en"
    },
    {
      "text": "through the last node, the multiplication, to get our final node\nof f equals negative 12.",
      "start": 415.045,
      "duration": 5.77800000000002,
      "language": "en"
    },
    {
      "text": "So here we want to give every\nintermediate variable a name.",
      "start": 424.31,
      "duration": 4.474,
      "language": "en"
    },
    {
      "text": "So here I've called this\nintermediate variable after the plus node q, and we\nhave q equals x plus y,",
      "start": 428.784,
      "duration": 6.213000000000022,
      "language": "en"
    },
    {
      "text": "and then f equals q times z,\nusing this intermediate node.",
      "start": 434.997,
      "duration": 3.612,
      "language": "en"
    },
    {
      "text": "And I've also written\nout here, the gradients",
      "start": 438.609,
      "duration": 2.732,
      "language": "en"
    },
    {
      "text": "of q with respect to x\nand y, which are just one",
      "start": 441.341,
      "duration": 3.422,
      "language": "en"
    },
    {
      "text": "because of the addition,\nand then the gradients of f",
      "start": 444.763,
      "duration": 3.368,
      "language": "en"
    },
    {
      "text": "with respect to q and z,\nwhich is z and q respectively",
      "start": 448.131,
      "duration": 3.522,
      "language": "en"
    },
    {
      "text": "because of the multiplication rule.",
      "start": 451.653,
      "duration": 2.954,
      "language": "en"
    },
    {
      "text": "And so what we want to\nfind, is we want to find the gradients of f with\nrespect to x, y and z.",
      "start": 454.607,
      "duration": 5.824000000000012,
      "language": "en"
    },
    {
      "text": "So what backprop is, it's\na recursive application of",
      "start": 463.356,
      "duration": 3.466,
      "language": "en"
    },
    {
      "text": "the chain rule, so we're\ngoing to start at the back,",
      "start": 466.822,
      "duration": 2.36,
      "language": "en"
    },
    {
      "text": "the very end of the computational graph,",
      "start": 469.182,
      "duration": 2.129,
      "language": "en"
    },
    {
      "text": "and then we're going to\nwork our way backwards",
      "start": 471.311,
      "duration": 2.049,
      "language": "en"
    },
    {
      "text": "and compute all the\ngradients along the way.",
      "start": 473.36,
      "duration": 3.013,
      "language": "en"
    },
    {
      "text": "So here if we start at\nthe very end, right,",
      "start": 476.373,
      "duration": 2.642,
      "language": "en"
    },
    {
      "text": "we want to compute the\ngradient of the output",
      "start": 479.015,
      "duration": 2.202,
      "language": "en"
    },
    {
      "text": "with respect to the last\nvariable, which is just f.",
      "start": 481.217,
      "duration": 3.457,
      "language": "en"
    },
    {
      "text": "And so this gradient is\njust one, it's trivial.",
      "start": 484.674,
      "duration": 3.917,
      "language": "en"
    },
    {
      "text": "So now, moving backwards,\nwe want the gradient",
      "start": 490.065,
      "duration": 2.539,
      "language": "en"
    },
    {
      "text": "with respect to z, right, and we know",
      "start": 492.604,
      "duration": 3.083,
      "language": "en"
    },
    {
      "text": "that df over dz is equal to q.",
      "start": 496.637,
      "duration": 2.5,
      "language": "en"
    },
    {
      "text": "So the value of q is just three,",
      "start": 499.993,
      "duration": 2.643,
      "language": "en"
    },
    {
      "text": "and so we have here, df\nover dz equals three.",
      "start": 502.636,
      "duration": 3.75,
      "language": "en"
    },
    {
      "text": "And so next if we want to do df over dq,",
      "start": 508.819,
      "duration": 2.869,
      "language": "en"
    },
    {
      "text": "what is the value of that?",
      "start": 511.688,
      "duration": 2.167,
      "language": "en"
    },
    {
      "text": "What is df over dq? So we have here, df over\ndq is equal to z, right,",
      "start": 516.732,
      "duration": 5.307999999999993,
      "language": "en"
    },
    {
      "text": "and the value of z is negative four.",
      "start": 526.012,
      "duration": 3.0,
      "language": "en"
    },
    {
      "text": "So here we have df over dq\nis equal to negative four.",
      "start": 529.963,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "Okay, so now continuing to\nmove backwards to the graph,",
      "start": 537.485,
      "duration": 3.317,
      "language": "en"
    },
    {
      "text": "we want to find df over dy, right,",
      "start": 540.802,
      "duration": 2.991,
      "language": "en"
    },
    {
      "text": "but here in this case, the\ngradient with respect to y,",
      "start": 543.793,
      "duration": 2.458,
      "language": "en"
    },
    {
      "text": "y is not connected directly to f, right?",
      "start": 546.251,
      "duration": 2.735,
      "language": "en"
    },
    {
      "text": "It's connected through an\nintermediate node of z,",
      "start": 548.986,
      "duration": 3.852,
      "language": "en"
    },
    {
      "text": "and so the way we're going to do this is we can leverage the\nchain rule which says",
      "start": 552.838,
      "duration": 5.159999999999968,
      "language": "en"
    },
    {
      "text": "that df over dy can be\nwritten as df over dq,",
      "start": 557.998,
      "duration": 3.75,
      "language": "en"
    },
    {
      "text": "times dq over dy, and\nso the intuition of this",
      "start": 562.746,
      "duration": 4.008,
      "language": "en"
    },
    {
      "text": "is that in order to get to\nfind the effect of y on f,",
      "start": 566.754,
      "duration": 3.953,
      "language": "en"
    },
    {
      "text": "this is actually equivalent to if we take",
      "start": 570.707,
      "duration": 2.641,
      "language": "en"
    },
    {
      "text": "the effect of q times q on f,\nwhich we already know, right?",
      "start": 573.348,
      "duration": 4.068,
      "language": "en"
    },
    {
      "text": "df over dq is equal to negative four,",
      "start": 577.416,
      "duration": 3.914,
      "language": "en"
    },
    {
      "text": "and we compound it with the\neffect of y on q, dq over dy.",
      "start": 581.33,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "So what's dq over dy\nequal to in this case?",
      "start": 586.604,
      "duration": 4.382,
      "language": "en"
    },
    {
      "text": "- [Student] One. - One, right. Exactly. So dq over dy is equal to\none, which means, you know,",
      "start": 590.986,
      "duration": 4.930000000000064,
      "language": "en"
    },
    {
      "text": "if we change y by a little bit,",
      "start": 595.916,
      "duration": 2.068,
      "language": "en"
    },
    {
      "text": "q is going to change by approximately the same amount right, this is the effect,",
      "start": 597.984,
      "duration": 3.643000000000029,
      "language": "en"
    },
    {
      "text": "and so what this is\ndoing is this is saying,",
      "start": 601.627,
      "duration": 3.958,
      "language": "en"
    },
    {
      "text": "well if I change y by a little bit, the effect of y on q is going to be one,",
      "start": 605.585,
      "duration": 5.22199999999998,
      "language": "en"
    },
    {
      "text": "and then the effect of q on f\nis going to be approximately",
      "start": 613.249,
      "duration": 3.633,
      "language": "en"
    },
    {
      "text": "a factor of negative four, right? So then we multiply these together",
      "start": 616.882,
      "duration": 3.5230000000001382,
      "language": "en"
    },
    {
      "text": "and we get that the effect of y on f",
      "start": 620.405,
      "duration": 3.648,
      "language": "en"
    },
    {
      "text": "is going to be negative four.",
      "start": 624.053,
      "duration": 2.417,
      "language": "en"
    },
    {
      "text": "Okay, so now if we want\nto do the same thing for",
      "start": 630.887,
      "duration": 2.362,
      "language": "en"
    },
    {
      "text": "the gradient with respect to x, right,",
      "start": 633.249,
      "duration": 2.383,
      "language": "en"
    },
    {
      "text": "we can do the, we can\nfollow the same procedure,",
      "start": 635.632,
      "duration": 2.442,
      "language": "en"
    },
    {
      "text": "and so what is this going to be?",
      "start": 638.074,
      "duration": 3.117,
      "language": "en"
    },
    {
      "text": "[students speaking away from microphone] - I heard the same.",
      "start": 641.191,
      "duration": 3.55499999999995,
      "language": "en"
    },
    {
      "text": "Yeah exactly, so in this\ncase we want to, again,",
      "start": 644.746,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "apply the chain rule, right? We know the effect of q on\nf is negative four,",
      "start": 649.636,
      "duration": 6.246000000000095,
      "language": "en"
    },
    {
      "text": "and here again, since we have\nalso the same addition node,",
      "start": 655.882,
      "duration": 3.285,
      "language": "en"
    },
    {
      "text": "dq over dx is equal to one, again,",
      "start": 659.167,
      "duration": 2.791,
      "language": "en"
    },
    {
      "text": "we have negative four times\none, right, and the gradient",
      "start": 661.958,
      "duration": 2.635,
      "language": "en"
    },
    {
      "text": "with respect to x is\ngoing to be negative four.",
      "start": 664.593,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "Okay, so what we're doing is, in backprop, is we basically have all of these nodes",
      "start": 671.467,
      "duration": 3.9220000000000255,
      "language": "en"
    },
    {
      "text": "in our computational graph, but each node",
      "start": 675.389,
      "duration": 2.457,
      "language": "en"
    },
    {
      "text": "is only aware of its\nimmediate surroundings, right?",
      "start": 677.846,
      "duration": 2.878,
      "language": "en"
    },
    {
      "text": "So we have, at each node,\nwe have the local inputs",
      "start": 680.724,
      "duration": 3.15,
      "language": "en"
    },
    {
      "text": "that are connected to this node, the values that are flowing into the node,",
      "start": 683.874,
      "duration": 3.4379999999998745,
      "language": "en"
    },
    {
      "text": "and then we also have the output that is directly outputted from this node.",
      "start": 687.312,
      "duration": 5.1200000000000045,
      "language": "en"
    },
    {
      "text": "So here our local inputs are\nx and y, and the output is z.",
      "start": 692.432,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "And at this node we also know\nthe local gradient, right,",
      "start": 699.777,
      "duration": 3.692,
      "language": "en"
    },
    {
      "text": "we can compute the gradient\nof z with respect to x,",
      "start": 703.469,
      "duration": 3.138,
      "language": "en"
    },
    {
      "text": "and the gradient of z with respect to y,",
      "start": 706.607,
      "duration": 2.298,
      "language": "en"
    },
    {
      "text": "and these are usually really\nsimple operations, right?",
      "start": 708.905,
      "duration": 2.312,
      "language": "en"
    },
    {
      "text": "Each node is going to be something like the addition or the multiplication",
      "start": 711.217,
      "duration": 3.604000000000042,
      "language": "en"
    },
    {
      "text": "that we had in that earlier example, which is something where\nwe can just write down",
      "start": 714.821,
      "duration": 3.4549999999999272,
      "language": "en"
    },
    {
      "text": "the gradient, and we\ndon't have to, you know, go through very complex\ncalculus in order to find this.",
      "start": 718.276,
      "duration": 6.38900000000001,
      "language": "en"
    },
    {
      "text": "- [Student] Can you go\nback and explain why",
      "start": 724.665,
      "duration": 2.848,
      "language": "en"
    },
    {
      "text": "more in the last slide was\ndifferent than planning",
      "start": 727.513,
      "duration": 4.186,
      "language": "en"
    },
    {
      "text": "the first part of it using\njust normal calculus?",
      "start": 731.699,
      "duration": 3.614,
      "language": "en"
    },
    {
      "text": "- Yeah, so basically if we go back,",
      "start": 735.313,
      "duration": 3.37,
      "language": "en"
    },
    {
      "text": "hold on, let me... So if we go back here, we\ncould exactly write out,",
      "start": 738.683,
      "duration": 5.788999999999987,
      "language": "en"
    },
    {
      "text": "find all of these using just calculus,",
      "start": 744.472,
      "duration": 2.093,
      "language": "en"
    },
    {
      "text": "so we could say, you know,\nwe want df over dx, right,",
      "start": 746.565,
      "duration": 3.651,
      "language": "en"
    },
    {
      "text": "and we can probably\nexpand out this expression",
      "start": 750.216,
      "duration": 2.356,
      "language": "en"
    },
    {
      "text": "and see that it's just going to be z,",
      "start": 752.572,
      "duration": 2.766,
      "language": "en"
    },
    {
      "text": "but we can do this for, in this case, because it's simple, but\nwe'll see examples later on",
      "start": 755.338,
      "duration": 4.188000000000102,
      "language": "en"
    },
    {
      "text": "where once this becomes a\nreally complicated expression,",
      "start": 759.526,
      "duration": 3.141,
      "language": "en"
    },
    {
      "text": "you don't want to have to use calculus",
      "start": 762.667,
      "duration": 2.249,
      "language": "en"
    },
    {
      "text": "to derive, right, the\ngradient for something,",
      "start": 764.916,
      "duration": 2.571,
      "language": "en"
    },
    {
      "text": "for a super-complicated expression, and instead, if you use this formalism",
      "start": 767.487,
      "duration": 4.607000000000085,
      "language": "en"
    },
    {
      "text": "and you break it down into\nthese computational nodes,",
      "start": 772.094,
      "duration": 2.924,
      "language": "en"
    },
    {
      "text": "then you can only ever work with gradients",
      "start": 775.018,
      "duration": 2.983,
      "language": "en"
    },
    {
      "text": "of very simple computations, right,",
      "start": 778.001,
      "duration": 3.611,
      "language": "en"
    },
    {
      "text": "at the level of, you know,\nadditions, multiplications,",
      "start": 781.612,
      "duration": 3.313,
      "language": "en"
    },
    {
      "text": "exponentials, things as\nsimple as you want them,",
      "start": 784.925,
      "duration": 2.013,
      "language": "en"
    },
    {
      "text": "and then you just use the chain rule to multiply all these together,",
      "start": 786.938,
      "duration": 3.1000000000000227,
      "language": "en"
    },
    {
      "text": "and get your, the value of your gradient",
      "start": 790.038,
      "duration": 2.366,
      "language": "en"
    },
    {
      "text": "without having to ever\nderive the entire expression.",
      "start": 792.404,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "Does that make sense? [student murmuring]",
      "start": 798.562,
      "duration": 2.8050000000000637,
      "language": "en"
    },
    {
      "text": "Okay, so we'll see an\nexample of this later.",
      "start": 801.367,
      "duration": 3.667,
      "language": "en"
    },
    {
      "text": "And so, was there another question, yeah? [student speaking away from microphone]",
      "start": 808.751,
      "duration": 3.4890000000000327,
      "language": "en"
    },
    {
      "text": "- [Student] What's the negative four next to the z representing?",
      "start": 812.24,
      "duration": 3.9060000000000628,
      "language": "en"
    },
    {
      "text": "- Negative, okay yeah,\nso the negative four,",
      "start": 816.146,
      "duration": 2.262,
      "language": "en"
    },
    {
      "text": "these were the, the green values on top were all the values of\nthe function as we passed",
      "start": 818.408,
      "duration": 5.423000000000002,
      "language": "en"
    },
    {
      "text": "it forward through the\ncomputational graph, right?",
      "start": 823.831,
      "duration": 2.792,
      "language": "en"
    },
    {
      "text": "So we said up here that x\nis equal to negative two,",
      "start": 826.623,
      "duration": 3.212,
      "language": "en"
    },
    {
      "text": "y is equal to five, and\nz equals negative four,",
      "start": 829.835,
      "duration": 2.756,
      "language": "en"
    },
    {
      "text": "so we filled in all of these\nvalues, and then we just wanted",
      "start": 832.591,
      "duration": 3.03,
      "language": "en"
    },
    {
      "text": "to compute the value of this function.",
      "start": 835.621,
      "duration": 3.665,
      "language": "en"
    },
    {
      "text": "Right, so we said this value\nof q is going to be x plus y,",
      "start": 839.286,
      "duration": 4.722,
      "language": "en"
    },
    {
      "text": "it's going to be negative\ntwo plus five, it is going",
      "start": 844.008,
      "duration": 2.11,
      "language": "en"
    },
    {
      "text": "to be three, and we have z\nis equal to negative four",
      "start": 846.118,
      "duration": 3.171,
      "language": "en"
    },
    {
      "text": "so we fill that in here,\nand then we multiplied q",
      "start": 849.289,
      "duration": 2.996,
      "language": "en"
    },
    {
      "text": "and z together, negative four times three in order to get the\nfinal value of f, right?",
      "start": 852.285,
      "duration": 4.600999999999999,
      "language": "en"
    },
    {
      "text": "And then the red values underneath were as we were filling in the gradients",
      "start": 856.886,
      "duration": 3.6539999999999964,
      "language": "en"
    },
    {
      "text": "as we were working backwards.",
      "start": 860.54,
      "duration": 2.417,
      "language": "en"
    },
    {
      "text": "Okay. Okay, so right, so we said that, you know,",
      "start": 864.927,
      "duration": 8.428999999999974,
      "language": "en"
    },
    {
      "text": "we have these local, these nodes, and each node basically gets\nits local inputs coming in",
      "start": 873.356,
      "duration": 5.613000000000056,
      "language": "en"
    },
    {
      "text": "and the output that it\nsees directly passing on to the next node, and we also\nhave these local gradients",
      "start": 878.969,
      "duration": 5.252999999999929,
      "language": "en"
    },
    {
      "text": "that we computed, right, the gradient of",
      "start": 884.222,
      "duration": 2.08,
      "language": "en"
    },
    {
      "text": "the immediate output of the node",
      "start": 886.302,
      "duration": 2.174,
      "language": "en"
    },
    {
      "text": "with respect to the inputs coming in.",
      "start": 888.476,
      "duration": 2.699,
      "language": "en"
    },
    {
      "text": "And so what happens during\nbackprop is we have these,",
      "start": 891.175,
      "duration": 3.98,
      "language": "en"
    },
    {
      "text": "we'll start from the\nback of the graph, right, and then we work our way from the end",
      "start": 895.155,
      "duration": 3.316000000000031,
      "language": "en"
    },
    {
      "text": "all the way back to the beginning, and when we reach each\nnode, at each node we have",
      "start": 898.471,
      "duration": 4.644999999999982,
      "language": "en"
    },
    {
      "text": "the upstream gradients coming back, right,",
      "start": 903.116,
      "duration": 2.477,
      "language": "en"
    },
    {
      "text": "with respect to the\nimmediate output of the node.",
      "start": 905.593,
      "duration": 3.387,
      "language": "en"
    },
    {
      "text": "So by the time we reach\nthis node in backprop,",
      "start": 908.98,
      "duration": 2.646,
      "language": "en"
    },
    {
      "text": "we've already computed the gradient of our final loss l,\nwith respect to z, right?",
      "start": 911.626,
      "duration": 6.182000000000016,
      "language": "en"
    },
    {
      "text": "And so now what we want to find next",
      "start": 917.808,
      "duration": 2.502,
      "language": "en"
    },
    {
      "text": "is we want to find the\ngradients with respect",
      "start": 920.31,
      "duration": 2.198,
      "language": "en"
    },
    {
      "text": "to just before the node,\nto the values of x and y.",
      "start": 922.508,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "And so as we saw earlier, we\ndo this using the chain rule,",
      "start": 927.679,
      "duration": 3.283,
      "language": "en"
    },
    {
      "text": "right, we have from the chain rule,",
      "start": 930.962,
      "duration": 2.091,
      "language": "en"
    },
    {
      "text": "that the gradient of this loss function with respect to x is going to be",
      "start": 933.053,
      "duration": 3.6369999999999436,
      "language": "en"
    },
    {
      "text": "the gradient with respect\nto z times, compounded by",
      "start": 936.69,
      "duration": 4.792,
      "language": "en"
    },
    {
      "text": "this gradient, local gradient\nof z with respect to x.",
      "start": 941.482,
      "duration": 3.505,
      "language": "en"
    },
    {
      "text": "Right, so in the chain rule we always take this upstream gradient coming down,",
      "start": 944.987,
      "duration": 3.435000000000059,
      "language": "en"
    },
    {
      "text": "and we multiply it by the local gradient",
      "start": 948.422,
      "duration": 2.228,
      "language": "en"
    },
    {
      "text": "in order to get the gradient\nwith respect to the input.",
      "start": 950.65,
      "duration": 4.421,
      "language": "en"
    },
    {
      "text": "- [Student] So, sorry, is it,",
      "start": 955.071,
      "duration": 2.278,
      "language": "en"
    },
    {
      "text": "it's different because\nthis would never work",
      "start": 957.349,
      "duration": 2.382,
      "language": "en"
    },
    {
      "text": "to get a general formula into the,",
      "start": 959.731,
      "duration": 2.218,
      "language": "en"
    },
    {
      "text": "or general symbolic\nformula for the gradient.",
      "start": 961.949,
      "duration": 2.529,
      "language": "en"
    },
    {
      "text": "It only works with instantaneous values,",
      "start": 964.478,
      "duration": 2.222,
      "language": "en"
    },
    {
      "text": "where you like. [student coughing] Or passing a little constant\nvalue as a symbolic.",
      "start": 966.7,
      "duration": 5.195999999999913,
      "language": "en"
    },
    {
      "text": "- So the question is\nwhether this only works",
      "start": 971.896,
      "duration": 4.439,
      "language": "en"
    },
    {
      "text": "because we're working\nwith the current values of",
      "start": 976.335,
      "duration": 3.161,
      "language": "en"
    },
    {
      "text": "the function, and so it works, right,",
      "start": 979.496,
      "duration": 3.083,
      "language": "en"
    },
    {
      "text": "given the current values of\nthe function that we plug in, but we can write an expression for this,",
      "start": 983.842,
      "duration": 3.7690000000000055,
      "language": "en"
    },
    {
      "text": "still in terms of the variables, right?",
      "start": 987.611,
      "duration": 2.208,
      "language": "en"
    },
    {
      "text": "So we'll see that gradient\nof L with respect to z",
      "start": 989.819,
      "duration": 3.816,
      "language": "en"
    },
    {
      "text": "is going to be some\nexpression, and gradient of z",
      "start": 993.635,
      "duration": 2.722,
      "language": "en"
    },
    {
      "text": "with respect to x is going to\nbe another expression, right?",
      "start": 996.357,
      "duration": 3.106,
      "language": "en"
    },
    {
      "text": "But we plug in these,\nwe plug in the values",
      "start": 999.463,
      "duration": 3.959,
      "language": "en"
    },
    {
      "text": "of these numbers at the\ntime in order to get",
      "start": 1003.422,
      "duration": 2.059,
      "language": "en"
    },
    {
      "text": "the value of the gradient\nwith respect to x.",
      "start": 1005.481,
      "duration": 3.227,
      "language": "en"
    },
    {
      "text": "So what you could do is you\ncould recursively plug in",
      "start": 1008.708,
      "duration": 3.25,
      "language": "en"
    },
    {
      "text": "all of these expressions, right?",
      "start": 1011.958,
      "duration": 3.245,
      "language": "en"
    },
    {
      "text": "Gradient with respect, z with respect to x",
      "start": 1015.203,
      "duration": 3.036,
      "language": "en"
    },
    {
      "text": "is going to be a simple,\nsimple expression, right?",
      "start": 1018.239,
      "duration": 3.203,
      "language": "en"
    },
    {
      "text": "So in this case, if we\nhave a multiplication node,",
      "start": 1021.442,
      "duration": 2.266,
      "language": "en"
    },
    {
      "text": "gradient of z with\nrespect to x is just going",
      "start": 1023.708,
      "duration": 2.031,
      "language": "en"
    },
    {
      "text": "to be y, right, we know that,",
      "start": 1025.739,
      "duration": 2.873,
      "language": "en"
    },
    {
      "text": "but the gradient of L with respect to z,",
      "start": 1028.612,
      "duration": 2.287,
      "language": "en"
    },
    {
      "text": "this is probably a complex part of the graph in itself, right, so\nhere's where we want to just,",
      "start": 1030.899,
      "duration": 6.455999999999904,
      "language": "en"
    },
    {
      "text": "in this case, have this numerical, right?",
      "start": 1037.355,
      "duration": 3.393,
      "language": "en"
    },
    {
      "text": "So as you said, basically\nthis is going to be just",
      "start": 1040.748,
      "duration": 2.357,
      "language": "en"
    },
    {
      "text": "a number coming down, right, a value,",
      "start": 1043.105,
      "duration": 2.318,
      "language": "en"
    },
    {
      "text": "and then we just multiply it with the expression that we have\nfor the local gradient.",
      "start": 1045.423,
      "duration": 5.296000000000049,
      "language": "en"
    },
    {
      "text": "And I think this will be\nmore clear when we go through a more complicated\nexample in a few slides.",
      "start": 1050.719,
      "duration": 4.928000000000111,
      "language": "en"
    },
    {
      "text": "Okay, so now the gradient\nof L with respect to y,",
      "start": 1058.225,
      "duration": 2.46,
      "language": "en"
    },
    {
      "text": "we have exactly the\nsame idea, where again,",
      "start": 1060.685,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "we use the chain rule,\nwe have gradient of L with respect to z, times the gradient of z",
      "start": 1064.284,
      "duration": 3.8849999999997635,
      "language": "en"
    },
    {
      "text": "with respect to y, right,\nwe use the chain rule,",
      "start": 1068.169,
      "duration": 2.492,
      "language": "en"
    },
    {
      "text": "multiply these together\nand get our gradient.",
      "start": 1070.661,
      "duration": 3.75,
      "language": "en"
    },
    {
      "text": "And then once we have these,\nwe'll pass these on to",
      "start": 1075.848,
      "duration": 2.062,
      "language": "en"
    },
    {
      "text": "the node directly before,\nor connected to this node.",
      "start": 1077.91,
      "duration": 2.906,
      "language": "en"
    },
    {
      "text": "And so the main thing\nto take away from this",
      "start": 1080.816,
      "duration": 2.668,
      "language": "en"
    },
    {
      "text": "is that at each node we just\nwant to have our local gradient",
      "start": 1083.484,
      "duration": 3.436,
      "language": "en"
    },
    {
      "text": "that we compute, just keep track of this,",
      "start": 1086.92,
      "duration": 2.127,
      "language": "en"
    },
    {
      "text": "and then during backprop as\nwe're receiving, you know,",
      "start": 1089.047,
      "duration": 3.235,
      "language": "en"
    },
    {
      "text": "numerical values of gradients\ncoming from upstream,",
      "start": 1092.282,
      "duration": 3.845,
      "language": "en"
    },
    {
      "text": "we just take what that is, multiply it by",
      "start": 1096.127,
      "duration": 2.144,
      "language": "en"
    },
    {
      "text": "the local gradient, and then this is what we then send back\nto the connected nodes,",
      "start": 1098.271,
      "duration": 5.470000000000027,
      "language": "en"
    },
    {
      "text": "the next nodes going backwards,\nwithout having to care",
      "start": 1103.741,
      "duration": 3.434,
      "language": "en"
    },
    {
      "text": "about anything else besides\nthese immediate surroundings.",
      "start": 1107.175,
      "duration": 4.489,
      "language": "en"
    },
    {
      "text": "So now we're going to go\nthrough another example,",
      "start": 1111.664,
      "duration": 2.131,
      "language": "en"
    },
    {
      "text": "this time a little bit more complex, so we can see more why\nbackprop is so useful.",
      "start": 1113.795,
      "duration": 5.44399999999996,
      "language": "en"
    },
    {
      "text": "So in this case, our\nfunction is f of w and x,",
      "start": 1119.239,
      "duration": 4.654,
      "language": "en"
    },
    {
      "text": "which is equal to one over one plus e",
      "start": 1123.893,
      "duration": 2.733,
      "language": "en"
    },
    {
      "text": "to the negative of w-zero times x-zero",
      "start": 1126.626,
      "duration": 2.95,
      "language": "en"
    },
    {
      "text": "plus w-one x-one, plus w-two, right?",
      "start": 1129.576,
      "duration": 3.043,
      "language": "en"
    },
    {
      "text": "So again, the first step always is we want",
      "start": 1132.619,
      "duration": 2.359,
      "language": "en"
    },
    {
      "text": "to write this out as\na computational graph.",
      "start": 1134.978,
      "duration": 2.547,
      "language": "en"
    },
    {
      "text": "So in this case we can see\nthat in this graph, right,",
      "start": 1137.525,
      "duration": 2.273,
      "language": "en"
    },
    {
      "text": "first we multiply together the\nw and x terms that we have,",
      "start": 1139.798,
      "duration": 3.065,
      "language": "en"
    },
    {
      "text": "w-zero with x-zero, w-one with x-one,",
      "start": 1142.863,
      "duration": 3.834,
      "language": "en"
    },
    {
      "text": "and w-two, then we add all\nof these together, right?",
      "start": 1146.697,
      "duration": 3.691,
      "language": "en"
    },
    {
      "text": "Then we do, scale it by negative one,",
      "start": 1150.388,
      "duration": 3.083,
      "language": "en"
    },
    {
      "text": "we take the exponential, we add one,",
      "start": 1154.525,
      "duration": 2.847,
      "language": "en"
    },
    {
      "text": "and then finally we do\none over this whole term.",
      "start": 1157.372,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "And then here I've also\nfilled in values of these,",
      "start": 1162.533,
      "duration": 2.637,
      "language": "en"
    },
    {
      "text": "so let's say given values that we have",
      "start": 1165.17,
      "duration": 2.411,
      "language": "en"
    },
    {
      "text": "for the ws and xs, right,\nwe can make a forward pass",
      "start": 1167.581,
      "duration": 3.145,
      "language": "en"
    },
    {
      "text": "and basically compute what the value is at every stage of the computation.",
      "start": 1170.726,
      "duration": 4.444999999999936,
      "language": "en"
    },
    {
      "text": "And here I've also written\ndown here at the bottom",
      "start": 1177.091,
      "duration": 2.965,
      "language": "en"
    },
    {
      "text": "the values, the expressions\nfor some derivatives",
      "start": 1180.056,
      "duration": 2.93,
      "language": "en"
    },
    {
      "text": "that are going to be helpful later on, so same as we did before\nwith the simple example.",
      "start": 1182.986,
      "duration": 6.352999999999838,
      "language": "en"
    },
    {
      "text": "Okay, so now then we're going\nto do backprop through here,",
      "start": 1189.339,
      "duration": 2.481,
      "language": "en"
    },
    {
      "text": "right, so again, we're going to start at the very end of the\ngraph, and so here again",
      "start": 1191.82,
      "duration": 4.83400000000006,
      "language": "en"
    },
    {
      "text": "the gradient of the output with\nrespect to the last variable",
      "start": 1196.654,
      "duration": 4.082,
      "language": "en"
    },
    {
      "text": "is just one, it's just trivial,",
      "start": 1200.736,
      "duration": 3.338,
      "language": "en"
    },
    {
      "text": "and so now moving\nbackwards one step, right?",
      "start": 1204.074,
      "duration": 3.249,
      "language": "en"
    },
    {
      "text": "So what's the gradient with respect to",
      "start": 1207.323,
      "duration": 3.445,
      "language": "en"
    },
    {
      "text": "the input just before one over x?",
      "start": 1210.768,
      "duration": 2.637,
      "language": "en"
    },
    {
      "text": "Well, so in this case, we know\nthat the upstream gradient",
      "start": 1213.405,
      "duration": 4.845,
      "language": "en"
    },
    {
      "text": "that we have coming down,\nright, is this red one, right?",
      "start": 1218.25,
      "duration": 3.342,
      "language": "en"
    },
    {
      "text": "This is the upstream gradient\nthat we have flowing down,",
      "start": 1221.592,
      "duration": 2.561,
      "language": "en"
    },
    {
      "text": "and then now we need to find\nthe local gradient, right,",
      "start": 1224.153,
      "duration": 2.785,
      "language": "en"
    },
    {
      "text": "and the local gradient of this node, this node is one over x, right,",
      "start": 1226.938,
      "duration": 3.242999999999938,
      "language": "en"
    },
    {
      "text": "so we have f of x equals\none over x here in red,",
      "start": 1230.181,
      "duration": 2.936,
      "language": "en"
    },
    {
      "text": "and the local gradient of this df over dx",
      "start": 1233.117,
      "duration": 2.754,
      "language": "en"
    },
    {
      "text": "is equal to negative one\nover x-squared, right?",
      "start": 1235.871,
      "duration": 4.064,
      "language": "en"
    },
    {
      "text": "So here we're going to take\nnegative one over x-squared,",
      "start": 1239.935,
      "duration": 3.765,
      "language": "en"
    },
    {
      "text": "and plug in the value\nof x that we had during",
      "start": 1243.7,
      "duration": 2.145,
      "language": "en"
    },
    {
      "text": "this forward pass, 1.37,\nand so our final gradient",
      "start": 1245.845,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "with respect to this variable is going to be negative one over\n1.37 squared times one",
      "start": 1251.325,
      "duration": 5.313000000000102,
      "language": "en"
    },
    {
      "text": "equals negative 0.53. So moving back to the next node,",
      "start": 1258.184,
      "duration": 8.585000000000036,
      "language": "en"
    },
    {
      "text": "we're going to go through the\nexact same process, right?",
      "start": 1266.769,
      "duration": 2.254,
      "language": "en"
    },
    {
      "text": "So here, the gradient\nflowing from upstream",
      "start": 1269.023,
      "duration": 3.845,
      "language": "en"
    },
    {
      "text": "is going to be negative 0.53, right,",
      "start": 1272.868,
      "duration": 3.139,
      "language": "en"
    },
    {
      "text": "and here the local gradient,\nthe node here is a plus one,",
      "start": 1276.007,
      "duration": 4.358,
      "language": "en"
    },
    {
      "text": "and so now looking at our\nreference of derivatives at",
      "start": 1280.365,
      "duration": 4.838,
      "language": "en"
    },
    {
      "text": "the bottom, we have that\nfor a constant plus x,",
      "start": 1285.203,
      "duration": 4.084,
      "language": "en"
    },
    {
      "text": "the local gradient is just one, right?",
      "start": 1289.287,
      "duration": 2.442,
      "language": "en"
    },
    {
      "text": "So what's the gradient with respect",
      "start": 1291.729,
      "duration": 2.48,
      "language": "en"
    },
    {
      "text": "to this variable using the chain rule?",
      "start": 1294.209,
      "duration": 3.167,
      "language": "en"
    },
    {
      "text": "So it's going to be the upstream gradient of negative 0.53 times\nour local gradient of one,",
      "start": 1302.883,
      "duration": 6.042000000000144,
      "language": "en"
    },
    {
      "text": "which is equal to negative 0.53.",
      "start": 1310.021,
      "duration": 2.667,
      "language": "en"
    },
    {
      "text": "So let's keep moving\nbackwards one more step.",
      "start": 1315.849,
      "duration": 3.755,
      "language": "en"
    },
    {
      "text": "So here we have the exponential, right?",
      "start": 1319.604,
      "duration": 2.494,
      "language": "en"
    },
    {
      "text": "So what's the upstream\ngradient coming down?",
      "start": 1322.098,
      "duration": 2.924,
      "language": "en"
    },
    {
      "text": "[student speaking away from microphone]",
      "start": 1325.022,
      "duration": 3.514,
      "language": "en"
    },
    {
      "text": "Right, so the upstream\ngradient is negative 0.53,",
      "start": 1328.536,
      "duration": 3.239,
      "language": "en"
    },
    {
      "text": "what's the local gradient here?",
      "start": 1331.775,
      "duration": 2.583,
      "language": "en"
    },
    {
      "text": "It's going to be the local\ngradient of e to the x, right?",
      "start": 1335.402,
      "duration": 2.6,
      "language": "en"
    },
    {
      "text": "This is an exponential\nnode, and so our chain rule",
      "start": 1338.002,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "is going to tell us that our gradient is going to be negative 0.53\ntimes e to the power of x,",
      "start": 1343.59,
      "duration": 6.132000000000062,
      "language": "en"
    },
    {
      "text": "which in this case is negative one, from our forward pass, and\nthis is going to give us",
      "start": 1350.869,
      "duration": 3.800999999999931,
      "language": "en"
    },
    {
      "text": "our final gradient of negative 0.2.",
      "start": 1354.67,
      "duration": 2.917,
      "language": "en"
    },
    {
      "text": "Okay, so now one more node here,",
      "start": 1360.215,
      "duration": 2.667,
      "language": "en"
    },
    {
      "text": "the next node is, that\nwe reach, is going to be",
      "start": 1364.234,
      "duration": 2.472,
      "language": "en"
    },
    {
      "text": "a multiplication with negative one, right?",
      "start": 1366.706,
      "duration": 2.206,
      "language": "en"
    },
    {
      "text": "So here, what's the upstream\ngradient coming down?",
      "start": 1368.912,
      "duration": 3.817,
      "language": "en"
    },
    {
      "text": "- [Student] Negative 0.2? - [Serena] Negative 0.2,\nright, and what's going to be",
      "start": 1372.729,
      "duration": 3.8359999999997854,
      "language": "en"
    },
    {
      "text": "the local gradient, can\nlook at the reference sheet.",
      "start": 1376.565,
      "duration": 4.945,
      "language": "en"
    },
    {
      "text": "It's going to be, what was it? I think I heard it.",
      "start": 1381.51,
      "duration": 2.3789999999999054,
      "language": "en"
    },
    {
      "text": "- [Student] That's minus one? - It's going to be minus\none, exactly, yeah,",
      "start": 1383.889,
      "duration": 5.79099999999994,
      "language": "en"
    },
    {
      "text": "because our local gradient\nsays it's going to be,",
      "start": 1389.68,
      "duration": 3.602,
      "language": "en"
    },
    {
      "text": "df over dx is a, right, and the value of a",
      "start": 1393.282,
      "duration": 2.694,
      "language": "en"
    },
    {
      "text": "that we scaled x by is negative one here.",
      "start": 1395.976,
      "duration": 3.244,
      "language": "en"
    },
    {
      "text": "So we have here that the gradient",
      "start": 1399.22,
      "duration": 2.586,
      "language": "en"
    },
    {
      "text": "is negative one times negative 0.2,",
      "start": 1401.806,
      "duration": 2.769,
      "language": "en"
    },
    {
      "text": "and so our gradient is 0.2.",
      "start": 1404.575,
      "duration": 2.25,
      "language": "en"
    },
    {
      "text": "Okay, so now we've\nreached an addition node,",
      "start": 1409.169,
      "duration": 3.756,
      "language": "en"
    },
    {
      "text": "and so in this case we\nhave these two branches",
      "start": 1412.925,
      "duration": 2.891,
      "language": "en"
    },
    {
      "text": "both connected to it, right? So what's the upstream gradient here?",
      "start": 1415.816,
      "duration": 3.47199999999998,
      "language": "en"
    },
    {
      "text": "It's going to be 0.2, right,\njust as everything else,",
      "start": 1419.288,
      "duration": 3.998,
      "language": "en"
    },
    {
      "text": "and here now the gradient with respect",
      "start": 1423.286,
      "duration": 2.9,
      "language": "en"
    },
    {
      "text": "to each of these branches,\nit's an addition, right,",
      "start": 1426.186,
      "duration": 3.936,
      "language": "en"
    },
    {
      "text": "and we saw from before\nin our simple example",
      "start": 1430.122,
      "duration": 2.464,
      "language": "en"
    },
    {
      "text": "that when we have an addition node, the gradient with respect\nto each of the inputs",
      "start": 1432.586,
      "duration": 3.6400000000001,
      "language": "en"
    },
    {
      "text": "to the addition is just\ngoing to be one, right?",
      "start": 1436.226,
      "duration": 3.04,
      "language": "en"
    },
    {
      "text": "So here, our local gradient\nfor looking at our top stream",
      "start": 1439.266,
      "duration": 4.395,
      "language": "en"
    },
    {
      "text": "is going to be one times\nthe upstream gradient",
      "start": 1443.661,
      "duration": 2.745,
      "language": "en"
    },
    {
      "text": "of 0.2, which is going to give\na total gradient of 0.2, right?",
      "start": 1446.406,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "And then we, for our bottom branch we'd do",
      "start": 1452.016,
      "duration": 2.203,
      "language": "en"
    },
    {
      "text": "the same thing, right, our\nupstream gradient is 0.2,",
      "start": 1454.219,
      "duration": 4.181,
      "language": "en"
    },
    {
      "text": "our local gradient is one again, and the total gradient is 0.2.",
      "start": 1458.4,
      "duration": 4.876999999999953,
      "language": "en"
    },
    {
      "text": "So is everything clear about this?",
      "start": 1463.277,
      "duration": 2.833,
      "language": "en"
    },
    {
      "text": "Okay. So we have a few more\ngradients to fill out,",
      "start": 1467.581,
      "duration": 5.176999999999907,
      "language": "en"
    },
    {
      "text": "so moving back now we've\nreached w-zero and x-zero,",
      "start": 1472.758,
      "duration": 4.89,
      "language": "en"
    },
    {
      "text": "and so here we have a\nmultiplication node, right,",
      "start": 1477.648,
      "duration": 3.438,
      "language": "en"
    },
    {
      "text": "so we saw the multiplication\nnode from before,",
      "start": 1481.086,
      "duration": 3.083,
      "language": "en"
    },
    {
      "text": "it just, the gradient with respect to one of the inputs just is\nthe value of the other input.",
      "start": 1484.169,
      "duration": 5.336999999999989,
      "language": "en"
    },
    {
      "text": "And so in this case, what's the gradient with respect to w-zero?",
      "start": 1489.506,
      "duration": 3.58199999999988,
      "language": "en"
    },
    {
      "text": "- [Student] Minus 0.2. - Minus, I'm hearing minus 0.2, exactly.",
      "start": 1496.927,
      "duration": 5.672000000000253,
      "language": "en"
    },
    {
      "text": "Yeah, so with respect to w-zero, we have our upstream gradient, 0.2, right,",
      "start": 1502.599,
      "duration": 5.267000000000053,
      "language": "en"
    },
    {
      "text": "times our, this is the bottom one, times our value of x,\nwhich is negative one,",
      "start": 1509.747,
      "duration": 4.033999999999878,
      "language": "en"
    },
    {
      "text": "we get negative 0.2 and\nwe can do the same thing",
      "start": 1513.781,
      "duration": 2.691,
      "language": "en"
    },
    {
      "text": "for our gradient with respect to x-zero.",
      "start": 1516.472,
      "duration": 2.328,
      "language": "en"
    },
    {
      "text": "It's going to be 0.2\ntimes the value of w-zero",
      "start": 1518.8,
      "duration": 3.208,
      "language": "en"
    },
    {
      "text": "which is two, and we get 0.4.",
      "start": 1522.008,
      "duration": 2.417,
      "language": "en"
    },
    {
      "text": "Okay, so here we've filled\nout most of these gradients,",
      "start": 1526.525,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "and so there was the question earlier",
      "start": 1532.2,
      "duration": 3.605,
      "language": "en"
    },
    {
      "text": "about why this is simpler\nthan just computing,",
      "start": 1535.805,
      "duration": 4.323,
      "language": "en"
    },
    {
      "text": "deriving the analytic gradient,\nthe expression with respect",
      "start": 1540.128,
      "duration": 2.943,
      "language": "en"
    },
    {
      "text": "to any of these variables, right? And so you can see here,\nall we ever dealt with",
      "start": 1543.071,
      "duration": 4.095000000000027,
      "language": "en"
    },
    {
      "text": "was expressions for local gradients",
      "start": 1547.166,
      "duration": 2.998,
      "language": "en"
    },
    {
      "text": "that we had to write out, so\nonce we had these expressions",
      "start": 1550.164,
      "duration": 2.001,
      "language": "en"
    },
    {
      "text": "for local gradients, all we did was plug in the values for\neach of these that we have,",
      "start": 1552.165,
      "duration": 4.685000000000173,
      "language": "en"
    },
    {
      "text": "and use the chain rule to\nnumerically multiply this all",
      "start": 1556.85,
      "duration": 2.806,
      "language": "en"
    },
    {
      "text": "the way backwards and get the gradients with respect to all of the variables.",
      "start": 1559.656,
      "duration": 4.95900000000006,
      "language": "en"
    },
    {
      "text": "And so, you know, we can also fill out",
      "start": 1568.779,
      "duration": 2.865,
      "language": "en"
    },
    {
      "text": "the gradients with respect\nto w-one and x-one here",
      "start": 1571.644,
      "duration": 2.846,
      "language": "en"
    },
    {
      "text": "in exactly the same way, and so one thing",
      "start": 1574.49,
      "duration": 2.045,
      "language": "en"
    },
    {
      "text": "that I want to note is that\nright when we're creating",
      "start": 1576.535,
      "duration": 3.228,
      "language": "en"
    },
    {
      "text": "these computational graphs, we can define",
      "start": 1579.763,
      "duration": 2.233,
      "language": "en"
    },
    {
      "text": "the computational nodes at any\ngranularity that we want to.",
      "start": 1581.996,
      "duration": 3.602,
      "language": "en"
    },
    {
      "text": "So in this case, we broke it down into",
      "start": 1585.598,
      "duration": 2.298,
      "language": "en"
    },
    {
      "text": "the absolute simplest\nthat we could, right, we broke it down into\nadditions and multiplications,",
      "start": 1587.896,
      "duration": 5.4240000000002055,
      "language": "en"
    },
    {
      "text": "you know, it basically can't\nget any simpler than that,",
      "start": 1593.32,
      "duration": 3.02,
      "language": "en"
    },
    {
      "text": "but in practice, right,\nwe can group some of",
      "start": 1596.34,
      "duration": 2.543,
      "language": "en"
    },
    {
      "text": "these nodes together into\nmore complex nodes if we want.",
      "start": 1598.883,
      "duration": 3.823,
      "language": "en"
    },
    {
      "text": "As long as we're able to write down the local gradient for that node, right?",
      "start": 1602.706,
      "duration": 4.538999999999987,
      "language": "en"
    },
    {
      "text": "And so as an example, if we\nlook at a sigmoid function,",
      "start": 1607.245,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "so I've defined the sigmoid function in",
      "start": 1613.153,
      "duration": 2.538,
      "language": "en"
    },
    {
      "text": "the upper-right here, of a sigmoid of x",
      "start": 1615.691,
      "duration": 2.17,
      "language": "en"
    },
    {
      "text": "is equal to one over one\nplus e to the negative x,",
      "start": 1617.861,
      "duration": 2.993,
      "language": "en"
    },
    {
      "text": "and this is something that's\na really common function",
      "start": 1620.854,
      "duration": 2.55,
      "language": "en"
    },
    {
      "text": "that you'll see a lot in\nthe rest of this class,",
      "start": 1623.404,
      "duration": 2.592,
      "language": "en"
    },
    {
      "text": "and we can compute the gradient for this,",
      "start": 1625.996,
      "duration": 3.908,
      "language": "en"
    },
    {
      "text": "we can write it out, and if\nwe do actually go through",
      "start": 1629.904,
      "duration": 2.782,
      "language": "en"
    },
    {
      "text": "the math of doing this analytically,",
      "start": 1632.686,
      "duration": 3.741,
      "language": "en"
    },
    {
      "text": "we can get a nice expression at the end. So in this case it's equal\nto one minus sigma of x,",
      "start": 1636.427,
      "duration": 6.171000000000049,
      "language": "en"
    },
    {
      "text": "so the output of this function\ntimes sigma of x, right?",
      "start": 1642.598,
      "duration": 4.011,
      "language": "en"
    },
    {
      "text": "And so in cases where we\nhave something like this,",
      "start": 1646.609,
      "duration": 2.925,
      "language": "en"
    },
    {
      "text": "we could just take all the computations",
      "start": 1649.534,
      "duration": 3.676,
      "language": "en"
    },
    {
      "text": "that we had in our graph\nthat made up this sigmoid,",
      "start": 1653.21,
      "duration": 2.52,
      "language": "en"
    },
    {
      "text": "and we could just replace it with one big node that's a sigmoid, right,",
      "start": 1655.73,
      "duration": 4.106999999999971,
      "language": "en"
    },
    {
      "text": "because we do know the local\ngradient for this gate,",
      "start": 1659.837,
      "duration": 3.818,
      "language": "en"
    },
    {
      "text": "it's this expression, d of the\nsigmoid of x over dx, right?",
      "start": 1663.655,
      "duration": 4.909,
      "language": "en"
    },
    {
      "text": "So basically the important thing here is",
      "start": 1668.564,
      "duration": 2.696,
      "language": "en"
    },
    {
      "text": "that you can, group any nodes that you want",
      "start": 1671.26,
      "duration": 3.535,
      "language": "en"
    },
    {
      "text": "to make any sorts of a little\nbit more complex nodes,",
      "start": 1674.795,
      "duration": 3.996,
      "language": "en"
    },
    {
      "text": "as long as you can write down\nthe local gradient for this.",
      "start": 1678.791,
      "duration": 2.854,
      "language": "en"
    },
    {
      "text": "And so all this is is\nbasically a trade-off between,",
      "start": 1681.645,
      "duration": 2.623,
      "language": "en"
    },
    {
      "text": "you know, how much math\nthat you want to do",
      "start": 1684.268,
      "duration": 2.165,
      "language": "en"
    },
    {
      "text": "in order to get a more, kind\nof concise and simpler graph,",
      "start": 1686.433,
      "duration": 4.61,
      "language": "en"
    },
    {
      "text": "right, versus how simple you want",
      "start": 1691.043,
      "duration": 2.75,
      "language": "en"
    },
    {
      "text": "each of your gradients to be, right? And then you can write out as complex of",
      "start": 1694.913,
      "duration": 4.376999999999953,
      "language": "en"
    },
    {
      "text": "a computational graph that you want.",
      "start": 1699.29,
      "duration": 2.326,
      "language": "en"
    },
    {
      "text": "Yeah, question? - [Student] This is a\nquestion on the graph itself,",
      "start": 1701.616,
      "duration": 3.6620000000000346,
      "language": "en"
    },
    {
      "text": "is there a reason that the\nfirst two multiplication nodes",
      "start": 1705.278,
      "duration": 2.871,
      "language": "en"
    },
    {
      "text": "and the weights are not connected\nto a single addition node?",
      "start": 1708.149,
      "duration": 3.906,
      "language": "en"
    },
    {
      "text": "- So they could also be connected into a single addition node,\nso the question was,",
      "start": 1712.055,
      "duration": 4.716000000000122,
      "language": "en"
    },
    {
      "text": "is there a reason why w-zero and x-zero",
      "start": 1716.771,
      "duration": 3.064,
      "language": "en"
    },
    {
      "text": "are not connected with w-two?",
      "start": 1719.835,
      "duration": 2.033,
      "language": "en"
    },
    {
      "text": "All of these additions\njust connected together,",
      "start": 1721.868,
      "duration": 2.299,
      "language": "en"
    },
    {
      "text": "and yeah, so the reason, the answer is",
      "start": 1724.167,
      "duration": 2.012,
      "language": "en"
    },
    {
      "text": "that you can do that if you want,",
      "start": 1726.179,
      "duration": 2.253,
      "language": "en"
    },
    {
      "text": "and in practice, maybe you\nwould actually want to do that",
      "start": 1728.432,
      "duration": 2.047,
      "language": "en"
    },
    {
      "text": "because this is still a\nvery simple node, right?",
      "start": 1730.479,
      "duration": 2.342,
      "language": "en"
    },
    {
      "text": "So in this case I just wrote\nthis out into as simple",
      "start": 1732.821,
      "duration": 3.837,
      "language": "en"
    },
    {
      "text": "as possible, where each node\nonly had up to two inputs,",
      "start": 1736.658,
      "duration": 4.756,
      "language": "en"
    },
    {
      "text": "but yeah, you could definitely do that.",
      "start": 1741.414,
      "duration": 3.085,
      "language": "en"
    },
    {
      "text": "Any other questions about this?",
      "start": 1744.499,
      "duration": 2.583,
      "language": "en"
    },
    {
      "text": "Okay, so the one thing that I really like",
      "start": 1748.682,
      "duration": 2.717,
      "language": "en"
    },
    {
      "text": "about thinking about this\nlike a computational graph",
      "start": 1751.399,
      "duration": 2.282,
      "language": "en"
    },
    {
      "text": "is that I feel very comforted, right, like anytime I have to take a gradient,",
      "start": 1753.681,
      "duration": 4.252999999999929,
      "language": "en"
    },
    {
      "text": "find gradients of something,\neven if the expression",
      "start": 1757.934,
      "duration": 2.319,
      "language": "en"
    },
    {
      "text": "that I want to compute\ngradients of is really hairy,",
      "start": 1760.253,
      "duration": 2.707,
      "language": "en"
    },
    {
      "text": "and really scary, you know,\nwhether it's something like this sigmoid or something worse,",
      "start": 1762.96,
      "duration": 3.979000000000042,
      "language": "en"
    },
    {
      "text": "I know that, you know, I could\nderive this if I want to,",
      "start": 1766.939,
      "duration": 3.763,
      "language": "en"
    },
    {
      "text": "but really, if I just\nsit down and write it out",
      "start": 1770.702,
      "duration": 2.512,
      "language": "en"
    },
    {
      "text": "in terms of a computational graph, I can go as simple as I need to",
      "start": 1773.214,
      "duration": 3.81899999999996,
      "language": "en"
    },
    {
      "text": "to always be able to apply\nbackprop and the chain rule,",
      "start": 1777.033,
      "duration": 3.372,
      "language": "en"
    },
    {
      "text": "and be able to compute all\nthe gradients that I need.",
      "start": 1780.405,
      "duration": 3.653,
      "language": "en"
    },
    {
      "text": "And so this is something that\nyou guys should think about",
      "start": 1784.058,
      "duration": 2.565,
      "language": "en"
    },
    {
      "text": "when you're doing your homeworks,\nas basically, you know,",
      "start": 1786.623,
      "duration": 4.581,
      "language": "en"
    },
    {
      "text": "anytime you're having trouble\nfinding gradients of something",
      "start": 1791.204,
      "duration": 2.234,
      "language": "en"
    },
    {
      "text": "just think about it as\na computational graph,",
      "start": 1793.438,
      "duration": 2.036,
      "language": "en"
    },
    {
      "text": "break it down into all of these parts, and then use the chain rule.",
      "start": 1795.474,
      "duration": 4.144000000000233,
      "language": "en"
    },
    {
      "text": "Okay, and so, you know, so we talked about",
      "start": 1800.884,
      "duration": 3.156,
      "language": "en"
    },
    {
      "text": "how we could group these\nset of nodes together",
      "start": 1804.04,
      "duration": 2.707,
      "language": "en"
    },
    {
      "text": "into a sigmoid gate, and\njust to confirm, like,",
      "start": 1806.747,
      "duration": 3.811,
      "language": "en"
    },
    {
      "text": "that this is actually exactly equivalent, we can plug this in, right?",
      "start": 1810.558,
      "duration": 3.5399999999999636,
      "language": "en"
    },
    {
      "text": "So we have that our input\nhere to the sigmoid gate",
      "start": 1814.098,
      "duration": 4.084,
      "language": "en"
    },
    {
      "text": "is going to be one, in\ngreen, and then we have",
      "start": 1818.182,
      "duration": 3.535,
      "language": "en"
    },
    {
      "text": "that the output is going\nto be here, 0.73, right,",
      "start": 1821.717,
      "duration": 4.083,
      "language": "en"
    },
    {
      "text": "and this'll work out if you plug it in to the sigmoid function.",
      "start": 1826.745,
      "duration": 3.1980000000000928,
      "language": "en"
    },
    {
      "text": "And so now if we want to\ndo, if we want to take",
      "start": 1829.943,
      "duration": 3.069,
      "language": "en"
    },
    {
      "text": "the gradient, and we want\nto treat this entire sigmoid",
      "start": 1833.012,
      "duration": 3.303,
      "language": "en"
    },
    {
      "text": "as one node, now what we should do",
      "start": 1836.315,
      "duration": 3.21,
      "language": "en"
    },
    {
      "text": "is we need to use this local gradient that we've derived up here, right?",
      "start": 1839.525,
      "duration": 3.3079999999999927,
      "language": "en"
    },
    {
      "text": "One minus sigmoid of x\ntimes the sigmoid of x.",
      "start": 1842.833,
      "duration": 3.129,
      "language": "en"
    },
    {
      "text": "So if we plug this in, and here we know",
      "start": 1845.962,
      "duration": 2.932,
      "language": "en"
    },
    {
      "text": "that the value of sigmoid of x was 0.73,",
      "start": 1848.894,
      "duration": 2.718,
      "language": "en"
    },
    {
      "text": "so if we plug this value\nin we'll see that this,",
      "start": 1851.612,
      "duration": 2.655,
      "language": "en"
    },
    {
      "text": "the value of this gradient\nis equal to 0.2, right,",
      "start": 1854.267,
      "duration": 3.276,
      "language": "en"
    },
    {
      "text": "and so the value of this\nlocal gradient is 0.2,",
      "start": 1857.543,
      "duration": 2.663,
      "language": "en"
    },
    {
      "text": "we multiply it by the x\nupstream gradient which is one,",
      "start": 1860.206,
      "duration": 3.26,
      "language": "en"
    },
    {
      "text": "and we're going to get\nout exactly the same value",
      "start": 1863.466,
      "duration": 3.578,
      "language": "en"
    },
    {
      "text": "of the gradient with respect\nto before the sigmoid gate,",
      "start": 1867.044,
      "duration": 2.223,
      "language": "en"
    },
    {
      "text": "as if we broke it down into all\nof the smaller computations.",
      "start": 1869.267,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "Okay, and so as we're looking\nat what's happening, right,",
      "start": 1877.191,
      "duration": 2.134,
      "language": "en"
    },
    {
      "text": "as we're taking these\ngradients going backwards",
      "start": 1879.325,
      "duration": 4.389,
      "language": "en"
    },
    {
      "text": "through our computational graph, there's some patterns that you'll notice",
      "start": 1883.714,
      "duration": 5.076000000000022,
      "language": "en"
    },
    {
      "text": "where there's some\nintuitive interpretation",
      "start": 1888.79,
      "duration": 2.438,
      "language": "en"
    },
    {
      "text": "that we can give these, right? So we saw that the add gate is\na gradient distributor right,",
      "start": 1891.228,
      "duration": 5.985999999999876,
      "language": "en"
    },
    {
      "text": "when we passed through\nthis addition gate here,",
      "start": 1897.214,
      "duration": 3.915,
      "language": "en"
    },
    {
      "text": "which had two branches coming out of it,",
      "start": 1901.129,
      "duration": 2.801,
      "language": "en"
    },
    {
      "text": "it took the gradient,\nthe upstream gradient",
      "start": 1903.93,
      "duration": 2.173,
      "language": "en"
    },
    {
      "text": "and it just distributed it,\npassed the exact same thing",
      "start": 1906.103,
      "duration": 2.483,
      "language": "en"
    },
    {
      "text": "to both of the branches\nthat were connected.",
      "start": 1908.586,
      "duration": 3.361,
      "language": "en"
    },
    {
      "text": "So here's a couple more\nthat we can think about.",
      "start": 1911.947,
      "duration": 3.209,
      "language": "en"
    },
    {
      "text": "So what's a max gate look like?",
      "start": 1915.156,
      "duration": 2.583,
      "language": "en"
    },
    {
      "text": "So we have a max gate\nhere at the bottom, right,",
      "start": 1919.214,
      "duration": 2.096,
      "language": "en"
    },
    {
      "text": "where the input's coming in are z and w,",
      "start": 1921.31,
      "duration": 3.515,
      "language": "en"
    },
    {
      "text": "z has a value of two, w has\na value of negative one,",
      "start": 1924.825,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "and then we took the max of\nthis, which is two, right,",
      "start": 1928.665,
      "duration": 2.779,
      "language": "en"
    },
    {
      "text": "and so we pass this\ndown into the remainder",
      "start": 1931.444,
      "duration": 2.807,
      "language": "en"
    },
    {
      "text": "of our computational graph.",
      "start": 1934.251,
      "duration": 2.626,
      "language": "en"
    },
    {
      "text": "So now if we're taking the\ngradients with respect to this,",
      "start": 1936.877,
      "duration": 3.191,
      "language": "en"
    },
    {
      "text": "the upstream gradient is, let's\nsay two coming back, right,",
      "start": 1940.068,
      "duration": 3.155,
      "language": "en"
    },
    {
      "text": "and what does this local\ngradient look like?",
      "start": 1943.223,
      "duration": 3.667,
      "language": "en"
    },
    {
      "text": "So anyone, yes? - [Student] It'll be zero for\none, and one for the other?",
      "start": 1950.057,
      "duration": 4.953999999999951,
      "language": "en"
    },
    {
      "text": "- Right. [student speaking away from microphone]",
      "start": 1955.011,
      "duration": 4.4240000000002055,
      "language": "en"
    },
    {
      "text": "Exactly, so the answer that was given",
      "start": 1959.435,
      "duration": 3.172,
      "language": "en"
    },
    {
      "text": "is that z will have a gradient of two,",
      "start": 1962.607,
      "duration": 3.266,
      "language": "en"
    },
    {
      "text": "w will have a value, a gradient of zero,",
      "start": 1965.873,
      "duration": 3.2,
      "language": "en"
    },
    {
      "text": "and so one of these is going to get the full value of the\ngradient just passed back,",
      "start": 1969.073,
      "duration": 4.038999999999987,
      "language": "en"
    },
    {
      "text": "and routed to that variable,\nand then the other one",
      "start": 1973.112,
      "duration": 4.028,
      "language": "en"
    },
    {
      "text": "will have a gradient of zero, and so,",
      "start": 1977.14,
      "duration": 3.083,
      "language": "en"
    },
    {
      "text": "so we can think of this as kind\nof a gradient router, right,",
      "start": 1981.328,
      "duration": 2.144,
      "language": "en"
    },
    {
      "text": "so, whereas the addition node passed back",
      "start": 1983.472,
      "duration": 2.477,
      "language": "en"
    },
    {
      "text": "the same gradient to\nboth branches coming in,",
      "start": 1985.949,
      "duration": 2.625,
      "language": "en"
    },
    {
      "text": "the max gate will just take the gradient and route it to one of the branches,",
      "start": 1988.574,
      "duration": 4.05600000000004,
      "language": "en"
    },
    {
      "text": "and this makes sense because\nif we look at our forward pass,",
      "start": 1992.63,
      "duration": 2.714,
      "language": "en"
    },
    {
      "text": "what's happening is that only the value that was the maximum got passed down to",
      "start": 1995.344,
      "duration": 4.048999999999978,
      "language": "en"
    },
    {
      "text": "the rest of the\ncomputational graph, right?",
      "start": 1999.393,
      "duration": 3.201,
      "language": "en"
    },
    {
      "text": "So it's the only value\nthat actually affected",
      "start": 2002.594,
      "duration": 2.428,
      "language": "en"
    },
    {
      "text": "our function computation at\nthe end, and so it makes sense",
      "start": 2005.022,
      "duration": 2.625,
      "language": "en"
    },
    {
      "text": "that when we're passing\nour gradients back, we just want to adjust what, you know,",
      "start": 2007.647,
      "duration": 4.962999999999965,
      "language": "en"
    },
    {
      "text": "flow it through that\nbranch of the computation.",
      "start": 2013.544,
      "duration": 4.726,
      "language": "en"
    },
    {
      "text": "Okay, and so another one,\nwhat's a multiplication gate,",
      "start": 2018.27,
      "duration": 2.435,
      "language": "en"
    },
    {
      "text": "which we saw earlier, is there\nany interpretation of this?",
      "start": 2020.705,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "[student speaking away from microphone]",
      "start": 2026.028,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "Okay, so the answer that was given",
      "start": 2032.976,
      "duration": 2.469,
      "language": "en"
    },
    {
      "text": "is that the local\ngradient is basically just the value of the other variable.",
      "start": 2035.445,
      "duration": 3.9629999999999654,
      "language": "en"
    },
    {
      "text": "Yeah, so that's exactly right. So we can think of this as\na gradient switcher, right?",
      "start": 2039.408,
      "duration": 5.304000000000087,
      "language": "en"
    },
    {
      "text": "A switcher, and I guess\na scaler, where we take the upstream gradient and we scale it by",
      "start": 2044.712,
      "duration": 4.0900000000001455,
      "language": "en"
    },
    {
      "text": "the value of the other branch.",
      "start": 2048.802,
      "duration": 2.5,
      "language": "en"
    },
    {
      "text": "Okay, and so one other thing to note is that when we have a place where one node",
      "start": 2055.719,
      "duration": 4.486000000000331,
      "language": "en"
    },
    {
      "text": "is connected to multiple nodes,",
      "start": 2060.205,
      "duration": 2.387,
      "language": "en"
    },
    {
      "text": "the gradients add up at this node, right?",
      "start": 2062.592,
      "duration": 3.595,
      "language": "en"
    },
    {
      "text": "So at these branches, using\nthe multivariate chain rule,",
      "start": 2066.187,
      "duration": 3.617,
      "language": "en"
    },
    {
      "text": "we're just going to take the value of the upstream gradient coming\nback from each of these nodes,",
      "start": 2069.804,
      "duration": 5.4699999999998,
      "language": "en"
    },
    {
      "text": "and we'll add these\ntogether to get the total",
      "start": 2075.274,
      "duration": 2.091,
      "language": "en"
    },
    {
      "text": "upstream gradient that's\nflowing back into this node,",
      "start": 2077.365,
      "duration": 3.135,
      "language": "en"
    },
    {
      "text": "and you can see this from\nthe multivariate chain rule",
      "start": 2080.5,
      "duration": 2.506,
      "language": "en"
    },
    {
      "text": "and also thinking about this,\nyou can think about this",
      "start": 2083.006,
      "duration": 4.041,
      "language": "en"
    },
    {
      "text": "that if you're going to\nchange this node a little bit,",
      "start": 2087.047,
      "duration": 2.8,
      "language": "en"
    },
    {
      "text": "it's going to affect both\nof these connected nodes",
      "start": 2089.847,
      "duration": 2.934,
      "language": "en"
    },
    {
      "text": "in the forward pass,\nright, when you're making",
      "start": 2092.781,
      "duration": 2.168,
      "language": "en"
    },
    {
      "text": "your forward pass through the graph.",
      "start": 2094.949,
      "duration": 2.314,
      "language": "en"
    },
    {
      "text": "And so then when you're\ndoing backprop, right,",
      "start": 2097.263,
      "duration": 2.215,
      "language": "en"
    },
    {
      "text": "then now the, both of\nthese gradients coming back",
      "start": 2099.478,
      "duration": 4.325,
      "language": "en"
    },
    {
      "text": "are going to affect this node, right,",
      "start": 2103.803,
      "duration": 2.212,
      "language": "en"
    },
    {
      "text": "and so that's how we're\ngoing to sum these up to be the total upstream gradient\nflowing back into this node.",
      "start": 2106.015,
      "duration": 6.710000000000036,
      "language": "en"
    },
    {
      "text": "Okay, so any questions about backprop,",
      "start": 2112.725,
      "duration": 3.167,
      "language": "en"
    },
    {
      "text": "going through these forward\nand backward passes?",
      "start": 2118.439,
      "duration": 3.894,
      "language": "en"
    },
    {
      "text": "- [Student] So we haven't did anything to actually update the weights.",
      "start": 2122.333,
      "duration": 3.157000000000153,
      "language": "en"
    },
    {
      "text": "[speaking away from microphone]",
      "start": 2125.49,
      "duration": 3.582,
      "language": "en"
    },
    {
      "text": "- Right, so the question is,\nwe haven't done anything yet",
      "start": 2129.072,
      "duration": 2.346,
      "language": "en"
    },
    {
      "text": "to update the values of these weights, we've only found the\ngradients with respect",
      "start": 2131.418,
      "duration": 4.576000000000022,
      "language": "en"
    },
    {
      "text": "to the variables, that's exactly right. So what we've talked about\nso far in this lecture is how",
      "start": 2135.994,
      "duration": 5.211999999999989,
      "language": "en"
    },
    {
      "text": "to compute gradients with\nrespect to any variables",
      "start": 2141.206,
      "duration": 3.864,
      "language": "en"
    },
    {
      "text": "in our function, right,\nand then once we have these",
      "start": 2145.07,
      "duration": 3.14,
      "language": "en"
    },
    {
      "text": "we can just apply everything we learned in",
      "start": 2148.21,
      "duration": 3.048,
      "language": "en"
    },
    {
      "text": "the optimization lecture,\nlast lecture, right?",
      "start": 2151.258,
      "duration": 3.161,
      "language": "en"
    },
    {
      "text": "So given the gradient,\nwe now take a step in",
      "start": 2154.419,
      "duration": 2.944,
      "language": "en"
    },
    {
      "text": "the direction of the gradient in order to update our weight,\nour parameters, right?",
      "start": 2157.363,
      "duration": 5.028000000000247,
      "language": "en"
    },
    {
      "text": "So you can just take this entire framework that we learned about last\nlecture for optimization,",
      "start": 2162.391,
      "duration": 4.657999999999902,
      "language": "en"
    },
    {
      "text": "and what we've done here is\njust learn how to compute",
      "start": 2167.049,
      "duration": 4.147,
      "language": "en"
    },
    {
      "text": "the gradients we need for\narbitrarily complex functions,",
      "start": 2171.196,
      "duration": 3.551,
      "language": "en"
    },
    {
      "text": "right, and so this is going\nto be useful when we talk",
      "start": 2174.747,
      "duration": 2.173,
      "language": "en"
    },
    {
      "text": "about complex functions like\nneural networks later on.",
      "start": 2176.92,
      "duration": 3.119,
      "language": "en"
    },
    {
      "text": "Yeah? - [Student] Do you mind writing out the,",
      "start": 2180.039,
      "duration": 2.6279999999997017,
      "language": "en"
    },
    {
      "text": "all the variate, so you could help explain this slide a little better?",
      "start": 2182.667,
      "duration": 4.485000000000127,
      "language": "en"
    },
    {
      "text": "- Yeah, so I can write\nthis maybe on the board.",
      "start": 2187.152,
      "duration": 3.917,
      "language": "en"
    },
    {
      "text": "Right, so basically if we're\ngoing to have, let's see,",
      "start": 2192.851,
      "duration": 4.579,
      "language": "en"
    },
    {
      "text": "if we're going to have the gradient of f",
      "start": 2197.43,
      "duration": 2.114,
      "language": "en"
    },
    {
      "text": "with respect to some variable x, right,",
      "start": 2199.544,
      "duration": 2.36,
      "language": "en"
    },
    {
      "text": "and let's say it's\nconnected through variables,",
      "start": 2201.904,
      "duration": 3.917,
      "language": "en"
    },
    {
      "text": "let's see, i, we can basically...",
      "start": 2209.203,
      "duration": 2.75,
      "language": "en"
    },
    {
      "text": "Right, so this is basically saying that if x is connected to\nthese multiple elements, right,",
      "start": 2221.68,
      "duration": 5.756000000000313,
      "language": "en"
    },
    {
      "text": "which in this case, different q-is,",
      "start": 2227.436,
      "duration": 2.723,
      "language": "en"
    },
    {
      "text": "then the chain rule is taking all,",
      "start": 2230.159,
      "duration": 2.833,
      "language": "en"
    },
    {
      "text": "it's going to take the effect of each of",
      "start": 2234.228,
      "duration": 2.225,
      "language": "en"
    },
    {
      "text": "these intermediate variables, right,",
      "start": 2236.453,
      "duration": 2.041,
      "language": "en"
    },
    {
      "text": "on our final output f, and\nthen compound each one with",
      "start": 2238.494,
      "duration": 4.403,
      "language": "en"
    },
    {
      "text": "the local effect of our variable x on",
      "start": 2242.897,
      "duration": 3.55,
      "language": "en"
    },
    {
      "text": "that intermediate value, right?",
      "start": 2246.447,
      "duration": 2.68,
      "language": "en"
    },
    {
      "text": "So yeah, it's basically just\nsumming all these up together.",
      "start": 2249.127,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "Okay, so now that we've, you\nknow, done all these examples",
      "start": 2255.755,
      "duration": 3.77,
      "language": "en"
    },
    {
      "text": "in the scalar case, we're going to look at",
      "start": 2259.525,
      "duration": 2.678,
      "language": "en"
    },
    {
      "text": "what happens when we have vectors, right?",
      "start": 2262.203,
      "duration": 2.517,
      "language": "en"
    },
    {
      "text": "So now if our variables x, y and z,",
      "start": 2264.72,
      "duration": 2.334,
      "language": "en"
    },
    {
      "text": "instead of just being numbers,\nwe have vectors for these.",
      "start": 2267.054,
      "duration": 3.196,
      "language": "en"
    },
    {
      "text": "And so everything stays exactly\nthe same, the entire flow,",
      "start": 2270.25,
      "duration": 3.627,
      "language": "en"
    },
    {
      "text": "the only difference is\nthat now our gradients",
      "start": 2273.877,
      "duration": 2.359,
      "language": "en"
    },
    {
      "text": "are going to be Jacobian matrices, right,",
      "start": 2276.236,
      "duration": 3.192,
      "language": "en"
    },
    {
      "text": "so these are now going\nto be matrices containing",
      "start": 2279.428,
      "duration": 4.586,
      "language": "en"
    },
    {
      "text": "the derivative of each\nelement of, for example z",
      "start": 2284.014,
      "duration": 3.727,
      "language": "en"
    },
    {
      "text": "with respect to each element of x.",
      "start": 2287.741,
      "duration": 2.833,
      "language": "en"
    },
    {
      "text": "Okay, and so to, you\nknow, so give an example",
      "start": 2294.237,
      "duration": 4.118,
      "language": "en"
    },
    {
      "text": "of something where this is\nhappening, right, let's say",
      "start": 2298.355,
      "duration": 2.466,
      "language": "en"
    },
    {
      "text": "that we have our input is\ngoing to now be a vector,",
      "start": 2300.821,
      "duration": 3.228,
      "language": "en"
    },
    {
      "text": "so let's say we have a\n4096-dimensional input vector,",
      "start": 2304.049,
      "duration": 3.572,
      "language": "en"
    },
    {
      "text": "and this is kind of a common\nsize that you might see",
      "start": 2307.621,
      "duration": 2.176,
      "language": "en"
    },
    {
      "text": "in convolutional neural networks later on,",
      "start": 2309.797,
      "duration": 4.045,
      "language": "en"
    },
    {
      "text": "and our node is going to be an\nelement-wise maximum, right?",
      "start": 2313.842,
      "duration": 4.076,
      "language": "en"
    },
    {
      "text": "So we have f of x is equal to the maximum",
      "start": 2317.918,
      "duration": 3.21,
      "language": "en"
    },
    {
      "text": "of x compared with zero\nelement-wise, and then our output",
      "start": 2321.128,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "is going to be also a\n4096-dimensional vector.",
      "start": 2326.875,
      "duration": 3.833,
      "language": "en"
    },
    {
      "text": "Okay, so in this case, what's the size of our Jacobian matrix?",
      "start": 2333.625,
      "duration": 3.344000000000051,
      "language": "en"
    },
    {
      "text": "Remember I said earlier,\nthe Jacobian matrix",
      "start": 2336.969,
      "duration": 2.312,
      "language": "en"
    },
    {
      "text": "is going to be, like each row is,",
      "start": 2339.281,
      "duration": 2.622,
      "language": "en"
    },
    {
      "text": "it's going to be partial derivatives, a matrix of partial derivatives\nof each dimension of",
      "start": 2341.903,
      "duration": 4.326000000000477,
      "language": "en"
    },
    {
      "text": "the output with respect to\neach dimension of the input.",
      "start": 2346.229,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "Okay, so the answer I\nheard was 4,096 squared,",
      "start": 2351.705,
      "duration": 3.525,
      "language": "en"
    },
    {
      "text": "and that's, yeah, that's correct.",
      "start": 2355.23,
      "duration": 2.342,
      "language": "en"
    },
    {
      "text": "So this is pretty large,\nright, 4,096 by 4,096",
      "start": 2357.572,
      "duration": 3.833,
      "language": "en"
    },
    {
      "text": "and in practice this is\ngoing to be even larger",
      "start": 2362.261,
      "duration": 2.942,
      "language": "en"
    },
    {
      "text": "because we're going to\nwork with many batches",
      "start": 2365.203,
      "duration": 2.197,
      "language": "en"
    },
    {
      "text": "of, you know, of, for example, 100 inputs",
      "start": 2367.4,
      "duration": 3.292,
      "language": "en"
    },
    {
      "text": "at the same time, right,\nand we'll put all of these",
      "start": 2370.692,
      "duration": 2.495,
      "language": "en"
    },
    {
      "text": "through our node at the same\ntime to be more efficient,",
      "start": 2373.187,
      "duration": 3.018,
      "language": "en"
    },
    {
      "text": "and so this is going to scale this by 100,",
      "start": 2376.205,
      "duration": 2.534,
      "language": "en"
    },
    {
      "text": "and in practice our Jacobian's\nactually going to turn out to be something like\n409,000 by 409,000 right,",
      "start": 2378.739,
      "duration": 6.342999999999847,
      "language": "en"
    },
    {
      "text": "so this is really huge, and basically",
      "start": 2385.082,
      "duration": 3.417,
      "language": "en"
    },
    {
      "text": "completely impractical to work with.",
      "start": 2388.499,
      "duration": 2.251,
      "language": "en"
    },
    {
      "text": "So in practice though,\nwe don't actually need",
      "start": 2390.75,
      "duration": 2.883,
      "language": "en"
    },
    {
      "text": "to compute this huge\nJacobian most of the time,",
      "start": 2393.633,
      "duration": 3.874,
      "language": "en"
    },
    {
      "text": "and so why is that, like, what does this Jacobian matrix look like?",
      "start": 2397.507,
      "duration": 3.394999999999982,
      "language": "en"
    },
    {
      "text": "If we think about what's happening here,",
      "start": 2400.902,
      "duration": 3.607,
      "language": "en"
    },
    {
      "text": "where we're taking this\nelement-wise maximum,",
      "start": 2404.509,
      "duration": 2.268,
      "language": "en"
    },
    {
      "text": "and we think about what are each of the partial derivatives, right,",
      "start": 2406.777,
      "duration": 4.121999999999844,
      "language": "en"
    },
    {
      "text": "which dimension of the inputs affect",
      "start": 2410.899,
      "duration": 2.989,
      "language": "en"
    },
    {
      "text": "which dimensions of the output? What sort of structure can we\nsee in our Jacobian matrix?",
      "start": 2413.888,
      "duration": 5.798000000000229,
      "language": "en"
    },
    {
      "text": "[student speaking away from microphone] Okay, so I heard that it's\ndiagonal, right, exactly.",
      "start": 2419.686,
      "duration": 4.182999999999538,
      "language": "en"
    },
    {
      "text": "So because this is element-wise,\nright, each element of",
      "start": 2423.869,
      "duration": 3.834,
      "language": "en"
    },
    {
      "text": "the input, say the first\ndimension, only affects",
      "start": 2427.703,
      "duration": 3.406,
      "language": "en"
    },
    {
      "text": "that corresponding element\nin the output, right?",
      "start": 2431.109,
      "duration": 3.632,
      "language": "en"
    },
    {
      "text": "And so because of that\nour Jacobian matrix,",
      "start": 2434.741,
      "duration": 3.273,
      "language": "en"
    },
    {
      "text": "which is just going to\nbe a diagonal matrix.",
      "start": 2438.014,
      "duration": 3.553,
      "language": "en"
    },
    {
      "text": "And so in practice then,\nwe don't actually have",
      "start": 2441.567,
      "duration": 2.357,
      "language": "en"
    },
    {
      "text": "to write out and formulate\nthis entire Jacobian,",
      "start": 2443.924,
      "duration": 3.274,
      "language": "en"
    },
    {
      "text": "we can just know the effect\nof x on the output, right,",
      "start": 2447.198,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "and then we can just\nuse these values, right,",
      "start": 2454.986,
      "duration": 2.897,
      "language": "en"
    },
    {
      "text": "and fill it in as we're\ncomputing the gradient.",
      "start": 2457.883,
      "duration": 3.917,
      "language": "en"
    },
    {
      "text": "Okay, so now we're going to go through",
      "start": 2464.1,
      "duration": 2.616,
      "language": "en"
    },
    {
      "text": "a more concrete vectorized\nexample of a computational graph.",
      "start": 2466.716,
      "duration": 3.977,
      "language": "en"
    },
    {
      "text": "Right, so let's look at a case where we have the function f of x and W",
      "start": 2470.693,
      "duration": 4.371999999999844,
      "language": "en"
    },
    {
      "text": "is equal to, basically the\nL-two of W multiplied by x,",
      "start": 2475.065,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "and so in this case we're going to say x is n-dimensional and W is n by n.",
      "start": 2482.407,
      "duration": 4.355999999999767,
      "language": "en"
    },
    {
      "text": "Right, so again our first step, writing out the\ncomputational graph, right?",
      "start": 2489.076,
      "duration": 3.5590000000001965,
      "language": "en"
    },
    {
      "text": "We have W multiplied by\nx, and then followed by,",
      "start": 2492.635,
      "duration": 2.668,
      "language": "en"
    },
    {
      "text": "I'm just going to call this L-two.",
      "start": 2495.303,
      "duration": 2.583,
      "language": "en"
    },
    {
      "text": "And so now let's also fill\nout some values for this,",
      "start": 2499.52,
      "duration": 3.302,
      "language": "en"
    },
    {
      "text": "so we can see that, you\nknow, let's say have W be",
      "start": 2502.822,
      "duration": 2.56,
      "language": "en"
    },
    {
      "text": "this two by two matrix, and x",
      "start": 2505.382,
      "duration": 2.178,
      "language": "en"
    },
    {
      "text": "is going to be this\ntwo-dimensional vector, right?",
      "start": 2507.56,
      "duration": 3.072,
      "language": "en"
    },
    {
      "text": "And so we can say, label\nagain our intermediate nodes.",
      "start": 2510.632,
      "duration": 3.317,
      "language": "en"
    },
    {
      "text": "So our intermediate node\nafter the multiplication",
      "start": 2513.949,
      "duration": 2.704,
      "language": "en"
    },
    {
      "text": "it's going to be q, we\nhave q equals W times x,",
      "start": 2516.653,
      "duration": 3.577,
      "language": "en"
    },
    {
      "text": "which we can write out\nelement-wise this way,",
      "start": 2520.23,
      "duration": 2.436,
      "language": "en"
    },
    {
      "text": "where the first element is\njust W-one-one times x-one",
      "start": 2522.666,
      "duration": 2.966,
      "language": "en"
    },
    {
      "text": "plus W-one-two times x-two and so on,",
      "start": 2525.632,
      "duration": 3.272,
      "language": "en"
    },
    {
      "text": "and then we can now express\nf in relation to q, right?",
      "start": 2528.904,
      "duration": 3.707,
      "language": "en"
    },
    {
      "text": "So looking at the second\nnode we have f of q",
      "start": 2532.611,
      "duration": 2.897,
      "language": "en"
    },
    {
      "text": "is equal to the L-two norm of q,",
      "start": 2535.508,
      "duration": 2.667,
      "language": "en"
    },
    {
      "text": "which is equal to q-one\nsquared plus q-two squared.",
      "start": 2539.26,
      "duration": 4.958,
      "language": "en"
    },
    {
      "text": "Okay, so we filled this in, right, we get q and then we get our final output.",
      "start": 2544.218,
      "duration": 5.204999999999927,
      "language": "en"
    },
    {
      "text": "Okay, so now let's do\nbackprop through this, right?",
      "start": 2550.491,
      "duration": 2.727,
      "language": "en"
    },
    {
      "text": "So again, this is always the first step,",
      "start": 2553.218,
      "duration": 3.115,
      "language": "en"
    },
    {
      "text": "we have the gradient with respect\nto our output is just one.",
      "start": 2556.333,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "Okay, so now let's move back one node,",
      "start": 2564.084,
      "duration": 3.421,
      "language": "en"
    },
    {
      "text": "so now we want to find the\ngradient with respect to q,",
      "start": 2567.505,
      "duration": 3.397,
      "language": "en"
    },
    {
      "text": "right, our intermediate\nvariable before the L-two.",
      "start": 2570.902,
      "duration": 4.591,
      "language": "en"
    },
    {
      "text": "And so q is a two-dimensional vector,",
      "start": 2575.493,
      "duration": 3.226,
      "language": "en"
    },
    {
      "text": "and what we want to do is we want to find how each element of q\naffects our final value of f,",
      "start": 2578.719,
      "duration": 5.51299999999992,
      "language": "en"
    },
    {
      "text": "right, and so if we\nlook at this expression that we've written out\nfor f here at the bottom,",
      "start": 2587.918,
      "duration": 4.036999999999807,
      "language": "en"
    },
    {
      "text": "we can see that the gradient of f",
      "start": 2591.955,
      "duration": 2.75,
      "language": "en"
    },
    {
      "text": "with respect to a specific\nq-i, let's say q-one,",
      "start": 2595.644,
      "duration": 3.649,
      "language": "en"
    },
    {
      "text": "is just going to be two times q-i, right?",
      "start": 2599.293,
      "duration": 3.843,
      "language": "en"
    },
    {
      "text": "This is just taking this derivative here,",
      "start": 2603.136,
      "duration": 3.433,
      "language": "en"
    },
    {
      "text": "and so we have this expression for,",
      "start": 2606.569,
      "duration": 3.724,
      "language": "en"
    },
    {
      "text": "with respect to each element of q-i,",
      "start": 2610.293,
      "duration": 2.16,
      "language": "en"
    },
    {
      "text": "we could also, you know, write this out",
      "start": 2612.453,
      "duration": 2.153,
      "language": "en"
    },
    {
      "text": "in vector form if we want to,",
      "start": 2614.606,
      "duration": 2.338,
      "language": "en"
    },
    {
      "text": "it's just going to be two\ntimes our vector of q, right,",
      "start": 2616.944,
      "duration": 2.925,
      "language": "en"
    },
    {
      "text": "if we want to write\nthis out in vector form, and so what we get is\nthat our gradient is 0.44,",
      "start": 2619.869,
      "duration": 5.8289999999997235,
      "language": "en"
    },
    {
      "text": "and 0.52, this vector, right?",
      "start": 2625.698,
      "duration": 2.528,
      "language": "en"
    },
    {
      "text": "And so you can see that it just took q",
      "start": 2628.226,
      "duration": 2.43,
      "language": "en"
    },
    {
      "text": "and it scaled it by two, right? Each element is just multiplied by two.",
      "start": 2630.656,
      "duration": 4.536000000000058,
      "language": "en"
    },
    {
      "text": "So the gradient of a vector\nis always going to be",
      "start": 2635.192,
      "duration": 3.58,
      "language": "en"
    },
    {
      "text": "the same size as the original vector,",
      "start": 2638.772,
      "duration": 2.41,
      "language": "en"
    },
    {
      "text": "and each element of this\ngradient is going to,",
      "start": 2641.182,
      "duration": 3.833,
      "language": "en"
    },
    {
      "text": "it means how much of\nthis particular element",
      "start": 2646.53,
      "duration": 2.602,
      "language": "en"
    },
    {
      "text": "affects our final output of the function.",
      "start": 2649.132,
      "duration": 3.417,
      "language": "en"
    },
    {
      "text": "Okay, so now let's move\none step backwards, right,",
      "start": 2656.126,
      "duration": 2.794,
      "language": "en"
    },
    {
      "text": "what's the gradient with respect to W?",
      "start": 2658.92,
      "duration": 3.603,
      "language": "en"
    },
    {
      "text": "And so here again we want\nto use the same concept",
      "start": 2662.523,
      "duration": 3.116,
      "language": "en"
    },
    {
      "text": "of trying to apply the chain rule, right, so we want to compute our local gradient",
      "start": 2665.639,
      "duration": 3.4899999999997817,
      "language": "en"
    },
    {
      "text": "of q with respect to W, and so let's look",
      "start": 2669.129,
      "duration": 2.199,
      "language": "en"
    },
    {
      "text": "at this again element-wise,\nand if we do that,",
      "start": 2671.328,
      "duration": 3.355,
      "language": "en"
    },
    {
      "text": "let's see what's the\neffect of each q, right,",
      "start": 2674.683,
      "duration": 3.201,
      "language": "en"
    },
    {
      "text": "each element of q with\nrespect to each element of W,",
      "start": 2677.884,
      "duration": 3.752,
      "language": "en"
    },
    {
      "text": "and so this is going to be the Jacobian that we talked about earlier,\nand if we look at this",
      "start": 2681.636,
      "duration": 5.956000000000131,
      "language": "en"
    },
    {
      "text": "in this multiplication, q is equal to W times x, right,\nwhat's the derivative,",
      "start": 2687.592,
      "duration": 5.5,
      "language": "en"
    },
    {
      "text": "or the gradient of the first element of q,",
      "start": 2696.236,
      "duration": 2.961,
      "language": "en"
    },
    {
      "text": "so our first element up top,\nwith respect to W-one-one?",
      "start": 2699.197,
      "duration": 4.765,
      "language": "en"
    },
    {
      "text": "So q-one with respect to W-one-one?",
      "start": 2703.962,
      "duration": 3.508,
      "language": "en"
    },
    {
      "text": "What's that value? X-one, exactly.",
      "start": 2707.47,
      "duration": 3.381000000000313,
      "language": "en"
    },
    {
      "text": "Yeah, so we know that this is x-one,",
      "start": 2710.851,
      "duration": 3.217,
      "language": "en"
    },
    {
      "text": "and we can write this\nout more generally of",
      "start": 2714.068,
      "duration": 3.591,
      "language": "en"
    },
    {
      "text": "the gradient of q-k with respect\nto W-i,j is equal to X-j.",
      "start": 2717.659,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "And then now if we want\nto find the gradient with respect to, of f,\nwith respect to each W-i,j.",
      "start": 2724.313,
      "duration": 5.964999999999691,
      "language": "en"
    },
    {
      "text": "So looking at these derivatives now,",
      "start": 2731.523,
      "duration": 3.809,
      "language": "en"
    },
    {
      "text": "we can use this chain rule\nthat we talked earlier",
      "start": 2735.332,
      "duration": 3.007,
      "language": "en"
    },
    {
      "text": "where we basically compound df over dq-k",
      "start": 2738.339,
      "duration": 3.333,
      "language": "en"
    },
    {
      "text": "for each element of q with dq-k over W-i,j",
      "start": 2743.77,
      "duration": 3.5,
      "language": "en"
    },
    {
      "text": "for each element of W-i,j, right?",
      "start": 2748.934,
      "duration": 2.564,
      "language": "en"
    },
    {
      "text": "So we find the effect of each element of W",
      "start": 2751.498,
      "duration": 2.065,
      "language": "en"
    },
    {
      "text": "on each element of q, and\nsum this across all q.",
      "start": 2753.563,
      "duration": 4.086,
      "language": "en"
    },
    {
      "text": "And so if you write this\nout, this is going to give",
      "start": 2757.649,
      "duration": 2.004,
      "language": "en"
    },
    {
      "text": "this expression of two\ntimes q-i times x-j.",
      "start": 2759.653,
      "duration": 3.583,
      "language": "en"
    },
    {
      "text": "Okay, and so filling this out then we get",
      "start": 2765.898,
      "duration": 2.846,
      "language": "en"
    },
    {
      "text": "this gradient with respect to W,",
      "start": 2768.744,
      "duration": 2.82,
      "language": "en"
    },
    {
      "text": "and so again we can compute\nthis each element-wise,",
      "start": 2771.564,
      "duration": 2.706,
      "language": "en"
    },
    {
      "text": "or we can also look at this\nexpression that we've derived",
      "start": 2774.27,
      "duration": 3.09,
      "language": "en"
    },
    {
      "text": "and write it out in\nvectorized form, right?",
      "start": 2777.36,
      "duration": 3.583,
      "language": "en"
    },
    {
      "text": "So okay, and remember, the important thing",
      "start": 2782.028,
      "duration": 2.799,
      "language": "en"
    },
    {
      "text": "is always to check the gradient\nwith respect to a variable",
      "start": 2784.827,
      "duration": 3.657,
      "language": "en"
    },
    {
      "text": "should have the same shape as\nthe variable, and something,",
      "start": 2788.484,
      "duration": 2.653,
      "language": "en"
    },
    {
      "text": "so this is something\nreally useful in practice",
      "start": 2791.137,
      "duration": 2.528,
      "language": "en"
    },
    {
      "text": "to sanity check, right,\nlike once you've computed",
      "start": 2793.665,
      "duration": 2.443,
      "language": "en"
    },
    {
      "text": "what your gradient should\nbe, check that this is the same shape as your variable,",
      "start": 2796.108,
      "duration": 4.600999999999658,
      "language": "en"
    },
    {
      "text": "because again, the element,\neach element of your gradient",
      "start": 2802.71,
      "duration": 3.51,
      "language": "en"
    },
    {
      "text": "is quantifying how much that element",
      "start": 2806.22,
      "duration": 2.956,
      "language": "en"
    },
    {
      "text": "is contributing to your, is\naffecting your final output.",
      "start": 2809.176,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "Yeah? [student speaking away from microphone]",
      "start": 2814.177,
      "duration": 3.311999999999898,
      "language": "en"
    },
    {
      "text": "The both sides, oh the both sides one is an indicator function,\nso this is saying",
      "start": 2817.489,
      "duration": 3.938000000000102,
      "language": "en"
    },
    {
      "text": "that it's just one if k equals i.",
      "start": 2821.427,
      "duration": 2.75,
      "language": "en"
    },
    {
      "text": "Okay, so let's see, so we've done that,",
      "start": 2828.276,
      "duration": 3.295,
      "language": "en"
    },
    {
      "text": "and so now just see, one more example.",
      "start": 2831.571,
      "duration": 3.167,
      "language": "en"
    },
    {
      "text": "Now our last thing we need to find is the gradient with respect to q-I.",
      "start": 2836.647,
      "duration": 4.3840000000000146,
      "language": "en"
    },
    {
      "text": "So here if we compute the\npartial derivatives we can see",
      "start": 2841.031,
      "duration": 3.793,
      "language": "en"
    },
    {
      "text": "that dq-k over dx-i is\nequal to W-k,i, right,",
      "start": 2844.824,
      "duration": 3.75,
      "language": "en"
    },
    {
      "text": "using the same way as we did it for W,",
      "start": 2849.535,
      "duration": 3.167,
      "language": "en"
    },
    {
      "text": "and then again we can\njust use the chain rule",
      "start": 2854.833,
      "duration": 3.869,
      "language": "en"
    },
    {
      "text": "and get the total\nexpression for that, right?",
      "start": 2858.702,
      "duration": 4.425,
      "language": "en"
    },
    {
      "text": "And so this is going to be the gradient with respect to x, again,\nof the same shape as x,",
      "start": 2863.127,
      "duration": 5.222999999999956,
      "language": "en"
    },
    {
      "text": "and we can also write this out in vectorized form if we want.",
      "start": 2868.35,
      "duration": 3.6720000000000255,
      "language": "en"
    },
    {
      "text": "Okay, so any questions about this, yeah?",
      "start": 2873.529,
      "duration": 2.913,
      "language": "en"
    },
    {
      "text": "[student speaking away from microphone]",
      "start": 2876.442,
      "duration": 2.658,
      "language": "en"
    },
    {
      "text": "So we are computing the Jacobian,",
      "start": 2879.1,
      "duration": 2.75,
      "language": "en"
    },
    {
      "text": "so let me go back here, right,",
      "start": 2883.194,
      "duration": 2.5,
      "language": "en"
    },
    {
      "text": "so if we're doing, so right, so we have",
      "start": 2887.414,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "these partial derivatives of q-k",
      "start": 2890.774,
      "duration": 2.099,
      "language": "en"
    },
    {
      "text": "with respect to x-i, right, and these",
      "start": 2892.873,
      "duration": 3.566,
      "language": "en"
    },
    {
      "text": "are forming your, the entries\nof your Jacobian, right?",
      "start": 2896.439,
      "duration": 4.275,
      "language": "en"
    },
    {
      "text": "And so in practice what we're going to do",
      "start": 2900.714,
      "duration": 2.165,
      "language": "en"
    },
    {
      "text": "is we basically take that,\nand you're going to see it",
      "start": 2902.879,
      "duration": 3.427,
      "language": "en"
    },
    {
      "text": "up there in the chain rule,\nso the vectorized expression",
      "start": 2906.306,
      "duration": 2.916,
      "language": "en"
    },
    {
      "text": "of gradient with respect to x, right,",
      "start": 2909.222,
      "duration": 2.037,
      "language": "en"
    },
    {
      "text": "this is going to have the Jacobian here",
      "start": 2911.259,
      "duration": 2.034,
      "language": "en"
    },
    {
      "text": "which is this transposed value here,",
      "start": 2913.293,
      "duration": 3.039,
      "language": "en"
    },
    {
      "text": "so you can write it\nout in vectorized form.",
      "start": 2916.332,
      "duration": 2.594,
      "language": "en"
    },
    {
      "text": "[student speaking away from microphone]",
      "start": 2918.926,
      "duration": 4.563,
      "language": "en"
    },
    {
      "text": "So well, so in this case the matrix is going to be the same size as W right,",
      "start": 2923.489,
      "duration": 4.1469999999999345,
      "language": "en"
    },
    {
      "text": "so it's not actually a large\nmatrix in this case, right?",
      "start": 2927.636,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "Okay, so the way that we've\nbeen thinking about this",
      "start": 2935.878,
      "duration": 3.063,
      "language": "en"
    },
    {
      "text": "is like a really modularized\nimplementation, right,",
      "start": 2938.941,
      "duration": 2.823,
      "language": "en"
    },
    {
      "text": "where in our computational graph, right,",
      "start": 2941.764,
      "duration": 3.541,
      "language": "en"
    },
    {
      "text": "we look at each node\nlocally and we compute",
      "start": 2945.305,
      "duration": 2.373,
      "language": "en"
    },
    {
      "text": "the local gradients and chain them with upstream gradients coming down,",
      "start": 2947.678,
      "duration": 3.286999999999807,
      "language": "en"
    },
    {
      "text": "and so you can think of this as basically",
      "start": 2950.965,
      "duration": 2.216,
      "language": "en"
    },
    {
      "text": "a forward and a backwards API, right?",
      "start": 2953.181,
      "duration": 2.156,
      "language": "en"
    },
    {
      "text": "In the forward pass we\nimplement the, you know,",
      "start": 2955.337,
      "duration": 3.849,
      "language": "en"
    },
    {
      "text": "a function computing\nthe output of this node,",
      "start": 2959.186,
      "duration": 3.21,
      "language": "en"
    },
    {
      "text": "and then in the backwards\npass we compute the gradient.",
      "start": 2962.396,
      "duration": 2.615,
      "language": "en"
    },
    {
      "text": "And so when we actually\nimplement this in code,",
      "start": 2965.011,
      "duration": 2.284,
      "language": "en"
    },
    {
      "text": "we're going to do this\nin exactly the same way.",
      "start": 2967.295,
      "duration": 3.297,
      "language": "en"
    },
    {
      "text": "So we can basically think\nabout, for each gate, right,",
      "start": 2970.592,
      "duration": 4.273,
      "language": "en"
    },
    {
      "text": "if we implement a forward\nfunction and a backward function,",
      "start": 2974.865,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "where the backward function\nis computing the chain rule,",
      "start": 2978.464,
      "duration": 2.444,
      "language": "en"
    },
    {
      "text": "then if we have our entire\ngraph, we can just make",
      "start": 2980.908,
      "duration": 4.664,
      "language": "en"
    },
    {
      "text": "a forward pass through the\nentire graph by iterating through",
      "start": 2985.572,
      "duration": 3.261,
      "language": "en"
    },
    {
      "text": "all the nodes in the graph, all the gates.",
      "start": 2988.833,
      "duration": 2.226,
      "language": "en"
    },
    {
      "text": "Here I'm going to use\nthe word gate and node, kind of interchangeably,\nwe can iterate through",
      "start": 2991.059,
      "duration": 3.7089999999998327,
      "language": "en"
    },
    {
      "text": "all of these gates and just call forward",
      "start": 2994.768,
      "duration": 2.426,
      "language": "en"
    },
    {
      "text": "on each of the gates, right? And we just want to do this\nin topologically sorted order,",
      "start": 2997.194,
      "duration": 4.154999999999745,
      "language": "en"
    },
    {
      "text": "so we process all of\nthe inputs coming in to",
      "start": 3001.349,
      "duration": 2.008,
      "language": "en"
    },
    {
      "text": "a node before we process that node.",
      "start": 3003.357,
      "duration": 2.975,
      "language": "en"
    },
    {
      "text": "And then going backwards,\nwe're just going to then",
      "start": 3006.332,
      "duration": 2.244,
      "language": "en"
    },
    {
      "text": "go through all of the gates\nin this reverse sorted order,",
      "start": 3008.576,
      "duration": 2.989,
      "language": "en"
    },
    {
      "text": "and then call backwards\non each of these gates.",
      "start": 3011.565,
      "duration": 3.917,
      "language": "en"
    },
    {
      "text": "Okay, and so if we look at then",
      "start": 3016.564,
      "duration": 2.583,
      "language": "en"
    },
    {
      "text": "the implementation for\nour particular gates,",
      "start": 3020.225,
      "duration": 2.06,
      "language": "en"
    },
    {
      "text": "so for example, this MultiplyGate here,",
      "start": 3022.285,
      "duration": 2.311,
      "language": "en"
    },
    {
      "text": "we want to implement\nthe forward pass, right,",
      "start": 3024.596,
      "duration": 2.364,
      "language": "en"
    },
    {
      "text": "so it gets x and y as inputs,\nand returns the value of z,",
      "start": 3026.96,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "and then when we go backwards, right, we get as input dz, which\nis our upstream gradient,",
      "start": 3032.52,
      "duration": 4.695999999999913,
      "language": "en"
    },
    {
      "text": "and we want to output the gradients",
      "start": 3037.216,
      "duration": 2.951,
      "language": "en"
    },
    {
      "text": "on the input's x and\ny to pass down, right?",
      "start": 3040.167,
      "duration": 2.356,
      "language": "en"
    },
    {
      "text": "So we're going to output dx and dy,",
      "start": 3042.523,
      "duration": 2.667,
      "language": "en"
    },
    {
      "text": "and so in this case, in this example,",
      "start": 3046.426,
      "duration": 2.684,
      "language": "en"
    },
    {
      "text": "everything is back to\nthe scalar case here,",
      "start": 3049.11,
      "duration": 2.457,
      "language": "en"
    },
    {
      "text": "and so if we look at\nthis in the forward pass,",
      "start": 3051.567,
      "duration": 4.007,
      "language": "en"
    },
    {
      "text": "one thing that's important\nis that we need to, we should cache the values\nof the forward pass, right,",
      "start": 3055.574,
      "duration": 3.7530000000001564,
      "language": "en"
    },
    {
      "text": "because we end up using this in the backward pass a lot of the time.",
      "start": 3059.327,
      "duration": 3.8289999999997235,
      "language": "en"
    },
    {
      "text": "So here in the forward pass,\nwe want to cache the values",
      "start": 3063.156,
      "duration": 2.905,
      "language": "en"
    },
    {
      "text": "of x and y, right, and\nin the backward pass,",
      "start": 3066.061,
      "duration": 4.533,
      "language": "en"
    },
    {
      "text": "using the chain rule,\nwe're going to, remember,",
      "start": 3070.594,
      "duration": 2.336,
      "language": "en"
    },
    {
      "text": "take the value of the upstream gradient and scale it by the value\nof the other branch, right,",
      "start": 3072.93,
      "duration": 4.414999999999964,
      "language": "en"
    },
    {
      "text": "and so we'll keep, for\ndx we'll take our value",
      "start": 3077.345,
      "duration": 2.949,
      "language": "en"
    },
    {
      "text": "of self.y that we kept, and multiply it",
      "start": 3080.294,
      "duration": 2.666,
      "language": "en"
    },
    {
      "text": "by dz coming down, and same for dy.",
      "start": 3082.96,
      "duration": 2.917,
      "language": "en"
    },
    {
      "text": "Okay, so if you look at a lot\nof deep-learning frameworks",
      "start": 3088.799,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "and libraries you'll see\nthat they exactly follow",
      "start": 3092.879,
      "duration": 2.479,
      "language": "en"
    },
    {
      "text": "this kind of modularization, right?",
      "start": 3095.358,
      "duration": 2.515,
      "language": "en"
    },
    {
      "text": "So for example, Caffe is a\npopular deep learning framework,",
      "start": 3097.873,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "and you'll see, if you go look\nthrough the Caffe source code",
      "start": 3103.284,
      "duration": 2.763,
      "language": "en"
    },
    {
      "text": "you'll get to some\ndirectory that says layers,",
      "start": 3106.047,
      "duration": 2.304,
      "language": "en"
    },
    {
      "text": "and in layers, which are\nbasically computational nodes,",
      "start": 3108.351,
      "duration": 4.092,
      "language": "en"
    },
    {
      "text": "usually layers might be\nslightly more, you know, some of these more complex\ncomputational nodes",
      "start": 3112.443,
      "duration": 3.7739999999998872,
      "language": "en"
    },
    {
      "text": "like the sigmoid that\nwe talked about earlier,",
      "start": 3116.217,
      "duration": 2.454,
      "language": "en"
    },
    {
      "text": "you'll see, basically just a whole list",
      "start": 3118.671,
      "duration": 2.884,
      "language": "en"
    },
    {
      "text": "of all different kinds of\ncomputational nodes, right?",
      "start": 3121.555,
      "duration": 2.787,
      "language": "en"
    },
    {
      "text": "So you might have the sigmoid, and I know there might be here, there's\nlike a convolution is one,",
      "start": 3124.342,
      "duration": 4.862999999999829,
      "language": "en"
    },
    {
      "text": "there's an Argmax is another layer,",
      "start": 3129.205,
      "duration": 2.72,
      "language": "en"
    },
    {
      "text": "you'll have all of these\nlayers and if you dig in",
      "start": 3131.925,
      "duration": 2.13,
      "language": "en"
    },
    {
      "text": "to each of them, they're\njust exactly implementing",
      "start": 3134.055,
      "duration": 2.285,
      "language": "en"
    },
    {
      "text": "a forward pass and a backward pass,",
      "start": 3136.34,
      "duration": 2.015,
      "language": "en"
    },
    {
      "text": "and then all of these are called",
      "start": 3138.355,
      "duration": 2.574,
      "language": "en"
    },
    {
      "text": "when we do forward and backward pass",
      "start": 3140.929,
      "duration": 2.143,
      "language": "en"
    },
    {
      "text": "through the entire network that we formed,",
      "start": 3143.072,
      "duration": 2.058,
      "language": "en"
    },
    {
      "text": "and so our network is just basically going",
      "start": 3145.13,
      "duration": 2.242,
      "language": "en"
    },
    {
      "text": "to be stacking up all of these,",
      "start": 3147.372,
      "duration": 3.021,
      "language": "en"
    },
    {
      "text": "the different layers that we\nchoose to use in the network.",
      "start": 3150.393,
      "duration": 4.074,
      "language": "en"
    },
    {
      "text": "So for example, if we\nlook at a specific one,",
      "start": 3154.467,
      "duration": 3.093,
      "language": "en"
    },
    {
      "text": "in this case a sigmoid layer, you'll see",
      "start": 3157.56,
      "duration": 3.053,
      "language": "en"
    },
    {
      "text": "that in the sigmoid layer, right, we've talked about the sigmoid function,",
      "start": 3160.613,
      "duration": 4.045000000000073,
      "language": "en"
    },
    {
      "text": "you'll see that there's a forward pass",
      "start": 3164.658,
      "duration": 2.482,
      "language": "en"
    },
    {
      "text": "which basically computes\nexactly the sigmoid expression,",
      "start": 3167.14,
      "duration": 4.303,
      "language": "en"
    },
    {
      "text": "and then a backward pass, right, where it is taking as input\nsomething, basically a top_diff,",
      "start": 3171.443,
      "duration": 5.807999999999538,
      "language": "en"
    },
    {
      "text": "which is our upstream\ngradient in this case,",
      "start": 3180.031,
      "duration": 2.127,
      "language": "en"
    },
    {
      "text": "and multiplying it by a local\ngradient that we compute.",
      "start": 3182.158,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "So in assignment one you'll get practice",
      "start": 3188.267,
      "duration": 2.272,
      "language": "en"
    },
    {
      "text": "with this kind of, this\ncomputational graph way of thinking",
      "start": 3190.539,
      "duration": 4.738,
      "language": "en"
    },
    {
      "text": "where, you know, you're\ngoing to be writing your SVM and Softmax classes,",
      "start": 3195.277,
      "duration": 4.001999999999953,
      "language": "en"
    },
    {
      "text": "and taking the gradients of these. And so again, remember always\nyou want to first step,",
      "start": 3199.279,
      "duration": 4.726000000000113,
      "language": "en"
    },
    {
      "text": "represent it as a\ncomputational graph, right?",
      "start": 3204.005,
      "duration": 3.855,
      "language": "en"
    },
    {
      "text": "Figure out what are all the computations that you did leading up to the output,",
      "start": 3207.86,
      "duration": 3.7719999999999345,
      "language": "en"
    },
    {
      "text": "and then when you, when it's time to do your backward pass,\njust take the gradient",
      "start": 3211.632,
      "duration": 4.153999999999996,
      "language": "en"
    },
    {
      "text": "with respect to each of\nthese intermediate variables",
      "start": 3215.786,
      "duration": 2.644,
      "language": "en"
    },
    {
      "text": "that you've defined in\nyour computational graph,",
      "start": 3218.43,
      "duration": 3.832,
      "language": "en"
    },
    {
      "text": "and use the chain rule to\nlink them all together.",
      "start": 3222.262,
      "duration": 4.083,
      "language": "en"
    },
    {
      "text": "Okay, so summary of what\nwe've talked about so far.",
      "start": 3227.63,
      "duration": 3.096,
      "language": "en"
    },
    {
      "text": "When we get down to, you know,\nworking with neural networks,",
      "start": 3230.726,
      "duration": 2.311,
      "language": "en"
    },
    {
      "text": "these are going to be\nreally large and complex,",
      "start": 3233.037,
      "duration": 2.075,
      "language": "en"
    },
    {
      "text": "so it's going to be\nimpractical to write down the gradient formula by hand\nfor all your parameters.",
      "start": 3235.112,
      "duration": 6.062999999999647,
      "language": "en"
    },
    {
      "text": "So in order to get these gradients, right,",
      "start": 3241.175,
      "duration": 3.689,
      "language": "en"
    },
    {
      "text": "we talked about how, what we\nshould use is backpropagation,",
      "start": 3244.864,
      "duration": 4.002,
      "language": "en"
    },
    {
      "text": "right, and this is kind of\none of the core techniques",
      "start": 3248.866,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "of, you know, neural\nnetworks, is basically",
      "start": 3252.226,
      "duration": 3.583,
      "language": "en"
    },
    {
      "text": "using backpropagation to\nget your gradients, right?",
      "start": 3256.66,
      "duration": 2.668,
      "language": "en"
    },
    {
      "text": "And so this is a recursive application of the chain rule where we have\nthis computational graph,",
      "start": 3259.328,
      "duration": 4.614000000000033,
      "language": "en"
    },
    {
      "text": "and we start at the back and\nwe go backwards through it",
      "start": 3263.942,
      "duration": 2.356,
      "language": "en"
    },
    {
      "text": "to compute the gradients with respect to all of the intermediate variables,",
      "start": 3266.298,
      "duration": 3.686000000000149,
      "language": "en"
    },
    {
      "text": "which are your inputs, your parameters, and everything else in the middle.",
      "start": 3269.984,
      "duration": 5.385000000000218,
      "language": "en"
    },
    {
      "text": "And we've also talked about how really this implementation and\nthis graph structure,",
      "start": 3275.369,
      "duration": 5.2729999999996835,
      "language": "en"
    },
    {
      "text": "each of these nodes is\nreally, you can see this",
      "start": 3280.642,
      "duration": 2.104,
      "language": "en"
    },
    {
      "text": "as implementing a forward\nand backwards API, right?",
      "start": 3282.746,
      "duration": 2.925,
      "language": "en"
    },
    {
      "text": "And so in the forward\npass we want to compute",
      "start": 3285.671,
      "duration": 2.305,
      "language": "en"
    },
    {
      "text": "the results of the operation, and we want to save any intermediate values",
      "start": 3287.976,
      "duration": 3.9609999999997854,
      "language": "en"
    },
    {
      "text": "that we might want to use later\nin our gradient computation,",
      "start": 3291.937,
      "duration": 3.212,
      "language": "en"
    },
    {
      "text": "and then in the backwards\npass we apply this chain rule",
      "start": 3295.149,
      "duration": 3.527,
      "language": "en"
    },
    {
      "text": "and we take this upstream gradient, we chain it, multiply it\nwith our local gradient",
      "start": 3298.676,
      "duration": 4.424999999999727,
      "language": "en"
    },
    {
      "text": "to compute the gradient with respect to",
      "start": 3303.101,
      "duration": 2.104,
      "language": "en"
    },
    {
      "text": "the inputs of the node,\nand we pass this down",
      "start": 3305.205,
      "duration": 3.035,
      "language": "en"
    },
    {
      "text": "to the nodes that are connected next.",
      "start": 3308.24,
      "duration": 3.083,
      "language": "en"
    },
    {
      "text": "Okay, so now finally we're going",
      "start": 3312.906,
      "duration": 2.357,
      "language": "en"
    },
    {
      "text": "to talk about neural networks.",
      "start": 3315.263,
      "duration": 2.337,
      "language": "en"
    },
    {
      "text": "All right, so really, you\nknow, neural networks,",
      "start": 3317.6,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "people draw a lot of analogies\nbetween neural networks",
      "start": 3324.254,
      "duration": 2.175,
      "language": "en"
    },
    {
      "text": "and the brain, and different types of biological inspirations,\nand we'll get to that in",
      "start": 3326.429,
      "duration": 3.7959999999998217,
      "language": "en"
    },
    {
      "text": "a little bit, but first let's\ntalk about it, you know,",
      "start": 3330.225,
      "duration": 3.041,
      "language": "en"
    },
    {
      "text": "just looking at it as a function, as a class of functions\nwithout all of the brain stuff.",
      "start": 3333.266,
      "duration": 6.119000000000142,
      "language": "en"
    },
    {
      "text": "So, so far we've talked about, you know, we've worked a lot with this\nlinear score function, right?",
      "start": 3339.385,
      "duration": 4.362999999999829,
      "language": "en"
    },
    {
      "text": "f equals W times x, and\nso we've been using this",
      "start": 3343.748,
      "duration": 2.825,
      "language": "en"
    },
    {
      "text": "as a running example of a\nfunction that we want to optimize.",
      "start": 3346.573,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "So instead of using the\nsingle in your transformation,",
      "start": 3351.716,
      "duration": 3.479,
      "language": "en"
    },
    {
      "text": "if we want a neural network where we can just, as the simplest form,",
      "start": 3355.195,
      "duration": 3.87099999999964,
      "language": "en"
    },
    {
      "text": "just stack two of these together, right?",
      "start": 3359.066,
      "duration": 2.184,
      "language": "en"
    },
    {
      "text": "Just a linear transformation\non top of another one",
      "start": 3361.25,
      "duration": 3.38,
      "language": "en"
    },
    {
      "text": "in order to get a two-layer\nneural network, right?",
      "start": 3364.63,
      "duration": 3.492,
      "language": "en"
    },
    {
      "text": "And so what this looks like is\nfirst we have our, you know,",
      "start": 3368.122,
      "duration": 3.329,
      "language": "en"
    },
    {
      "text": "a matrix multiply of W-one with x,",
      "start": 3371.451,
      "duration": 2.833,
      "language": "en"
    },
    {
      "text": "and then we get this intermediate variable",
      "start": 3375.921,
      "duration": 3.421,
      "language": "en"
    },
    {
      "text": "and we have this non-linear\nfunction of a max of zero",
      "start": 3379.342,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "with W, max with this\noutput of this linear layer,",
      "start": 3384.761,
      "duration": 3.945,
      "language": "en"
    },
    {
      "text": "and it's really important to\nhave these non-linearities",
      "start": 3388.706,
      "duration": 3.387,
      "language": "en"
    },
    {
      "text": "in place, which we'll\ntalk about more later, because otherwise if you just\nstack linear layers on top",
      "start": 3392.093,
      "duration": 4.387000000000171,
      "language": "en"
    },
    {
      "text": "of each other, they're\njust going to collapse to, like a single linear function.",
      "start": 3396.48,
      "duration": 4.496000000000095,
      "language": "en"
    },
    {
      "text": "Okay, so we have our first linear layer",
      "start": 3400.976,
      "duration": 2.169,
      "language": "en"
    },
    {
      "text": "and then we have this\nnon-linearity, right,",
      "start": 3403.145,
      "duration": 2.701,
      "language": "en"
    },
    {
      "text": "and then on top of this we'll\nadd another linear layer.",
      "start": 3405.846,
      "duration": 3.501,
      "language": "en"
    },
    {
      "text": "And then from here, finally\nwe can get our score function,",
      "start": 3409.347,
      "duration": 2.963,
      "language": "en"
    },
    {
      "text": "our output vector of scores.",
      "start": 3412.31,
      "duration": 2.652,
      "language": "en"
    },
    {
      "text": "So basically, like, more broadly speaking,",
      "start": 3414.962,
      "duration": 2.6,
      "language": "en"
    },
    {
      "text": "neural networks are a class of functions",
      "start": 3417.562,
      "duration": 2.627,
      "language": "en"
    },
    {
      "text": "where we have simpler functions, right,",
      "start": 3420.189,
      "duration": 2.463,
      "language": "en"
    },
    {
      "text": "that are stacked on top of each other, and we stack them in a hierarchical way",
      "start": 3422.652,
      "duration": 3.6089999999999236,
      "language": "en"
    },
    {
      "text": "in order to make up a more\ncomplex non-linear function,",
      "start": 3426.261,
      "duration": 3.817,
      "language": "en"
    },
    {
      "text": "and so this is the idea of\nhaving, basically multiple stages",
      "start": 3430.078,
      "duration": 3.749,
      "language": "en"
    },
    {
      "text": "of hierarchical computation, right?",
      "start": 3433.827,
      "duration": 2.875,
      "language": "en"
    },
    {
      "text": "And so, you know, so this is kind of",
      "start": 3436.702,
      "duration": 2.753,
      "language": "en"
    },
    {
      "text": "the main way that we do this is by taking",
      "start": 3439.455,
      "duration": 3.134,
      "language": "en"
    },
    {
      "text": "something like this matrix\nmultiply, this linear layer,",
      "start": 3442.589,
      "duration": 3.631,
      "language": "en"
    },
    {
      "text": "and we just stack multiple\nof these on top of each other",
      "start": 3446.22,
      "duration": 3.984,
      "language": "en"
    },
    {
      "text": "with non-linear functions\nin-between, right?",
      "start": 3450.204,
      "duration": 3.667,
      "language": "en"
    },
    {
      "text": "And so one thing that this\ncan help solve is if we look,",
      "start": 3458.446,
      "duration": 2.739,
      "language": "en"
    },
    {
      "text": "if we remember back to\nthis linear score function that we were talking about, right,",
      "start": 3461.185,
      "duration": 3.9850000000001273,
      "language": "en"
    },
    {
      "text": "remember we discussed earlier how each row",
      "start": 3465.17,
      "duration": 4.22,
      "language": "en"
    },
    {
      "text": "of our weight matrix W was\nsomething like a template.",
      "start": 3469.39,
      "duration": 3.893,
      "language": "en"
    },
    {
      "text": "It was a template that sort\nof expressed, you know,",
      "start": 3473.283,
      "duration": 2.254,
      "language": "en"
    },
    {
      "text": "what we're looking for in the input",
      "start": 3475.537,
      "duration": 3.16,
      "language": "en"
    },
    {
      "text": "for a specific class, right,\nso for example, you know,",
      "start": 3478.697,
      "duration": 2.352,
      "language": "en"
    },
    {
      "text": "the car template looks something like this kind of fuzzy red car,\nand we were looking for this",
      "start": 3481.049,
      "duration": 5.060999999999694,
      "language": "en"
    },
    {
      "text": "in the input to compute the\nscore for the car class.",
      "start": 3486.11,
      "duration": 4.041,
      "language": "en"
    },
    {
      "text": "And we talked about one\nof the problems with this is that there's only one template, right?",
      "start": 3490.151,
      "duration": 3.4150000000004184,
      "language": "en"
    },
    {
      "text": "There's this red car, whereas in practice,",
      "start": 3493.566,
      "duration": 2.282,
      "language": "en"
    },
    {
      "text": "we actually have multiple modes, right? We might want, we're looking\nfor, you know, a red car,",
      "start": 3495.848,
      "duration": 3.688999999999851,
      "language": "en"
    },
    {
      "text": "there's also a yellow\ncar, like all of these are different kinds of cars, and so what",
      "start": 3499.537,
      "duration": 4.756000000000313,
      "language": "en"
    },
    {
      "text": "this kind of multiple\nlayer network lets you do",
      "start": 3504.293,
      "duration": 3.917,
      "language": "en"
    },
    {
      "text": "is now, you know, each of\nthis intermediate variable h,",
      "start": 3509.287,
      "duration": 4.379,
      "language": "en"
    },
    {
      "text": "right, W-one can still be\nthese kinds of templates,",
      "start": 3513.666,
      "duration": 3.119,
      "language": "en"
    },
    {
      "text": "but now you have all of these scores",
      "start": 3516.785,
      "duration": 2.158,
      "language": "en"
    },
    {
      "text": "for these templates in h,\nand we can have another layer",
      "start": 3518.943,
      "duration": 3.063,
      "language": "en"
    },
    {
      "text": "on top that's combining\nthese together, right?",
      "start": 3522.006,
      "duration": 3.497,
      "language": "en"
    },
    {
      "text": "So we can say that actually\nmy car class should be,",
      "start": 3525.503,
      "duration": 3.19,
      "language": "en"
    },
    {
      "text": "you know, connected to, we're looking for both red cars as well\nas yellow cars, right,",
      "start": 3528.693,
      "duration": 4.9090000000001055,
      "language": "en"
    },
    {
      "text": "because we have this matrix W-two",
      "start": 3533.602,
      "duration": 3.05,
      "language": "en"
    },
    {
      "text": "which is now a weighting\nof all of our vector in h.",
      "start": 3536.652,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "Okay, any questions about this?",
      "start": 3545.691,
      "duration": 2.787,
      "language": "en"
    },
    {
      "text": "Yeah? [student speaking away from microphone]",
      "start": 3548.478,
      "duration": 5.317000000000007,
      "language": "en"
    },
    {
      "text": "Yeah, so there's a lot of ways, so there's a lot of different\nnon-linear functions",
      "start": 3553.795,
      "duration": 3.393999999999778,
      "language": "en"
    },
    {
      "text": "that you can choose from,\nand we'll talk later on",
      "start": 3557.189,
      "duration": 3.632,
      "language": "en"
    },
    {
      "text": "in a later lecture about\nall the different kinds of non-linearities that\nyou might want to use.",
      "start": 3560.821,
      "duration": 5.058999999999742,
      "language": "en"
    },
    {
      "text": "- [Student] For the pictures in the slide,",
      "start": 3565.88,
      "duration": 2.719,
      "language": "en"
    },
    {
      "text": "so, on the bottom row you have images",
      "start": 3568.599,
      "duration": 2.812,
      "language": "en"
    },
    {
      "text": "of your vector W-one weight, and so maybe",
      "start": 3571.411,
      "duration": 3.417,
      "language": "en"
    },
    {
      "text": "you would have images\nof another vector W-two?",
      "start": 3578.703,
      "duration": 3.833,
      "language": "en"
    },
    {
      "text": "- So W-one, because it's\ndirectly connected to",
      "start": 3586.014,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "the input x, this is what's\nlike, really interpretable,",
      "start": 3589.534,
      "duration": 3.431,
      "language": "en"
    },
    {
      "text": "because you can formulate\nall of these templates.",
      "start": 3592.965,
      "duration": 3.944,
      "language": "en"
    },
    {
      "text": "W-two, so h is going to be a score",
      "start": 3596.909,
      "duration": 2.583,
      "language": "en"
    },
    {
      "text": "of how much of each template\nyou solve, for example,",
      "start": 3600.389,
      "duration": 3.181,
      "language": "en"
    },
    {
      "text": "all right, so it might be\nlike you have a, you know,",
      "start": 3603.57,
      "duration": 2.477,
      "language": "en"
    },
    {
      "text": "like a, I don't know, two for the red car,",
      "start": 3606.047,
      "duration": 3.593,
      "language": "en"
    },
    {
      "text": "and like, one for the yellow\ncar or something like that.",
      "start": 3609.64,
      "duration": 2.225,
      "language": "en"
    },
    {
      "text": "- [Student] Oh, okay, so\ninstead of W-one being just 10,",
      "start": 3611.865,
      "duration": 4.813,
      "language": "en"
    },
    {
      "text": "like, you would have a left-facing horse",
      "start": 3616.678,
      "duration": 2.649,
      "language": "en"
    },
    {
      "text": "and a right-facing horse,\nand they'd both be included--",
      "start": 3619.327,
      "duration": 2.48,
      "language": "en"
    },
    {
      "text": "- Exactly, so the question\nis basically whether",
      "start": 3621.807,
      "duration": 4.057,
      "language": "en"
    },
    {
      "text": "in W-one you could have\nboth left-facing horse and right-facing horse,\nright, and so yeah, exactly.",
      "start": 3625.864,
      "duration": 4.66800000000012,
      "language": "en"
    },
    {
      "text": "So now W-one can be many different\nkinds of templates right?",
      "start": 3630.532,
      "duration": 4.058,
      "language": "en"
    },
    {
      "text": "They're not, and then W-two,\nnow we can, like basically",
      "start": 3634.59,
      "duration": 3.915,
      "language": "en"
    },
    {
      "text": "it's a weighted sum of\nall of these templates.",
      "start": 3638.505,
      "duration": 2.799,
      "language": "en"
    },
    {
      "text": "So now it allows you to weight\ntogether multiple templates",
      "start": 3641.304,
      "duration": 2.157,
      "language": "en"
    },
    {
      "text": "in order to get the final\nscore for a particular class.",
      "start": 3643.461,
      "duration": 3.553,
      "language": "en"
    },
    {
      "text": "- [Student] So if you're\nprocessing an image then it's actually left-facing horse.",
      "start": 3647.014,
      "duration": 3.699000000000069,
      "language": "en"
    },
    {
      "text": "It'll get a really high score with",
      "start": 3650.713,
      "duration": 2.809,
      "language": "en"
    },
    {
      "text": "the left-facing horse template, and a lower score with the\nright-facing horse template,",
      "start": 3653.522,
      "duration": 4.664000000000215,
      "language": "en"
    },
    {
      "text": "and then this will take\nthe maximum of the two?",
      "start": 3658.186,
      "duration": 4.366,
      "language": "en"
    },
    {
      "text": "- Right, so okay, so the question is, if our image x is like a left-facing horse",
      "start": 3662.552,
      "duration": 6.1349999999997635,
      "language": "en"
    },
    {
      "text": "and in W-one we have a template of a left-facing horse and\na right-facing horse,",
      "start": 3668.687,
      "duration": 4.4529999999999745,
      "language": "en"
    },
    {
      "text": "then what's happening, right? So what happens is yeah,\nso in h you might have",
      "start": 3673.14,
      "duration": 4.25,
      "language": "en"
    },
    {
      "text": "a really high score for\nyour left-facing horse,",
      "start": 3677.39,
      "duration": 3.809,
      "language": "en"
    },
    {
      "text": "kind of a lower score for\nyour right-facing horse,",
      "start": 3681.199,
      "duration": 3.428,
      "language": "en"
    },
    {
      "text": "and W-two is, it's a weighted\nsum, so it's not a maximum.",
      "start": 3684.627,
      "duration": 3.349,
      "language": "en"
    },
    {
      "text": "It's a weighted sum of these templates,",
      "start": 3687.976,
      "duration": 2.773,
      "language": "en"
    },
    {
      "text": "but if you have either a really high score",
      "start": 3690.749,
      "duration": 2.466,
      "language": "en"
    },
    {
      "text": "for one of these templates,\nor let's say you have, kind of",
      "start": 3693.215,
      "duration": 2.382,
      "language": "en"
    },
    {
      "text": "a lower and medium score\nfor both of these templates,",
      "start": 3695.597,
      "duration": 2.816,
      "language": "en"
    },
    {
      "text": "all of these kinds of combinations are going to give high scores, right?",
      "start": 3698.413,
      "duration": 3.5579999999999927,
      "language": "en"
    },
    {
      "text": "And so in the end what you're going to get is something that generally scores high",
      "start": 3701.971,
      "duration": 3.181999999999789,
      "language": "en"
    },
    {
      "text": "when you have a horse of any kind.",
      "start": 3705.153,
      "duration": 2.833,
      "language": "en"
    },
    {
      "text": "So let's say you had a front-facing horse, you might have medium values for both",
      "start": 3709.195,
      "duration": 4.274999999999636,
      "language": "en"
    },
    {
      "text": "the left and the right templates.",
      "start": 3713.47,
      "duration": 3.431,
      "language": "en"
    },
    {
      "text": "Yeah, question? - [Student] So is W-two\ndoing the weighting,",
      "start": 3716.901,
      "duration": 3.093000000000302,
      "language": "en"
    },
    {
      "text": "or is h doing the weighting? - W-two is doing the\nweighting, so the question is,",
      "start": 3719.994,
      "duration": 3.5119999999997162,
      "language": "en"
    },
    {
      "text": "\"Is W-two doing the weighting\nor is h doing the weighting?\"",
      "start": 3723.506,
      "duration": 2.891,
      "language": "en"
    },
    {
      "text": "h is the value, like in this example,",
      "start": 3726.397,
      "duration": 3.313,
      "language": "en"
    },
    {
      "text": "h is the value of scores\nfor each of your templates",
      "start": 3729.71,
      "duration": 4.434,
      "language": "en"
    },
    {
      "text": "that you have in W-one, right?",
      "start": 3734.144,
      "duration": 3.133,
      "language": "en"
    },
    {
      "text": "So h is like the score function, right,",
      "start": 3737.277,
      "duration": 3.851,
      "language": "en"
    },
    {
      "text": "it's how much of each\ntemplate in W-one is present,",
      "start": 3741.128,
      "duration": 4.723,
      "language": "en"
    },
    {
      "text": "and then W-two is going\nto weight all of these,",
      "start": 3745.851,
      "duration": 3.776,
      "language": "en"
    },
    {
      "text": "weight all of these intermediate scores to get your final score for the class.",
      "start": 3749.627,
      "duration": 4.346999999999753,
      "language": "en"
    },
    {
      "text": "- [Student] And which\nis the non-linear thing?",
      "start": 3753.974,
      "duration": 2.22,
      "language": "en"
    },
    {
      "text": "- So the question is, \"which\nis the non-linear thing?\" So the non-linearity usually\nhappens right before h,",
      "start": 3756.194,
      "duration": 5.976999999999862,
      "language": "en"
    },
    {
      "text": "so h is the value right\nafter the non-linearity.",
      "start": 3763.643,
      "duration": 4.253,
      "language": "en"
    },
    {
      "text": "So we're talking about\nthis, like, you know, intuitively as this example of like,",
      "start": 3767.896,
      "duration": 3.669999999999618,
      "language": "en"
    },
    {
      "text": "W-one is looking for, you know,",
      "start": 3771.566,
      "duration": 2.362,
      "language": "en"
    },
    {
      "text": "has these same templates as before, and W-two is a weighting for these.",
      "start": 3773.928,
      "duration": 3.22400000000016,
      "language": "en"
    },
    {
      "text": "In practice it's not\nexactly like this, right,",
      "start": 3777.152,
      "duration": 2.162,
      "language": "en"
    },
    {
      "text": "because as you said, there's all these non-linearities thrown in and so on,",
      "start": 3779.314,
      "duration": 4.044000000000324,
      "language": "en"
    },
    {
      "text": "but it has this approximate\ntype of interpretation to it.",
      "start": 3783.358,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "- [Student] So h is just W-one-x then?",
      "start": 3788.435,
      "duration": 3.167,
      "language": "en"
    },
    {
      "text": "- Yeah, yeah, so the\nquestion is h just W-one-x?",
      "start": 3793.811,
      "duration": 2.904,
      "language": "en"
    },
    {
      "text": "So h is just W-one times x,\nwith the max function on top.",
      "start": 3796.715,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "Oh, let me just, okay so,",
      "start": 3807.004,
      "duration": 2.083,
      "language": "en"
    },
    {
      "text": "so we've talked about\nthis as an example of a two-layer neural network,\nand we can stack more layers",
      "start": 3810.259,
      "duration": 4.130999999999858,
      "language": "en"
    },
    {
      "text": "of these to get deeper networks\nof arbitrary depth, right?",
      "start": 3814.39,
      "duration": 2.831,
      "language": "en"
    },
    {
      "text": "So we can just do this one more time at another non-linearity and\nmatrix multiply now by W-three,",
      "start": 3817.221,
      "duration": 6.148000000000138,
      "language": "en"
    },
    {
      "text": "and now we have a three-layer\nneural network, right?",
      "start": 3824.943,
      "duration": 2.83,
      "language": "en"
    },
    {
      "text": "And so this is where the\nterm deep neural networks",
      "start": 3827.773,
      "duration": 2.148,
      "language": "en"
    },
    {
      "text": "is basically coming from, right? This idea that you can stack\nmultiple of these layers,",
      "start": 3829.921,
      "duration": 5.1599999999998545,
      "language": "en"
    },
    {
      "text": "you know, for very deep networks.",
      "start": 3835.081,
      "duration": 2.75,
      "language": "en"
    },
    {
      "text": "And so in homework you'll get a practice",
      "start": 3840.153,
      "duration": 3.333,
      "language": "en"
    },
    {
      "text": "of writing and you know, training one of",
      "start": 3844.453,
      "duration": 3.17,
      "language": "en"
    },
    {
      "text": "these neural networks, I\nthink in assignment two,",
      "start": 3847.623,
      "duration": 2.74,
      "language": "en"
    },
    {
      "text": "but basically a full\nimplementation of this using",
      "start": 3850.363,
      "duration": 3.171,
      "language": "en"
    },
    {
      "text": "this idea of forward pass, right, and backward passes, and using chain rule",
      "start": 3853.534,
      "duration": 4.543999999999869,
      "language": "en"
    },
    {
      "text": "to compute gradients\nthat we've already seen.",
      "start": 3858.078,
      "duration": 2.419,
      "language": "en"
    },
    {
      "text": "The entire implementation of\na two-layer neural network",
      "start": 3860.497,
      "duration": 2.427,
      "language": "en"
    },
    {
      "text": "is actually really simple, it\ncan just be done in 20 lines,",
      "start": 3862.924,
      "duration": 4.52,
      "language": "en"
    },
    {
      "text": "and so you'll get some practice\nwith this in assignment two,",
      "start": 3867.444,
      "duration": 3.687,
      "language": "en"
    },
    {
      "text": "writing out all of these parts.",
      "start": 3871.131,
      "duration": 2.63,
      "language": "en"
    },
    {
      "text": "And okay, so now that we've sort of seen",
      "start": 3873.761,
      "duration": 2.508,
      "language": "en"
    },
    {
      "text": "what neural networks are\nas a function, right, like, you know, we hear\npeople talking a lot about",
      "start": 3876.269,
      "duration": 4.735999999999876,
      "language": "en"
    },
    {
      "text": "how there's biological\ninspirations for neural networks,",
      "start": 3881.005,
      "duration": 3.608,
      "language": "en"
    },
    {
      "text": "and so even though it's\nimportant that to emphasize",
      "start": 3884.613,
      "duration": 3.203,
      "language": "en"
    },
    {
      "text": "that these analogies are really loose,",
      "start": 3887.816,
      "duration": 3.11,
      "language": "en"
    },
    {
      "text": "it's really just very loose ties,",
      "start": 3890.926,
      "duration": 2.949,
      "language": "en"
    },
    {
      "text": "but it's still interesting to understand",
      "start": 3893.875,
      "duration": 2.428,
      "language": "en"
    },
    {
      "text": "where some of these connections\nand inspirations come from.",
      "start": 3896.303,
      "duration": 2.846,
      "language": "en"
    },
    {
      "text": "And so now I'm going to\ntalk briefly about that.",
      "start": 3899.149,
      "duration": 4.296,
      "language": "en"
    },
    {
      "text": "So if we think about a neuron, in kind of",
      "start": 3903.445,
      "duration": 2.267,
      "language": "en"
    },
    {
      "text": "a very simple way, this neuron is,",
      "start": 3905.712,
      "duration": 2.833,
      "language": "en"
    },
    {
      "text": "here's a diagram of a neuron. We have the impulses\nthat are carried towards",
      "start": 3909.396,
      "duration": 4.640999999999622,
      "language": "en"
    },
    {
      "text": "each neuron, right, so we have a lot of neurons connected together\nand each neuron has dendrites,",
      "start": 3914.037,
      "duration": 5.4659999999998945,
      "language": "en"
    },
    {
      "text": "right, and these are sort\nof, these are what receives",
      "start": 3919.503,
      "duration": 2.505,
      "language": "en"
    },
    {
      "text": "the impulses that come into the neuron.",
      "start": 3922.008,
      "duration": 3.166,
      "language": "en"
    },
    {
      "text": "And then we have a cell body, right, that basically integrates\nthese signals coming in",
      "start": 3925.174,
      "duration": 5.188000000000102,
      "language": "en"
    },
    {
      "text": "and then there's a kind\nof, then it takes this,",
      "start": 3930.362,
      "duration": 3.656,
      "language": "en"
    },
    {
      "text": "and after integrating all\nthese signals, it passes on,",
      "start": 3934.018,
      "duration": 2.909,
      "language": "en"
    },
    {
      "text": "you know, the impulse carries away from the cell body to downstream\nneurons that it's connected to,",
      "start": 3936.927,
      "duration": 5.6159999999999854,
      "language": "en"
    },
    {
      "text": "right, and it carries\nthis away through axons.",
      "start": 3942.543,
      "duration": 3.833,
      "language": "en"
    },
    {
      "text": "So now if we look at what\nwe've been doing so far, right,",
      "start": 3948.337,
      "duration": 3.243,
      "language": "en"
    },
    {
      "text": "with each computational node, you can see",
      "start": 3951.58,
      "duration": 2.821,
      "language": "en"
    },
    {
      "text": "that this actually has, you can see it",
      "start": 3954.401,
      "duration": 2.996,
      "language": "en"
    },
    {
      "text": "in kind of a similar way, right?",
      "start": 3957.397,
      "duration": 2.195,
      "language": "en"
    },
    {
      "text": "Where nodes are connected to each other in",
      "start": 3959.592,
      "duration": 2.102,
      "language": "en"
    },
    {
      "text": "the computational graph,\nand we have inputs,",
      "start": 3961.694,
      "duration": 3.994,
      "language": "en"
    },
    {
      "text": "or signals, x, x right,\ncoming into a neuron,",
      "start": 3965.688,
      "duration": 4.295,
      "language": "en"
    },
    {
      "text": "and then all of these x,\nright, x-zero, x-one, x-two,",
      "start": 3969.983,
      "duration": 3.749,
      "language": "en"
    },
    {
      "text": "these are combined and\nintegrated together, right,",
      "start": 3973.732,
      "duration": 2.98,
      "language": "en"
    },
    {
      "text": "using, for example our weights, W.",
      "start": 3976.712,
      "duration": 3.1,
      "language": "en"
    },
    {
      "text": "So we do some sort of computation, right,",
      "start": 3979.812,
      "duration": 3.009,
      "language": "en"
    },
    {
      "text": "and in some of the computations\nwe've been doing so far,",
      "start": 3982.821,
      "duration": 2.288,
      "language": "en"
    },
    {
      "text": "something like W times x plus b, right,",
      "start": 3985.109,
      "duration": 4.272,
      "language": "en"
    },
    {
      "text": "integrating all these together,",
      "start": 3989.381,
      "duration": 2.229,
      "language": "en"
    },
    {
      "text": "and then we have an activation function that we apply on top, we get\nthis value of this output,",
      "start": 3991.61,
      "duration": 5.320999999999458,
      "language": "en"
    },
    {
      "text": "and we pass it down to\nthe connecting neurons.",
      "start": 3996.931,
      "duration": 3.833,
      "language": "en"
    },
    {
      "text": "So if you look at that\nthis, this is actually,",
      "start": 4001.733,
      "duration": 2.335,
      "language": "en"
    },
    {
      "text": "you can think about this in\na very similar way, right? Like, you know, these are\nwhat's the signals coming in",
      "start": 4004.068,
      "duration": 5.166999999999916,
      "language": "en"
    },
    {
      "text": "are kind of the, connected\nat synapses, right?",
      "start": 4009.235,
      "duration": 4.169,
      "language": "en"
    },
    {
      "text": "The synapse connecting\nthe multiple neurons,",
      "start": 4013.404,
      "duration": 2.885,
      "language": "en"
    },
    {
      "text": "the dendrites are\nintegrating all of these,",
      "start": 4016.289,
      "duration": 3.456,
      "language": "en"
    },
    {
      "text": "they're integrating all of\nthis information together in",
      "start": 4019.745,
      "duration": 2.982,
      "language": "en"
    },
    {
      "text": "the cell body, and then we have the output carried on the output later on.",
      "start": 4022.727,
      "duration": 5.762000000000171,
      "language": "en"
    },
    {
      "text": "And so this is kind of the analogy that you can draw between them,",
      "start": 4028.489,
      "duration": 3.469000000000051,
      "language": "en"
    },
    {
      "text": "and if you look at these\nactivation functions, right?",
      "start": 4031.958,
      "duration": 3.282,
      "language": "en"
    },
    {
      "text": "This is what basically takes\nall the inputs coming in",
      "start": 4035.24,
      "duration": 3.47,
      "language": "en"
    },
    {
      "text": "and outputs one number\nthat's going out later on,",
      "start": 4038.71,
      "duration": 3.108,
      "language": "en"
    },
    {
      "text": "and we've talked about examples like sigmoid activation function, right,",
      "start": 4041.818,
      "duration": 4.404999999999745,
      "language": "en"
    },
    {
      "text": "and different kinds of non-linearities,",
      "start": 4046.223,
      "duration": 2.071,
      "language": "en"
    },
    {
      "text": "and so sort of one kind of\nloose analogy that you can draw",
      "start": 4048.294,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "is that these\nnon-linearities can represent",
      "start": 4054.601,
      "duration": 2.739,
      "language": "en"
    },
    {
      "text": "something sort of like the firing,",
      "start": 4057.34,
      "duration": 2.491,
      "language": "en"
    },
    {
      "text": "or spiking rate of the neurons, right?",
      "start": 4059.831,
      "duration": 2.537,
      "language": "en"
    },
    {
      "text": "Where our neurons transmit\nsignals to connecting neurons",
      "start": 4062.368,
      "duration": 4.403,
      "language": "en"
    },
    {
      "text": "using kind of these\ndiscrete spikes, right?",
      "start": 4066.771,
      "duration": 2.403,
      "language": "en"
    },
    {
      "text": "And so we can think of, you know,",
      "start": 4069.174,
      "duration": 2.041,
      "language": "en"
    },
    {
      "text": "if they're spiking very\nfast then there's kind of",
      "start": 4071.215,
      "duration": 2.694,
      "language": "en"
    },
    {
      "text": "a strong signal that's passed later on,",
      "start": 4073.909,
      "duration": 2.755,
      "language": "en"
    },
    {
      "text": "and so we can think of this value after our activation function\nas sort of, in a sense,",
      "start": 4076.664,
      "duration": 5.37099999999964,
      "language": "en"
    },
    {
      "text": "sort of this firing rate\nthat we're going to pass on.",
      "start": 4082.035,
      "duration": 3.3,
      "language": "en"
    },
    {
      "text": "And you know in practice,\nI think neuroscientists",
      "start": 4085.335,
      "duration": 2.749,
      "language": "en"
    },
    {
      "text": "who are actually studying this say that kind of one of the non-linearities",
      "start": 4088.084,
      "duration": 3.5300000000002,
      "language": "en"
    },
    {
      "text": "that are most similar to the way",
      "start": 4091.614,
      "duration": 2.879,
      "language": "en"
    },
    {
      "text": "that neurons are actually behaving is a ReLU function, which\nis a ReLU non-linearity,",
      "start": 4094.493,
      "duration": 5.067999999999756,
      "language": "en"
    },
    {
      "text": "which is something that we're going to look at more later\non, but it's a function",
      "start": 4099.561,
      "duration": 3.86200000000008,
      "language": "en"
    },
    {
      "text": "that's at zero for all\nnegative values of input,",
      "start": 4103.423,
      "duration": 4.737,
      "language": "en"
    },
    {
      "text": "and then it's a linear\nfunction for everything",
      "start": 4108.16,
      "duration": 3.556,
      "language": "en"
    },
    {
      "text": "that's in kind of a positive regime.",
      "start": 4111.716,
      "duration": 2.356,
      "language": "en"
    },
    {
      "text": "And so, you know, we'll talk more about this activation function later on,",
      "start": 4114.072,
      "duration": 2.6549999999997453,
      "language": "en"
    },
    {
      "text": "but that's kind of, in practice,",
      "start": 4116.727,
      "duration": 2.449,
      "language": "en"
    },
    {
      "text": "maybe the one that's most similar to how neurons are actually behaving.",
      "start": 4119.176,
      "duration": 4.828999999998814,
      "language": "en"
    },
    {
      "text": "But it's really important\nto be extremely careful",
      "start": 4126.021,
      "duration": 3.026,
      "language": "en"
    },
    {
      "text": "with making any of these\nsorts of brain analogies,",
      "start": 4129.047,
      "duration": 3.01,
      "language": "en"
    },
    {
      "text": "because in practice biological neurons are way more complex than this.",
      "start": 4132.057,
      "duration": 4.331000000000131,
      "language": "en"
    },
    {
      "text": "There's many different\nkinds of biological neurons,",
      "start": 4136.388,
      "duration": 3.828,
      "language": "en"
    },
    {
      "text": "the dendrites can perform really complex non-linear computations.",
      "start": 4140.216,
      "duration": 4.734999999999673,
      "language": "en"
    },
    {
      "text": "Our synapses, right, the\nW-zeros that we had earlier",
      "start": 4144.951,
      "duration": 2.933,
      "language": "en"
    },
    {
      "text": "where we drew this analogy,\nare not single weights",
      "start": 4147.884,
      "duration": 3.497,
      "language": "en"
    },
    {
      "text": "like we had, they're actually\nreally complex, you know,",
      "start": 4151.381,
      "duration": 2.652,
      "language": "en"
    },
    {
      "text": "non-linear dynamical systems in practice,",
      "start": 4154.033,
      "duration": 3.054,
      "language": "en"
    },
    {
      "text": "and also this idea of interpreting\nour activation function",
      "start": 4157.087,
      "duration": 4.515,
      "language": "en"
    },
    {
      "text": "as a sort of rate code\nor firing rate is also,",
      "start": 4161.602,
      "duration": 3.923,
      "language": "en"
    },
    {
      "text": "is insufficient in practice, you know.",
      "start": 4165.525,
      "duration": 2.998,
      "language": "en"
    },
    {
      "text": "It's just this kind of\nfiring rate is probably not",
      "start": 4168.523,
      "duration": 3.728,
      "language": "en"
    },
    {
      "text": "a sufficient model of how neurons",
      "start": 4172.251,
      "duration": 2.421,
      "language": "en"
    },
    {
      "text": "will actually communicate to\ndownstream neurons, right,",
      "start": 4174.672,
      "duration": 2.543,
      "language": "en"
    },
    {
      "text": "like even as a very simple\nway, there's a very,",
      "start": 4177.215,
      "duration": 4.151,
      "language": "en"
    },
    {
      "text": "the neurons will fire at a variable rate,",
      "start": 4181.366,
      "duration": 3.183,
      "language": "en"
    },
    {
      "text": "and this variability probably\nshould be taken into account.",
      "start": 4184.549,
      "duration": 3.294,
      "language": "en"
    },
    {
      "text": "And so there's all of these, you know,",
      "start": 4187.843,
      "duration": 2.071,
      "language": "en"
    },
    {
      "text": "it's kind of a much more complex thing",
      "start": 4189.914,
      "duration": 3.444,
      "language": "en"
    },
    {
      "text": "than what we're dealing with.",
      "start": 4193.358,
      "duration": 2.868,
      "language": "en"
    },
    {
      "text": "There's references, for example\nthis dendritic computation",
      "start": 4196.226,
      "duration": 4.466,
      "language": "en"
    },
    {
      "text": "that you can look at if you're\ninterested in this topic,",
      "start": 4200.692,
      "duration": 3.252,
      "language": "en"
    },
    {
      "text": "but yeah, so that in practice, you know,",
      "start": 4203.944,
      "duration": 2.43,
      "language": "en"
    },
    {
      "text": "we can sort of see how\nit may resemble a neuron",
      "start": 4206.374,
      "duration": 2.327,
      "language": "en"
    },
    {
      "text": "at this very high level, but neurons are,",
      "start": 4208.701,
      "duration": 3.182,
      "language": "en"
    },
    {
      "text": "in practice, much more\ncomplicated than that.",
      "start": 4211.883,
      "duration": 4.033,
      "language": "en"
    },
    {
      "text": "Okay, so we talked about how\nthere's many different kinds",
      "start": 4215.916,
      "duration": 3.082,
      "language": "en"
    },
    {
      "text": "of activation functions\nthat could be used,",
      "start": 4218.998,
      "duration": 2.12,
      "language": "en"
    },
    {
      "text": "there's the ReLU that I mentioned earlier,",
      "start": 4221.118,
      "duration": 2.947,
      "language": "en"
    },
    {
      "text": "and we'll talk about all\nof these different kinds of activation functions in\nmuch more detail later on,",
      "start": 4224.065,
      "duration": 5.76299999999992,
      "language": "en"
    },
    {
      "text": "choices of these activation functions",
      "start": 4229.828,
      "duration": 2.04,
      "language": "en"
    },
    {
      "text": "that you might want to use.",
      "start": 4231.868,
      "duration": 2.606,
      "language": "en"
    },
    {
      "text": "And so we'll also talk\nabout different kinds of neural network architectures.",
      "start": 4234.474,
      "duration": 4.4340000000001965,
      "language": "en"
    },
    {
      "text": "So we gave the example\nof these fully connected",
      "start": 4238.908,
      "duration": 3.917,
      "language": "en"
    },
    {
      "text": "neural networks, right,\nwhere each layer is this matrix multiply, and\nso the way we actually want",
      "start": 4244.362,
      "duration": 6.048000000000684,
      "language": "en"
    },
    {
      "text": "to call these is, we said\ntwo-layer neural network before,",
      "start": 4252.362,
      "duration": 3.607,
      "language": "en"
    },
    {
      "text": "and that corresponded to\nthe fact that we have two",
      "start": 4255.969,
      "duration": 2.566,
      "language": "en"
    },
    {
      "text": "of these linear layers,\nright, where we're doing",
      "start": 4258.535,
      "duration": 2.821,
      "language": "en"
    },
    {
      "text": "a matrix multiply, two\nfully connected layers",
      "start": 4261.356,
      "duration": 3.018,
      "language": "en"
    },
    {
      "text": "is what we call these.",
      "start": 4264.374,
      "duration": 2.133,
      "language": "en"
    },
    {
      "text": "We could also call this a\none-hidden-layer neural network,",
      "start": 4266.507,
      "duration": 2.676,
      "language": "en"
    },
    {
      "text": "so instead of counting the number of matrix multiplies we're doing,",
      "start": 4269.183,
      "duration": 4.002999999999702,
      "language": "en"
    },
    {
      "text": "counting the number of\nhidden layers that we have.",
      "start": 4273.186,
      "duration": 2.333,
      "language": "en"
    },
    {
      "text": "I think it's, you can use either,",
      "start": 4275.519,
      "duration": 2.135,
      "language": "en"
    },
    {
      "text": "I think maybe two-layer neural network is something that's a\nlittle more commonly used.",
      "start": 4277.654,
      "duration": 4.264000000000124,
      "language": "en"
    },
    {
      "text": "And then also here, for our\nthree-layer neural network",
      "start": 4281.918,
      "duration": 2.115,
      "language": "en"
    },
    {
      "text": "that we have, this can also be called",
      "start": 4284.033,
      "duration": 3.286,
      "language": "en"
    },
    {
      "text": "a two-hidden-layer neural network.",
      "start": 4287.319,
      "duration": 2.833,
      "language": "en"
    },
    {
      "text": "And so we saw that, you\nknow, when we're doing",
      "start": 4292.77,
      "duration": 3.989,
      "language": "en"
    },
    {
      "text": "this type of feed forward, right, forward pass through a neural network,",
      "start": 4296.759,
      "duration": 4.570999999999913,
      "language": "en"
    },
    {
      "text": "each of these nodes in this network",
      "start": 4301.33,
      "duration": 2.917,
      "language": "en"
    },
    {
      "text": "is basically doing the\nkind of operation of",
      "start": 4306.768,
      "duration": 3.486,
      "language": "en"
    },
    {
      "text": "the neuron that I showed earlier, right?",
      "start": 4310.254,
      "duration": 3.333,
      "language": "en"
    },
    {
      "text": "And so what's actually happening is",
      "start": 4315.875,
      "duration": 2.108,
      "language": "en"
    },
    {
      "text": "is basically each hidden\nlayer you can think of",
      "start": 4317.983,
      "duration": 2.513,
      "language": "en"
    },
    {
      "text": "as a whole vector, right,\na set of these neurons,",
      "start": 4320.496,
      "duration": 3.281,
      "language": "en"
    },
    {
      "text": "and so by writing it out this way",
      "start": 4323.777,
      "duration": 2.336,
      "language": "en"
    },
    {
      "text": "with these matrix multiplies\nto compute our neuron values,",
      "start": 4326.113,
      "duration": 4.715,
      "language": "en"
    },
    {
      "text": "it's a way that we can\nefficiently evaluate",
      "start": 4330.828,
      "duration": 2.335,
      "language": "en"
    },
    {
      "text": "this entire layer of neurons, right? So with one matrix multiply\nwe get output values",
      "start": 4333.163,
      "duration": 5.391000000000531,
      "language": "en"
    },
    {
      "text": "of, you know, of a layer of let's say 10,",
      "start": 4338.554,
      "duration": 3.11,
      "language": "en"
    },
    {
      "text": "or 50 or 100 of neurons.",
      "start": 4341.664,
      "duration": 2.0,
      "language": "en"
    },
    {
      "text": "All right, and so looking at\nthis again, writing this out,",
      "start": 4346.389,
      "duration": 4.693,
      "language": "en"
    },
    {
      "text": "all out in matrix form,\nmatrix-vector form,",
      "start": 4351.082,
      "duration": 3.515,
      "language": "en"
    },
    {
      "text": "we have our, you know, non-linearity here.",
      "start": 4354.597,
      "duration": 3.582,
      "language": "en"
    },
    {
      "text": "F that we're using, in this\ncase a sigmoid function, right,",
      "start": 4358.179,
      "duration": 2.564,
      "language": "en"
    },
    {
      "text": "and we can take our data\nx, some input vector",
      "start": 4360.743,
      "duration": 3.308,
      "language": "en"
    },
    {
      "text": "or our values, and we can apply\nour first matrix multiply,",
      "start": 4364.051,
      "duration": 4.175,
      "language": "en"
    },
    {
      "text": "W-one on top of this,\nthen our non-linearity,",
      "start": 4368.226,
      "duration": 3.75,
      "language": "en"
    },
    {
      "text": "then a second matrix multiply to get a second hidden layer, h-two,",
      "start": 4372.985,
      "duration": 3.143000000000029,
      "language": "en"
    },
    {
      "text": "and then we have our final output, right?",
      "start": 4376.128,
      "duration": 3.055,
      "language": "en"
    },
    {
      "text": "And so, you know, this\nis basically all you need",
      "start": 4379.183,
      "duration": 3.048,
      "language": "en"
    },
    {
      "text": "to be able to write a neural network,",
      "start": 4382.231,
      "duration": 3.058,
      "language": "en"
    },
    {
      "text": "and as we saw earlier, the backward pass.",
      "start": 4385.289,
      "duration": 3.08,
      "language": "en"
    },
    {
      "text": "You then just use backprop\nto compute all of those,",
      "start": 4388.369,
      "duration": 2.83,
      "language": "en"
    },
    {
      "text": "and so that's basically all there is",
      "start": 4391.199,
      "duration": 3.0,
      "language": "en"
    },
    {
      "text": "to kind of the main idea\nof what's a neural network.",
      "start": 4395.289,
      "duration": 4.167,
      "language": "en"
    },
    {
      "text": "Okay, so just to\nsummarize, we talked about",
      "start": 4401.451,
      "duration": 2.664,
      "language": "en"
    },
    {
      "text": "how we could arrange neurons\ninto these computations, right,",
      "start": 4404.115,
      "duration": 4.339,
      "language": "en"
    },
    {
      "text": "of fully-connected or linear layers.",
      "start": 4408.454,
      "duration": 3.589,
      "language": "en"
    },
    {
      "text": "This abstraction of a\nlayer has a nice property",
      "start": 4412.043,
      "duration": 2.599,
      "language": "en"
    },
    {
      "text": "that we can use very\nefficient vectorized code",
      "start": 4414.642,
      "duration": 2.084,
      "language": "en"
    },
    {
      "text": "to compute all of these. We also talked about how it's important",
      "start": 4416.726,
      "duration": 3.8750000000009095,
      "language": "en"
    },
    {
      "text": "to keep in mind that neural networks",
      "start": 4420.601,
      "duration": 2.027,
      "language": "en"
    },
    {
      "text": "do have some, you know,\nanalogy and loose inspiration",
      "start": 4422.628,
      "duration": 3.177,
      "language": "en"
    },
    {
      "text": "from biology, but they're\nnot really neural.",
      "start": 4425.805,
      "duration": 2.909,
      "language": "en"
    },
    {
      "text": "I mean, this is a pretty loose analogy that we're making, and\nnext time we'll talk",
      "start": 4428.714,
      "duration": 4.320999999999913,
      "language": "en"
    },
    {
      "text": "about convolutional neural networks.",
      "start": 4433.035,
      "duration": 3.284,
      "language": "en"
    },
    {
      "text": "Okay, thanks.",
      "start": 4436.319,
      "duration": 1.083,
      "language": "en"
    }
  ]
}