{
  "video_id": "ptFiH_bHnJw",
  "created_at": "2025-07-26T19:13:05.690396",
  "segment_count": 2397,
  "metadata": {
    "video_id": "ptFiH_bHnJw",
    "language": "en",
    "segment_count": 2397
  },
  "transcript": [
    {
      "text": "Um, as you may have noticed, I'm a",
      "start": 5.04,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "little bit less uh innovative in my",
      "start": 6.72,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "lecturing than Percy. So, you're going",
      "start": 8.96,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "to get um PowerPoint slides rather than",
      "start": 10.4,
      "duration": 4.319,
      "language": "en"
    },
    {
      "text": "um executable Python ones, but you",
      "start": 12.8,
      "duration": 4.239,
      "language": "en"
    },
    {
      "text": "should be able to find the PDFs um on",
      "start": 14.719,
      "duration": 5.041,
      "language": "en"
    },
    {
      "text": "the website as well. So I I've titled",
      "start": 17.039,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "this lecture everything you didn't want",
      "start": 19.76,
      "duration": 2.96,
      "language": "en"
    },
    {
      "text": "to know about LM architecture and",
      "start": 21.279,
      "duration": 3.041,
      "language": "en"
    },
    {
      "text": "training because we're going to get into",
      "start": 22.72,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "some of the nitty-gritty details that I",
      "start": 24.32,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "think most other classes uh would spare",
      "start": 26.08,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "you the details of you know like what",
      "start": 28.24,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "should my hyperparameters be and those",
      "start": 30.4,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "kinds of questions um some minor",
      "start": 32.079,
      "duration": 4.961,
      "language": "en"
    },
    {
      "text": "logistics um also if you're doing the",
      "start": 34.88,
      "duration": 3.999,
      "language": "en"
    },
    {
      "text": "assignments we are updating assignments",
      "start": 37.04,
      "duration": 5.039,
      "language": "en"
    },
    {
      "text": "as we find some uh mostly minor bugs",
      "start": 38.879,
      "duration": 5.041,
      "language": "en"
    },
    {
      "text": "make sure you pull uh updates to the",
      "start": 42.079,
      "duration": 4.761,
      "language": "en"
    },
    {
      "text": "assignments um as you go along",
      "start": 43.92,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "um Okay. So, what we're going to do,",
      "start": 46.84,
      "duration": 4.44,
      "language": "en"
    },
    {
      "text": "we're going to start with a quick recap",
      "start": 49.6,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "of a transformer. Um, and I'll give you",
      "start": 51.28,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "two variants of a standard transformer.",
      "start": 53.36,
      "duration": 3.519,
      "language": "en"
    },
    {
      "text": "One that's, you know, probably coming",
      "start": 55.44,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "from the the standard transformer, you",
      "start": 56.879,
      "duration": 4.441,
      "language": "en"
    },
    {
      "text": "know, lectures that you might see in",
      "start": 59.84,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "224n. Um, and then I'll talk about what",
      "start": 61.32,
      "duration": 4.44,
      "language": "en"
    },
    {
      "text": "you implement, um, and kind of the",
      "start": 64.0,
      "duration": 3.4,
      "language": "en"
    },
    {
      "text": "modern consensus variant of a",
      "start": 65.76,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "transformer. And then we're going to",
      "start": 67.4,
      "duration": 3.56,
      "language": "en"
    },
    {
      "text": "take a much more kind of datadriven",
      "start": 69.04,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "perspective to understanding transformer",
      "start": 70.96,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "architectures. So the question that",
      "start": 73.2,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "we're going to ask is people have",
      "start": 74.72,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "trained lots of LLMs at this point and",
      "start": 76.56,
      "duration": 4.239,
      "language": "en"
    },
    {
      "text": "you can go and read you know all of",
      "start": 79.119,
      "duration": 3.441,
      "language": "en"
    },
    {
      "text": "those papers and try to understand what",
      "start": 80.799,
      "duration": 3.921,
      "language": "en"
    },
    {
      "text": "has changed what has been in common and",
      "start": 82.56,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "from that kind of almost an evolutionary",
      "start": 84.72,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "analysis you know try to understand what",
      "start": 86.56,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "are the things that are really important",
      "start": 88.96,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "to make transformers work right so",
      "start": 90.24,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "today's theme is the theme of the class",
      "start": 92.32,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "is the best way to learn is hands-on",
      "start": 94.32,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "experience but the theme of this lecture",
      "start": 96.0,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "because we can't train all these",
      "start": 97.68,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "transformers is to learn from the",
      "start": 98.88,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "experience of others",
      "start": 100.56,
      "duration": 2.72,
      "language": "en"
    },
    {
      "text": "So the starting point is the original",
      "start": 103.68,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "transformer, right? So just as a review,",
      "start": 106.0,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "right? Hopefully you all remember this",
      "start": 108.24,
      "duration": 4.879,
      "language": "en"
    },
    {
      "text": "from 224N or your other NLP classes. You",
      "start": 110.159,
      "duration": 5.041,
      "language": "en"
    },
    {
      "text": "know, you've got some simple position",
      "start": 113.119,
      "duration": 4.081,
      "language": "en"
    },
    {
      "text": "embeddings at the bottom. You've got",
      "start": 115.2,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "multi head attention, you've got uh",
      "start": 117.2,
      "duration": 3.919,
      "language": "en"
    },
    {
      "text": "layer norms afterwards, you've got a",
      "start": 119.68,
      "duration": 2.96,
      "language": "en"
    },
    {
      "text": "residual stream going upwards, you've",
      "start": 121.119,
      "duration": 3.841,
      "language": "en"
    },
    {
      "text": "got a MLP, and then a softmax at the",
      "start": 122.64,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "very end. Um, and we're going to see",
      "start": 124.96,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "variance to all these different pieces",
      "start": 126.32,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "um until we get to basically the most",
      "start": 128.56,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "modern variants um of the transformer",
      "start": 130.399,
      "duration": 4.161,
      "language": "en"
    },
    {
      "text": "and the and the latest one I'll talk",
      "start": 132.959,
      "duration": 4.121,
      "language": "en"
    },
    {
      "text": "about will be just you know a few months",
      "start": 134.56,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "before. So what you implemented is not",
      "start": 137.08,
      "duration": 5.08,
      "language": "en"
    },
    {
      "text": "you know the the vanilla transformer",
      "start": 139.92,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "variant um from the original paper. Um",
      "start": 142.16,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "we've modified a few things you know",
      "start": 144.8,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "we've put the layer norm in front of the",
      "start": 146.8,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "block. So you can see um on this slide",
      "start": 148.48,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "over here that you know there's the norm",
      "start": 150.64,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "is over here right before each of these",
      "start": 152.319,
      "duration": 4.721,
      "language": "en"
    },
    {
      "text": "blocks in the residual stream. Um we've",
      "start": 154.48,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "asked you to implement rotary position",
      "start": 157.04,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "embeddings. Um the feed forward layers",
      "start": 158.72,
      "duration": 4.879,
      "language": "en"
    },
    {
      "text": "use something called a swiggloo. Um and",
      "start": 161.12,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "then linear layers um you know now emit",
      "start": 163.599,
      "duration": 5.681,
      "language": "en"
    },
    {
      "text": "these bias terms. Um and you might ask",
      "start": 166.4,
      "duration": 4.559,
      "language": "en"
    },
    {
      "text": "why have you forced us to implement this",
      "start": 169.28,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "weird variant of a transformer instead",
      "start": 170.959,
      "duration": 3.841,
      "language": "en"
    },
    {
      "text": "of the original transformer is all you",
      "start": 172.879,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "need transformer. Um, and so we're going",
      "start": 174.8,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "to go through some of those questions.",
      "start": 178.319,
      "duration": 2.721,
      "language": "en"
    },
    {
      "text": "And then yesterday I was thinking, okay,",
      "start": 179.68,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "I should I should catch up on all the",
      "start": 181.04,
      "duration": 3.119,
      "language": "en"
    },
    {
      "text": "developments that have happened in",
      "start": 182.56,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "architectures over the last year. Um,",
      "start": 184.159,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "and Percy warned me about this because",
      "start": 186.0,
      "duration": 2.64,
      "language": "en"
    },
    {
      "text": "he said, you're going to have to redo",
      "start": 187.519,
      "duration": 3.041,
      "language": "en"
    },
    {
      "text": "the lecture every year. And so I started",
      "start": 188.64,
      "duration": 3.04,
      "language": "en"
    },
    {
      "text": "looking and I was like, all right, yeah,",
      "start": 190.56,
      "duration": 2.48,
      "language": "en"
    },
    {
      "text": "there's a couple good good papers",
      "start": 191.68,
      "duration": 3.279,
      "language": "en"
    },
    {
      "text": "recently. There's Command A, there's two",
      "start": 193.04,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "mode furious, there's, you know, small",
      "start": 194.959,
      "duration": 4.161,
      "language": "en"
    },
    {
      "text": "LM and 54. And then you go looking and",
      "start": 196.879,
      "duration": 3.921,
      "language": "en"
    },
    {
      "text": "you're like, wow, yeah, there's Gemma 3",
      "start": 199.12,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "and Quent 2.5 and intern LM and then",
      "start": 200.8,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "there's, you know, more. I can't even",
      "start": 203.36,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "sort of you know cover uh the screen",
      "start": 205.12,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "with these guys right there's there's a",
      "start": 207.04,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "lot of models there were about uh 19 new",
      "start": 208.48,
      "duration": 5.759,
      "language": "en"
    },
    {
      "text": "dense model releases in the last year um",
      "start": 211.28,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "many of them with minor architecture",
      "start": 214.239,
      "duration": 3.441,
      "language": "en"
    },
    {
      "text": "tweaks and on the one hand it's kind of",
      "start": 215.68,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "annoying to go through all these papers",
      "start": 217.68,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "and say like you know what is happening",
      "start": 219.36,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "in all of these um but also it's like a",
      "start": 221.68,
      "duration": 3.919,
      "language": "en"
    },
    {
      "text": "actually wealth of information because",
      "start": 224.08,
      "duration": 3.04,
      "language": "en"
    },
    {
      "text": "not all of them do the same thing and",
      "start": 225.599,
      "duration": 3.041,
      "language": "en"
    },
    {
      "text": "you can kind of see you know not all of",
      "start": 227.12,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "you can especially in the back can see",
      "start": 228.64,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "the details of this slide um but I I put",
      "start": 230.4,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "together a little spreadsheet sheet of",
      "start": 233.36,
      "duration": 2.879,
      "language": "en"
    },
    {
      "text": "you know what all these models are doing",
      "start": 234.799,
      "duration": 2.8,
      "language": "en"
    },
    {
      "text": "and starting with you know all the way",
      "start": 236.239,
      "duration": 3.761,
      "language": "en"
    },
    {
      "text": "from 2017 the original transformer all",
      "start": 237.599,
      "duration": 4.481,
      "language": "en"
    },
    {
      "text": "the way to 2025 what the newest models",
      "start": 240.0,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "are doing and we'll talk about this as",
      "start": 242.08,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "we go but you kind of see sort of",
      "start": 244.48,
      "duration": 3.759,
      "language": "en"
    },
    {
      "text": "certain kinds of architecture changes",
      "start": 246.56,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "sort of being explored like so here on",
      "start": 248.239,
      "duration": 3.441,
      "language": "en"
    },
    {
      "text": "this column is position embeddings",
      "start": 250.08,
      "duration": 2.879,
      "language": "en"
    },
    {
      "text": "people used to do all sorts of stuff",
      "start": 251.68,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "like absolute relative rope uh there was",
      "start": 252.959,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "a sort of um alibi phase for some people",
      "start": 255.519,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "but then now starting around 2023",
      "start": 258.479,
      "duration": 3.681,
      "language": "en"
    },
    {
      "text": "everyone just does rope right so you can",
      "start": 260.479,
      "duration": 3.041,
      "language": "en"
    },
    {
      "text": "kind of see this the convergent",
      "start": 262.16,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "evolution almost um of neural",
      "start": 263.52,
      "duration": 3.119,
      "language": "en"
    },
    {
      "text": "architectures and we're going to talk",
      "start": 265.52,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "about um all of these different kinds of",
      "start": 266.639,
      "duration": 3.241,
      "language": "en"
    },
    {
      "text": "things.",
      "start": 268.4,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "Right? So the the parts that I'll cover",
      "start": 269.88,
      "duration": 5.4,
      "language": "en"
    },
    {
      "text": "so this is a preview of the three major",
      "start": 273.04,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "sections of this lecture and if I have",
      "start": 275.28,
      "duration": 2.96,
      "language": "en"
    },
    {
      "text": "time I'm also going to talk about um",
      "start": 276.8,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "different attention variants at the end.",
      "start": 278.24,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "Um the first thing is going to be",
      "start": 280.24,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "architecture variations. Um that's what",
      "start": 281.68,
      "duration": 3.519,
      "language": "en"
    },
    {
      "text": "I'm going to talk about. So activations,",
      "start": 283.52,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "feed forwards, attention variance,",
      "start": 285.199,
      "duration": 2.961,
      "language": "en"
    },
    {
      "text": "position embeddings, all of those",
      "start": 286.96,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "things. Um and then having nailed down",
      "start": 288.16,
      "duration": 4.319,
      "language": "en"
    },
    {
      "text": "the architecture, what do we have to do?",
      "start": 290.32,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "Well, we have to pick hyperparameters,",
      "start": 292.479,
      "duration": 3.921,
      "language": "en"
    },
    {
      "text": "right? Like how big do we make the the",
      "start": 294.08,
      "duration": 4.559,
      "language": "en"
    },
    {
      "text": "hidden dimension? How big do we make the",
      "start": 296.4,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "sort of inner projection layer inside of",
      "start": 298.639,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "MLP? Um, what do we do about the number",
      "start": 300.4,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "of dimensions? How many vocab elements?",
      "start": 302.639,
      "duration": 3.201,
      "language": "en"
    },
    {
      "text": "Those are all sort of important things",
      "start": 304.4,
      "duration": 2.639,
      "language": "en"
    },
    {
      "text": "that you have to choose when you're",
      "start": 305.84,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "actually training uh your language",
      "start": 307.039,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "model. Um, and you don't want to just",
      "start": 308.72,
      "duration": 3.919,
      "language": "en"
    },
    {
      "text": "sort of pick these out of a hat, right?",
      "start": 311.36,
      "duration": 2.8,
      "language": "en"
    },
    {
      "text": "You want to select them in some fairly",
      "start": 312.639,
      "duration": 3.921,
      "language": "en"
    },
    {
      "text": "intelligent way. So, we're going to",
      "start": 314.16,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "start with um architecture variations.",
      "start": 316.56,
      "duration": 6.639,
      "language": "en"
    },
    {
      "text": "Um and the the two things that I'll you",
      "start": 319.68,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "know mention right here and I'll you",
      "start": 323.199,
      "duration": 3.521,
      "language": "en"
    },
    {
      "text": "know go back to them as I talk. The",
      "start": 324.72,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "first one is you know there's not that",
      "start": 326.72,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "much consensus in a lot of the choices.",
      "start": 328.8,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "Um there's been sort of convergent you",
      "start": 330.72,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "know evolution in the last few years. Um",
      "start": 332.56,
      "duration": 3.359,
      "language": "en"
    },
    {
      "text": "what I'll call like llamaike",
      "start": 334.72,
      "duration": 3.039,
      "language": "en"
    },
    {
      "text": "architectures at the very bottom here",
      "start": 335.919,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "but people do all sorts of things. They",
      "start": 337.759,
      "duration": 3.201,
      "language": "en"
    },
    {
      "text": "swap between layer norm and RMS norm.",
      "start": 339.199,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "They do serial versus parallel layers.",
      "start": 340.96,
      "duration": 3.679,
      "language": "en"
    },
    {
      "text": "There's one choice that basically",
      "start": 343.039,
      "duration": 3.921,
      "language": "en"
    },
    {
      "text": "everyone does es since the first very",
      "start": 344.639,
      "duration": 4.081,
      "language": "en"
    },
    {
      "text": "first GPT. Um, and I'll talk about that",
      "start": 346.96,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "in a bit. Um, but there's, you know,",
      "start": 348.72,
      "duration": 4.479,
      "language": "en"
    },
    {
      "text": "lots of different variations that we can",
      "start": 351.28,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "learn from here. The big one I've",
      "start": 353.199,
      "duration": 5.041,
      "language": "en"
    },
    {
      "text": "already talked about this guy in 224N.",
      "start": 356.0,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "So, if you remember that lecture, this",
      "start": 358.24,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "will be review for you, uh, rather than",
      "start": 360.08,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "being totally new. I think the one thing",
      "start": 361.6,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "basically everyone agrees on and agreed",
      "start": 364.08,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "on almost from the very start is the use",
      "start": 366.4,
      "duration": 5.919,
      "language": "en"
    },
    {
      "text": "of pre-norm versus uh, postnorm. Um,",
      "start": 369.28,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "that terminology will get a little bit",
      "start": 372.319,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "more confusing. Um but the original",
      "start": 373.84,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "transformer paper did you know this",
      "start": 375.919,
      "duration": 4.241,
      "language": "en"
    },
    {
      "text": "thing on the left over here where you",
      "start": 378.0,
      "duration": 4.479,
      "language": "en"
    },
    {
      "text": "had your residual stream in the gray um",
      "start": 380.16,
      "duration": 4.479,
      "language": "en"
    },
    {
      "text": "and you know in addition to the residual",
      "start": 382.479,
      "duration": 4.481,
      "language": "en"
    },
    {
      "text": "stream you had these layer norms after",
      "start": 384.639,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "sort of every subcomponent. So you would",
      "start": 386.96,
      "duration": 2.799,
      "language": "en"
    },
    {
      "text": "do your multi head attention you would",
      "start": 388.479,
      "duration": 3.121,
      "language": "en"
    },
    {
      "text": "add back to the residual stream and then",
      "start": 389.759,
      "duration": 3.121,
      "language": "en"
    },
    {
      "text": "you would layer norm that and then you",
      "start": 391.6,
      "duration": 3.039,
      "language": "en"
    },
    {
      "text": "would do the same thing with your uh",
      "start": 392.88,
      "duration": 3.12,
      "language": "en"
    },
    {
      "text": "fully connected layer and then you would",
      "start": 394.639,
      "duration": 5.441,
      "language": "en"
    },
    {
      "text": "layerm it. Um and very very early on um",
      "start": 396.0,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "people realized that moving this layer",
      "start": 400.08,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "norm to the front of this sort of",
      "start": 402.0,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "non-residual part so this block on the",
      "start": 404.08,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "right um did much better in many",
      "start": 406.24,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "different ways and and basically almost",
      "start": 408.96,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "all modern LMS that I know of use this",
      "start": 410.88,
      "duration": 5.759,
      "language": "en"
    },
    {
      "text": "kind of porm um there there have been",
      "start": 414.08,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "some sort of new innovations recently",
      "start": 416.639,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "that I'll touch on in two slides um but",
      "start": 418.16,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "lots of you know models have moved to",
      "start": 421.199,
      "duration": 5.601,
      "language": "en"
    },
    {
      "text": "this the one exception um is opt 350M um",
      "start": 423.44,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "which I'm guessing, you know, they they",
      "start": 426.8,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "kind of messed that one up and and that",
      "start": 428.16,
      "duration": 3.2,
      "language": "en"
    },
    {
      "text": "was sort of orphaned um when they were",
      "start": 429.68,
      "duration": 3.519,
      "language": "en"
    },
    {
      "text": "training. Um that was a fun find in my",
      "start": 431.36,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "in my survey of architectures. Um so",
      "start": 433.199,
      "duration": 5.201,
      "language": "en"
    },
    {
      "text": "this pre versus postnorm thing, if you",
      "start": 436.56,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "look into why it was originally",
      "start": 438.4,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "developed, um the arguments were that,",
      "start": 439.84,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "you know, if you wanted to use this",
      "start": 442.4,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "postnorm stuff, it was much less stable.",
      "start": 444.4,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "And so you would have to do some careful",
      "start": 446.72,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "learning rate warm-up style things to to",
      "start": 448.56,
      "duration": 4.479,
      "language": "en"
    },
    {
      "text": "make it train in a stable way. Um and so",
      "start": 450.88,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "if you look at some of the earlier",
      "start": 453.039,
      "duration": 4.481,
      "language": "en"
    },
    {
      "text": "papers you know arguing for this prenorm",
      "start": 454.4,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "approach um salar and yen and also this",
      "start": 457.52,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "xiang in 2020 paper you almost always",
      "start": 460.24,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "see sort of this comparison of hey if we",
      "start": 462.88,
      "duration": 5.039,
      "language": "en"
    },
    {
      "text": "use p-orm and we do some other stability",
      "start": 465.52,
      "duration": 4.239,
      "language": "en"
    },
    {
      "text": "inducing tricks then we can remove",
      "start": 467.919,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "warm-up and these systems work just as",
      "start": 469.759,
      "duration": 5.201,
      "language": "en"
    },
    {
      "text": "well if not better um than sort of the",
      "start": 471.919,
      "duration": 5.601,
      "language": "en"
    },
    {
      "text": "you know the postnorm layer norm with",
      "start": 474.96,
      "duration": 4.239,
      "language": "en"
    },
    {
      "text": "careful warm-up type approaches and you",
      "start": 477.52,
      "duration": 2.959,
      "language": "en"
    },
    {
      "text": "see this in in sort of a machine",
      "start": 479.199,
      "duration": 3.761,
      "language": "en"
    },
    {
      "text": "translation setting here Um you see this",
      "start": 480.479,
      "duration": 5.521,
      "language": "en"
    },
    {
      "text": "as well uh on the right um on you know",
      "start": 482.96,
      "duration": 4.799,
      "language": "en"
    },
    {
      "text": "various other tasks especially using",
      "start": 486.0,
      "duration": 4.919,
      "language": "en"
    },
    {
      "text": "BERT which was trained with um",
      "start": 487.759,
      "duration": 5.761,
      "language": "en"
    },
    {
      "text": "postnorm. So um there were many",
      "start": 490.919,
      "duration": 4.68,
      "language": "en"
    },
    {
      "text": "arguments about why this was helpful.",
      "start": 493.52,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "There were arguments about gradient",
      "start": 495.599,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "attenuation across layers like if you do",
      "start": 496.96,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "pre-orm then the gradient sizes would",
      "start": 499.039,
      "duration": 3.681,
      "language": "en"
    },
    {
      "text": "remain constant whereas if you did",
      "start": 501.44,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "postnorm um you know without warm-up",
      "start": 502.72,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "then it would sort of blow up in this",
      "start": 505.52,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "orange way. It's a reasonable argument,",
      "start": 507.12,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "but I think a maybe more closer to",
      "start": 509.28,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "modern intuition would be this argument",
      "start": 511.52,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "that um porm is just a more stable",
      "start": 513.2,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "architecture to train. And so some of",
      "start": 515.919,
      "duration": 4.641,
      "language": "en"
    },
    {
      "text": "the earlier work by Solazar and um",
      "start": 517.68,
      "duration": 5.359,
      "language": "en"
    },
    {
      "text": "identified all these loss spikes um that",
      "start": 520.56,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "if you were training with prenorm kind",
      "start": 523.039,
      "duration": 3.681,
      "language": "en"
    },
    {
      "text": "of in blue here um you would see a lot",
      "start": 524.8,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "more loss spikes and the training would",
      "start": 526.72,
      "duration": 4.559,
      "language": "en"
    },
    {
      "text": "be kind of unstable um you know as you",
      "start": 528.48,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "were training. So the you see the the",
      "start": 531.279,
      "duration": 3.441,
      "language": "en"
    },
    {
      "text": "gradient norm here you know is spiking",
      "start": 532.8,
      "duration": 4.039,
      "language": "en"
    },
    {
      "text": "and generally higher than the one with",
      "start": 534.72,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "prenor. And so today um you see prenorm",
      "start": 536.839,
      "duration": 5.641,
      "language": "en"
    },
    {
      "text": "and other layer norm tricks being used",
      "start": 540.48,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "essentially as as stability inducing um",
      "start": 542.48,
      "duration": 6.56,
      "language": "en"
    },
    {
      "text": "stability inducing aids for using large",
      "start": 546.32,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "training large neural networks. Um and",
      "start": 549.04,
      "duration": 6.08,
      "language": "en"
    },
    {
      "text": "so this brings us to one new fairly I",
      "start": 551.6,
      "duration": 5.359,
      "language": "en"
    },
    {
      "text": "think recent innovation. I think this",
      "start": 555.12,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "didn't exist when I gave this lecture",
      "start": 556.959,
      "duration": 5.041,
      "language": "en"
    },
    {
      "text": "last year. um which is this variant that",
      "start": 558.72,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "I don't think really has a has a great",
      "start": 562.0,
      "duration": 2.959,
      "language": "en"
    },
    {
      "text": "name but I'm just going to call it the",
      "start": 563.76,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "double norm for the moment here. So this",
      "start": 564.959,
      "duration": 4.401,
      "language": "en"
    },
    {
      "text": "is the original figure that I showed you",
      "start": 567.36,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "at the very beginning and we know that",
      "start": 569.36,
      "duration": 2.96,
      "language": "en"
    },
    {
      "text": "putting layer norms in the residual",
      "start": 570.8,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "stream is bad. Um but actually someone",
      "start": 572.32,
      "duration": 5.6,
      "language": "en"
    },
    {
      "text": "in 224n this year asked well but why do",
      "start": 574.72,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "you have to put the layer norm in the",
      "start": 577.92,
      "duration": 3.039,
      "language": "en"
    },
    {
      "text": "front? Why can't you put it you know",
      "start": 579.44,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "after the feed forward network? And of",
      "start": 580.959,
      "duration": 4.721,
      "language": "en"
    },
    {
      "text": "course you can and not only that um sort",
      "start": 582.8,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "of recent people have have gone around",
      "start": 585.68,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "and just just add the layer norm after",
      "start": 587.36,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "the you know the blocks as well. And so",
      "start": 589.2,
      "duration": 5.199,
      "language": "en"
    },
    {
      "text": "Grock and GMA 2 both take this approach",
      "start": 591.44,
      "duration": 6.24,
      "language": "en"
    },
    {
      "text": "of layer norms both in front and after.",
      "start": 594.399,
      "duration": 6.801,
      "language": "en"
    },
    {
      "text": "Um 2 does only the the layer norm after",
      "start": 597.68,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "the feed forward um and the multi head",
      "start": 601.2,
      "duration": 3.199,
      "language": "en"
    },
    {
      "text": "attention. And so this is actually kind",
      "start": 603.12,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "of an interesting change. Um porm has",
      "start": 604.399,
      "duration": 4.481,
      "language": "en"
    },
    {
      "text": "just been kind of dominant and the only",
      "start": 606.88,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "thing for a while um but things have",
      "start": 608.88,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "been changed up a little bit. So now now",
      "start": 610.8,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "there's a a new variant and this is",
      "start": 612.48,
      "duration": 3.359,
      "language": "en"
    },
    {
      "text": "actually you know there's been some some",
      "start": 614.32,
      "duration": 3.519,
      "language": "en"
    },
    {
      "text": "evaluations of this kind of approach. Uh",
      "start": 615.839,
      "duration": 3.281,
      "language": "en"
    },
    {
      "text": "people have argued it's a little bit",
      "start": 617.839,
      "duration": 3.841,
      "language": "en"
    },
    {
      "text": "more stable and nicer to train on these",
      "start": 619.12,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "uh larger models. By the way feel free",
      "start": 621.68,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "to um stop me and ask me questions as",
      "start": 624.24,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "well. I have a tendency to to sort of",
      "start": 626.8,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "keep going if if no one stops me. So yes",
      "start": 628.64,
      "duration": 5.759,
      "language": "en"
    },
    {
      "text": "uh why is layer in the residual bad? Why",
      "start": 631.04,
      "duration": 4.799,
      "language": "en"
    },
    {
      "text": "is layer norm and the residual bad?",
      "start": 634.399,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "That's a that's a good question. Um I",
      "start": 635.839,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "don't think I can give you like a you",
      "start": 637.839,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "know this is the proof of why it's bad.",
      "start": 639.519,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "I think one, you know, intuitive",
      "start": 641.44,
      "duration": 3.2,
      "language": "en"
    },
    {
      "text": "argument for why this might be bad is",
      "start": 643.12,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "that the residual gives you this",
      "start": 644.64,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "identity connection all the way from",
      "start": 646.48,
      "duration": 2.96,
      "language": "en"
    },
    {
      "text": "almost the top of the network all the",
      "start": 647.92,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "way all the way to the bottom. And so if",
      "start": 649.44,
      "duration": 2.959,
      "language": "en"
    },
    {
      "text": "you're trying to train really deep",
      "start": 651.2,
      "duration": 2.4,
      "language": "en"
    },
    {
      "text": "networks, this makes gradient",
      "start": 652.399,
      "duration": 3.201,
      "language": "en"
    },
    {
      "text": "propagation very easy, right? So there's",
      "start": 653.6,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "lots of arguments about how you know",
      "start": 655.6,
      "duration": 3.919,
      "language": "en"
    },
    {
      "text": "LSTMs and these other kinds of you know",
      "start": 657.36,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "state space models have difficulty",
      "start": 659.519,
      "duration": 3.201,
      "language": "en"
    },
    {
      "text": "propagating gradients backwards. An",
      "start": 660.959,
      "duration": 3.12,
      "language": "en"
    },
    {
      "text": "identity connection does not have any",
      "start": 662.72,
      "duration": 3.119,
      "language": "en"
    },
    {
      "text": "such problems. And so putting layer",
      "start": 664.079,
      "duration": 3.2,
      "language": "en"
    },
    {
      "text": "norms in the middle, you know, might",
      "start": 665.839,
      "duration": 2.961,
      "language": "en"
    },
    {
      "text": "mess with that kind of gradient sort of",
      "start": 667.279,
      "duration": 3.521,
      "language": "en"
    },
    {
      "text": "behavior. And that you of course you see",
      "start": 668.8,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "uh back here right this is exactly the",
      "start": 670.8,
      "duration": 4.68,
      "language": "en"
    },
    {
      "text": "kind of plot you expect to see if that's",
      "start": 672.56,
      "duration": 6.36,
      "language": "en"
    },
    {
      "text": "happening. Okay cool.",
      "start": 675.48,
      "duration": 7.799,
      "language": "en"
    },
    {
      "text": "Um the other thing that people now do um",
      "start": 678.92,
      "duration": 6.599,
      "language": "en"
    },
    {
      "text": "is in the original transformer people",
      "start": 683.279,
      "duration": 4.481,
      "language": "en"
    },
    {
      "text": "did you know layer norm u and so layer",
      "start": 685.519,
      "duration": 4.721,
      "language": "en"
    },
    {
      "text": "norm is this uh equation over here. What",
      "start": 687.76,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "you do is you have you know the",
      "start": 690.24,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "activations x coming in you subtract the",
      "start": 691.76,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "empirical mean. So that's the average of",
      "start": 694.48,
      "duration": 4.479,
      "language": "en"
    },
    {
      "text": "the x's up top and then you divide by",
      "start": 696.56,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "you know the standard de or the variance",
      "start": 698.959,
      "duration": 4.161,
      "language": "en"
    },
    {
      "text": "plus a little fudge factor epsilon and",
      "start": 700.959,
      "duration": 3.521,
      "language": "en"
    },
    {
      "text": "then you square root that so you can",
      "start": 703.12,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "roughly think of it as a standard",
      "start": 704.48,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "deviation right so that's going to you",
      "start": 706.0,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "know standardize your your activations x",
      "start": 707.92,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "you're going to scale it up by a gamma",
      "start": 710.56,
      "duration": 3.04,
      "language": "en"
    },
    {
      "text": "that's a learnable parameter and then",
      "start": 712.0,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "shift it by a beta right so this makes",
      "start": 713.6,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "sense you you're going to normalize you",
      "start": 715.76,
      "duration": 2.96,
      "language": "en"
    },
    {
      "text": "know your activations and then you're",
      "start": 717.68,
      "duration": 2.24,
      "language": "en"
    },
    {
      "text": "going to shift them around to whatever",
      "start": 718.72,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "point you want and many models use this",
      "start": 719.92,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "layer norm thing and it worked quite",
      "start": 722.72,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "well um but many models have sort of now",
      "start": 724.079,
      "duration": 4.721,
      "language": "en"
    },
    {
      "text": "moved on to RMS norm and this is one of",
      "start": 726.8,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "the consensus changes like basically all",
      "start": 728.8,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "the models have switched to using RMS",
      "start": 730.48,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "norm um and now what do you do you just",
      "start": 732.56,
      "duration": 4.719,
      "language": "en"
    },
    {
      "text": "drop um the mean adjustment so you don't",
      "start": 735.12,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "subtract the mean you don't add a bias",
      "start": 737.279,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "term um and many notable models do this",
      "start": 739.12,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "the llama family palm chinchilla t5",
      "start": 741.279,
      "duration": 5.201,
      "language": "en"
    },
    {
      "text": "they've all moved to to RMS norm um and",
      "start": 743.279,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "what's the reason for this um one reason",
      "start": 746.48,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "is that it doesn't really make a",
      "start": 748.959,
      "duration": 3.281,
      "language": "en"
    },
    {
      "text": "difference turns out if you train models",
      "start": 750.56,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "with RMS norm does just as well as",
      "start": 752.24,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "training you know, layer norm. And so",
      "start": 754.399,
      "duration": 4.801,
      "language": "en"
    },
    {
      "text": "there's a simplification argument. Um,",
      "start": 756.24,
      "duration": 4.719,
      "language": "en"
    },
    {
      "text": "but really I think the argument that's",
      "start": 759.2,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "often given um in these papers and I",
      "start": 760.959,
      "duration": 4.081,
      "language": "en"
    },
    {
      "text": "think it's good to appreciate kind of",
      "start": 763.68,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "the details of this argument is that uh",
      "start": 765.04,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "you going to RMS norm is, you know, it's",
      "start": 768.079,
      "duration": 4.641,
      "language": "en"
    },
    {
      "text": "faster and just as good. So in what way",
      "start": 770.72,
      "duration": 4.239,
      "language": "en"
    },
    {
      "text": "is it faster? Well, if I don't subtract",
      "start": 772.72,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "the mean, it's fewer operations. If I",
      "start": 774.959,
      "duration": 4.161,
      "language": "en"
    },
    {
      "text": "don't have to add that bias term beta",
      "start": 776.88,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "back, it's fewer parameters that I have",
      "start": 779.12,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "to load from memory back into sort of my",
      "start": 781.12,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "compute units, right? So I don't have to",
      "start": 783.2,
      "duration": 3.24,
      "language": "en"
    },
    {
      "text": "you know retrieve these this sort of",
      "start": 784.88,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "state. Um and some of you might be",
      "start": 786.44,
      "duration": 5.32,
      "language": "en"
    },
    {
      "text": "thinking but wait you told me in 224n",
      "start": 789.2,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "that nothing but matrix multiplies",
      "start": 791.76,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "matter for the purpose of runtime right",
      "start": 793.36,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "and this is not a matrix multiply and so",
      "start": 795.04,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "I shouldn't care about you know any of",
      "start": 797.04,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "this and that's a reasonable perspective",
      "start": 798.639,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "to take if you think about you know the",
      "start": 800.56,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "number of the percentage of flops that",
      "start": 802.639,
      "duration": 5.601,
      "language": "en"
    },
    {
      "text": "is taken up by different operations in a",
      "start": 805.6,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "transformer. um this table um there's a",
      "start": 808.24,
      "duration": 6.399,
      "language": "en"
    },
    {
      "text": "nice uh paper by even all in 2023 um I",
      "start": 810.56,
      "duration": 5.6,
      "language": "en"
    },
    {
      "text": "think the title is like memory movement",
      "start": 814.639,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "is all you need or something that does",
      "start": 816.16,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "profiling of all the different",
      "start": 818.24,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "components of a of a transformer and you",
      "start": 819.76,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "see that you know tensor contractions",
      "start": 822.399,
      "duration": 2.961,
      "language": "en"
    },
    {
      "text": "which are like matrix multiplies that's",
      "start": 823.839,
      "duration": 4.481,
      "language": "en"
    },
    {
      "text": "like 99.8% 8% of the flops um that",
      "start": 825.36,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "happen in a transformer. And so, you",
      "start": 828.32,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "know, saving 0.17% of your flops doesn't",
      "start": 830.0,
      "duration": 6.56,
      "language": "en"
    },
    {
      "text": "seem like a like a huge win. Um but I",
      "start": 832.8,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "think one of the things that's important",
      "start": 836.56,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "for architecture design now is to not",
      "start": 838.0,
      "duration": 5.519,
      "language": "en"
    },
    {
      "text": "just think about flops because you know",
      "start": 841.36,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "flops are important, but that's not the",
      "start": 843.519,
      "duration": 2.961,
      "language": "en"
    },
    {
      "text": "only resource that you have to think",
      "start": 845.04,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "about. Um it's also that you have to",
      "start": 846.48,
      "duration": 5.039,
      "language": "en"
    },
    {
      "text": "think carefully about you know memory",
      "start": 849.199,
      "duration": 5.041,
      "language": "en"
    },
    {
      "text": "movement. Um and so even though you know",
      "start": 851.519,
      "duration": 4.641,
      "language": "en"
    },
    {
      "text": "tensor contractions so this is things",
      "start": 854.24,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "like matrix multiplies that's like 99.8%",
      "start": 856.16,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "8% of the flops. You know, if you have",
      "start": 858.399,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "things like the softmax operation or",
      "start": 860.88,
      "duration": 3.519,
      "language": "en"
    },
    {
      "text": "layer norms, all these like",
      "start": 863.279,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "normalization operations that happen um",
      "start": 864.399,
      "duration": 5.921,
      "language": "en"
    },
    {
      "text": "in a transformer, they're 0.17% of the",
      "start": 867.279,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "flops, but actually they're 25% of the",
      "start": 870.32,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "runtime. And a big reason for that is",
      "start": 872.959,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "because you know these normalization",
      "start": 875.36,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "operations still incur a lot of memory",
      "start": 877.279,
      "duration": 4.721,
      "language": "en"
    },
    {
      "text": "movement overhead, right? And so it does",
      "start": 879.44,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "actually matter to try to optimize some",
      "start": 882.0,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "of these like lower level things because",
      "start": 883.92,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "it's not just about flops. It's also",
      "start": 886.399,
      "duration": 3.361,
      "language": "en"
    },
    {
      "text": "about memory movement. I'm going to",
      "start": 888.48,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "emphasize this quite a bit more as I get",
      "start": 889.76,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "into the systems lecture. Like when we",
      "start": 892.0,
      "duration": 3.199,
      "language": "en"
    },
    {
      "text": "talk about GPU architectures, it's going",
      "start": 893.6,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "to become very very very important to",
      "start": 895.199,
      "duration": 4.801,
      "language": "en"
    },
    {
      "text": "think about memory not just about flops.",
      "start": 897.12,
      "duration": 5.839,
      "language": "en"
    },
    {
      "text": "And so this is one of the reasons um why",
      "start": 900.0,
      "duration": 6.959,
      "language": "en"
    },
    {
      "text": "RMS norm has now become sort of um much",
      "start": 902.959,
      "duration": 5.521,
      "language": "en"
    },
    {
      "text": "more popular. And so I I went back and",
      "start": 906.959,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "looked at some of the earlier uh RMS",
      "start": 908.48,
      "duration": 5.359,
      "language": "en"
    },
    {
      "text": "norm papers. I think the the sad thing",
      "start": 910.959,
      "duration": 5.041,
      "language": "en"
    },
    {
      "text": "is that there aren't quite as many",
      "start": 913.839,
      "duration": 4.161,
      "language": "en"
    },
    {
      "text": "papers published by industry labs with",
      "start": 916.0,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "you know big nice ablations. And so many",
      "start": 918.0,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "of the ablations that I'll show you are",
      "start": 920.079,
      "duration": 3.281,
      "language": "en"
    },
    {
      "text": "going to be from from a couple years",
      "start": 921.92,
      "duration": 4.479,
      "language": "en"
    },
    {
      "text": "back. Um but Nang at all in 2020 had",
      "start": 923.36,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "this very nice ablation showing you know",
      "start": 926.399,
      "duration": 3.041,
      "language": "en"
    },
    {
      "text": "here's the vanilla transformer here's",
      "start": 927.92,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "the RMS norm version and you kind of see",
      "start": 929.44,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "the exact thing I told you. you know the",
      "start": 931.68,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "the number of steps per second that you",
      "start": 933.36,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "can do in a vanilla transformer 3.5 per",
      "start": 935.04,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "second with RMS norm you get 3.68 68.",
      "start": 937.12,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "You know, not a huge gain, but that's in",
      "start": 939.6,
      "duration": 4.239,
      "language": "en"
    },
    {
      "text": "some sense for free. And you get, you",
      "start": 941.519,
      "duration": 5.281,
      "language": "en"
    },
    {
      "text": "know, a final loss that's uh lower than",
      "start": 943.839,
      "duration": 4.641,
      "language": "en"
    },
    {
      "text": "the volul transformer. So that's great,",
      "start": 946.8,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "right? In some sense, we've gotten uh",
      "start": 948.48,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "runtime improvements and we've gotten uh",
      "start": 950.399,
      "duration": 4.481,
      "language": "en"
    },
    {
      "text": "in fact, at least in this case, loss",
      "start": 953.04,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "improvements. And so that's a win-win um",
      "start": 954.88,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "for us.",
      "start": 957.6,
      "duration": 4.799,
      "language": "en"
    },
    {
      "text": "The final thing that I'll say which is",
      "start": 960.0,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "very much in line with this RMSORM thing",
      "start": 962.399,
      "duration": 5.281,
      "language": "en"
    },
    {
      "text": "in terms of theme is that most modern",
      "start": 965.12,
      "duration": 5.279,
      "language": "en"
    },
    {
      "text": "transformers do not have bias terms. Um",
      "start": 967.68,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "so the original transformer if you look",
      "start": 970.399,
      "duration": 4.641,
      "language": "en"
    },
    {
      "text": "at the the FFN um will look something",
      "start": 972.24,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "like this right you have your inputs X",
      "start": 975.04,
      "duration": 2.96,
      "language": "en"
    },
    {
      "text": "you're going to do a linear layer with a",
      "start": 976.56,
      "duration": 3.12,
      "language": "en"
    },
    {
      "text": "bias term and then you'll relue it and",
      "start": 978.0,
      "duration": 3.279,
      "language": "en"
    },
    {
      "text": "then you'll have a second linear layer",
      "start": 979.68,
      "duration": 4.839,
      "language": "en"
    },
    {
      "text": "wrapping around it. But um most",
      "start": 981.279,
      "duration": 5.281,
      "language": "en"
    },
    {
      "text": "implementations uh if they're not gated",
      "start": 984.519,
      "duration": 3.24,
      "language": "en"
    },
    {
      "text": "units, which I'll talk about in a",
      "start": 986.56,
      "duration": 2.719,
      "language": "en"
    },
    {
      "text": "moment, uh look actually something like",
      "start": 987.759,
      "duration": 3.161,
      "language": "en"
    },
    {
      "text": "this. They've just dropped the bias",
      "start": 989.279,
      "duration": 3.841,
      "language": "en"
    },
    {
      "text": "terms. You can just make this argument",
      "start": 990.92,
      "duration": 3.64,
      "language": "en"
    },
    {
      "text": "from basically the same kinds of",
      "start": 993.12,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "underlying principles. You know, they",
      "start": 994.56,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "perform just as well. Um matrix",
      "start": 996.56,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "multiplies are apparently um all that",
      "start": 998.56,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "you need to get these guys to work. Um",
      "start": 1000.56,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "and the other thing which is maybe more",
      "start": 1002.72,
      "duration": 4.479,
      "language": "en"
    },
    {
      "text": "subtle is actually optimization",
      "start": 1004.48,
      "duration": 5.359,
      "language": "en"
    },
    {
      "text": "stability. Um I don't quite have the",
      "start": 1007.199,
      "duration": 4.401,
      "language": "en"
    },
    {
      "text": "deepest understanding of why the bias",
      "start": 1009.839,
      "duration": 3.281,
      "language": "en"
    },
    {
      "text": "terms are particularly bad for",
      "start": 1011.6,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "stability. Um but there's been sort of",
      "start": 1013.12,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "really clear empirical observations that",
      "start": 1015.6,
      "duration": 3.679,
      "language": "en"
    },
    {
      "text": "people have made that basically dropping",
      "start": 1017.519,
      "duration": 4.481,
      "language": "en"
    },
    {
      "text": "these bias terms often stabilizes uh the",
      "start": 1019.279,
      "duration": 4.161,
      "language": "en"
    },
    {
      "text": "training of these largest neural",
      "start": 1022.0,
      "duration": 3.199,
      "language": "en"
    },
    {
      "text": "networks. And so now a lot of the",
      "start": 1023.44,
      "duration": 3.359,
      "language": "en"
    },
    {
      "text": "implementations now emit bias terms",
      "start": 1025.199,
      "duration": 3.681,
      "language": "en"
    },
    {
      "text": "entirely um and train only on these like",
      "start": 1026.799,
      "duration": 4.681,
      "language": "en"
    },
    {
      "text": "pure matrix multiply um kind of",
      "start": 1028.88,
      "duration": 5.919,
      "language": "en"
    },
    {
      "text": "settings. So that's the that's the layer",
      "start": 1031.48,
      "duration": 6.359,
      "language": "en"
    },
    {
      "text": "norm bit. Um, and so there's kind of two",
      "start": 1034.799,
      "duration": 4.721,
      "language": "en"
    },
    {
      "text": "things that, you know, you should kind",
      "start": 1037.839,
      "duration": 3.521,
      "language": "en"
    },
    {
      "text": "of think of. This is nice because the",
      "start": 1039.52,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "story is pretty clear. Everyone does",
      "start": 1041.36,
      "duration": 3.04,
      "language": "en"
    },
    {
      "text": "something and so you should just kind of",
      "start": 1043.12,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "know this, right? Basically, everyone",
      "start": 1044.4,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "does porm or at least they do the the",
      "start": 1046.0,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "layer norms outside of the residual",
      "start": 1048.24,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "stream. Like that's kind of the iron",
      "start": 1049.84,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "rule, right? Um, you know, you get nicer",
      "start": 1051.12,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "gradient propagation, you get much more",
      "start": 1053.84,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "stable training. Um, it just doesn't",
      "start": 1055.44,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "make sense to do it the other way. Um,",
      "start": 1057.12,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "most people or most almost everybody uh",
      "start": 1059.28,
      "duration": 6.08,
      "language": "en"
    },
    {
      "text": "does RMS norm. um pra in practice it",
      "start": 1062.4,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "works almost as well has fewer",
      "start": 1065.36,
      "duration": 3.12,
      "language": "en"
    },
    {
      "text": "parameters to move around and this idea",
      "start": 1066.72,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "of dropping bias terms just broadly",
      "start": 1068.48,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "applies a lot of these models just don't",
      "start": 1070.88,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "have bias terms um in most places um I",
      "start": 1072.64,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "think the one exception to this RMS norm",
      "start": 1075.52,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "one as I was reading yesterday uh is I",
      "start": 1077.28,
      "duration": 4.639,
      "language": "en"
    },
    {
      "text": "think coher both command and R plus use",
      "start": 1079.52,
      "duration": 6.88,
      "language": "en"
    },
    {
      "text": "layer norm quite sure why okay any",
      "start": 1081.919,
      "duration": 7.601,
      "language": "en"
    },
    {
      "text": "questions on kind of the layer norm rs",
      "start": 1086.4,
      "duration": 6.08,
      "language": "en"
    },
    {
      "text": "MS norm and bias term stuff before I",
      "start": 1089.52,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "move on yes questions",
      "start": 1092.48,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "Do you think there are some long-term",
      "start": 1094.08,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "lessons you can take away from these",
      "start": 1096.08,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "details that are more future proof",
      "start": 1097.679,
      "duration": 4.561,
      "language": "en"
    },
    {
      "text": "potentially or do you think these are",
      "start": 1099.36,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "Yeah. So the question was is there is",
      "start": 1102.24,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "there something more future proof and I",
      "start": 1103.6,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "think it's hard to have like the the",
      "start": 1105.84,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "biggest picture in in many ways deep",
      "start": 1107.76,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "learning has been very empirical and",
      "start": 1110.48,
      "duration": 3.199,
      "language": "en"
    },
    {
      "text": "like bottom up rather than top down. But",
      "start": 1112.08,
      "duration": 3.2,
      "language": "en"
    },
    {
      "text": "I do think there's some generalizable",
      "start": 1113.679,
      "duration": 3.521,
      "language": "en"
    },
    {
      "text": "lessons that you could sort of draw from",
      "start": 1115.28,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "here. I think the lesson of you know",
      "start": 1117.2,
      "duration": 4.719,
      "language": "en"
    },
    {
      "text": "have very direct identity map residual",
      "start": 1119.2,
      "duration": 4.719,
      "language": "en"
    },
    {
      "text": "connections is sort of a story and a",
      "start": 1121.919,
      "duration": 3.681,
      "language": "en"
    },
    {
      "text": "lesson that has played out in many many",
      "start": 1123.919,
      "duration": 3.201,
      "language": "en"
    },
    {
      "text": "different kinds of architectures not",
      "start": 1125.6,
      "duration": 3.079,
      "language": "en"
    },
    {
      "text": "just you know in these kinds of",
      "start": 1127.12,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "architectures. Um the effectiveness of",
      "start": 1128.679,
      "duration": 4.841,
      "language": "en"
    },
    {
      "text": "layerorm we'll see once again later on",
      "start": 1131.12,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "in this lecture has been very effective",
      "start": 1133.52,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "and so not letting your activations",
      "start": 1135.6,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "drift in sort of scale is another thing",
      "start": 1137.36,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "that I think generally has been very",
      "start": 1139.52,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "effective for training stability. Um",
      "start": 1141.12,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "those two seem like fairly generalizable",
      "start": 1143.76,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "lessons. um we will also kind of see",
      "start": 1146.0,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "sort of sort of the systems concerns",
      "start": 1148.88,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "come into play again. So this is another",
      "start": 1150.48,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "generalizable lesson of sort of thinking",
      "start": 1152.48,
      "duration": 3.439,
      "language": "en"
    },
    {
      "text": "really carefully about the impact of",
      "start": 1154.24,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "your architecture on the systems",
      "start": 1155.919,
      "duration": 4.681,
      "language": "en"
    },
    {
      "text": "components of your of your uh",
      "start": 1157.6,
      "duration": 7.04,
      "language": "en"
    },
    {
      "text": "design. Okay. So now um there's this",
      "start": 1160.6,
      "duration": 6.04,
      "language": "en"
    },
    {
      "text": "other component which is the activations",
      "start": 1164.64,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "um and there is a whole big zoo of",
      "start": 1166.64,
      "duration": 6.56,
      "language": "en"
    },
    {
      "text": "activations um relu swish lu glu and",
      "start": 1168.96,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "then there's I mean these aren't",
      "start": 1173.2,
      "duration": 2.719,
      "language": "en"
    },
    {
      "text": "activations there are different kinds of",
      "start": 1174.32,
      "duration": 7.52,
      "language": "en"
    },
    {
      "text": "mlps uh galu regul swigloo and lilu um",
      "start": 1175.919,
      "duration": 8.481,
      "language": "en"
    },
    {
      "text": "and yeah I think this is exactly the",
      "start": 1181.84,
      "duration": 5.199,
      "language": "en"
    },
    {
      "text": "kind of thing that I didn't originally",
      "start": 1184.4,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "want to learn when I got into doing deep",
      "start": 1187.039,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "learning I was like I don't care about",
      "start": 1189.36,
      "duration": 2.559,
      "language": "en"
    },
    {
      "text": "activations it's going to train anyway",
      "start": 1190.559,
      "duration": 3.761,
      "language": "en"
    },
    {
      "text": "Okay. Um, but it really does matter",
      "start": 1191.919,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "unfortunately um for both you and me",
      "start": 1194.32,
      "duration": 5.359,
      "language": "en"
    },
    {
      "text": "that swiggloo and other glu variants",
      "start": 1196.799,
      "duration": 4.641,
      "language": "en"
    },
    {
      "text": "just consistently work well. And so I",
      "start": 1199.679,
      "duration": 3.681,
      "language": "en"
    },
    {
      "text": "will explain those to you and you should",
      "start": 1201.44,
      "duration": 3.2,
      "language": "en"
    },
    {
      "text": "think about them carefully because they",
      "start": 1203.36,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "do work um and internalize that, right?",
      "start": 1204.64,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "Um so I think the relu and maybe the",
      "start": 1207.44,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "galu you all should already know, right?",
      "start": 1210.48,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "The relu you learn in like some of the",
      "start": 1212.72,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "the most basic deep learning classes,",
      "start": 1214.64,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "right? You just take the max of zero and",
      "start": 1216.4,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "in the case of an MLP, right? You've got",
      "start": 1218.32,
      "duration": 3.359,
      "language": "en"
    },
    {
      "text": "your I've dropped the bias terms here.",
      "start": 1220.0,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "You know, xw1 you take, you know, the",
      "start": 1221.679,
      "duration": 4.961,
      "language": "en"
    },
    {
      "text": "relu and then you do w2. Fairly easy,",
      "start": 1223.84,
      "duration": 6.079,
      "language": "en"
    },
    {
      "text": "right? Uh a gel is a gausian error",
      "start": 1226.64,
      "duration": 6.48,
      "language": "en"
    },
    {
      "text": "linear unit. Um this one multiplies um",
      "start": 1229.919,
      "duration": 6.161,
      "language": "en"
    },
    {
      "text": "the linear with a cdf of a gausian. Um",
      "start": 1233.12,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "and so it's basically going to be like",
      "start": 1236.08,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "the relu but with a little bit of a bump",
      "start": 1238.64,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "here. Hopefully you can see that um over",
      "start": 1241.12,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "here. This is not just flat at the very",
      "start": 1243.44,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "bottom. Um this makes things a little",
      "start": 1245.28,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "bit more differentiable which may or may",
      "start": 1246.96,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "not help. Um and the GPT family of",
      "start": 1249.679,
      "duration": 6.641,
      "language": "en"
    },
    {
      "text": "models um 123 and GPDJ and so on uh all",
      "start": 1252.4,
      "duration": 7.2,
      "language": "en"
    },
    {
      "text": "use the GLU. Um and the original",
      "start": 1256.32,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "transformer and some of the older models",
      "start": 1259.6,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "used uh the relu and really almost all",
      "start": 1261.2,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "the modern models have switched to uh",
      "start": 1264.24,
      "duration": 4.319,
      "language": "en"
    },
    {
      "text": "the gated linear units like swiggloo and",
      "start": 1266.48,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "the geekaloo and and others, right? Um,",
      "start": 1268.559,
      "duration": 4.561,
      "language": "en"
    },
    {
      "text": "and really I think this is, you know,",
      "start": 1271.52,
      "duration": 3.519,
      "language": "en"
    },
    {
      "text": "the the Google folks really pushed for",
      "start": 1273.12,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "this like Palm and P5 and others. Um,",
      "start": 1275.039,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "but since it's sort of been tried and",
      "start": 1278.08,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "true, basically almost all the models",
      "start": 1279.919,
      "duration": 6.521,
      "language": "en"
    },
    {
      "text": "post 2023 um, use a gated linear",
      "start": 1281.76,
      "duration": 7.44,
      "language": "en"
    },
    {
      "text": "unit. And so, you know, going back to",
      "start": 1286.44,
      "duration": 4.359,
      "language": "en"
    },
    {
      "text": "that earlier question of like what",
      "start": 1289.2,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "generalizable architecture things can we",
      "start": 1290.799,
      "duration": 4.161,
      "language": "en"
    },
    {
      "text": "learn from this lecture, you know, there",
      "start": 1292.88,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "are some things that have really",
      "start": 1294.96,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "consistently been very useful. residual",
      "start": 1296.32,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "connections, layer norms. Um, gating is",
      "start": 1298.32,
      "duration": 4.719,
      "language": "en"
    },
    {
      "text": "yet another one, right? And so this is",
      "start": 1301.36,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "another place where gating just appears",
      "start": 1303.039,
      "duration": 4.561,
      "language": "en"
    },
    {
      "text": "and is a very good way of doing things.",
      "start": 1304.96,
      "duration": 5.599,
      "language": "en"
    },
    {
      "text": "So originally we this is our our fully",
      "start": 1307.6,
      "duration": 4.319,
      "language": "en"
    },
    {
      "text": "connected layer right here, right? This",
      "start": 1310.559,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "is with a relu. Now instead of doing",
      "start": 1311.919,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "just linear and a relu, what I'm going",
      "start": 1314.159,
      "duration": 4.961,
      "language": "en"
    },
    {
      "text": "to do is I'm going to gate you know the",
      "start": 1316.799,
      "duration": 5.441,
      "language": "en"
    },
    {
      "text": "output here with a entry-wise linear",
      "start": 1319.12,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "term. So x.v V is going to give me a",
      "start": 1322.24,
      "duration": 4.84,
      "language": "en"
    },
    {
      "text": "vector and I'm going to multiply that",
      "start": 1324.4,
      "duration": 6.08,
      "language": "en"
    },
    {
      "text": "entry-wise with my original inside term",
      "start": 1327.08,
      "duration": 5.64,
      "language": "en"
    },
    {
      "text": "of the MLP and then I'm going to",
      "start": 1330.48,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "multiply the whole thing with W2. Right?",
      "start": 1332.72,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "So the way to think about this is I've",
      "start": 1334.88,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "gated sort of the hidden part of the",
      "start": 1337.12,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "MLP. Right? So I've got my original",
      "start": 1339.44,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "activation that takes my inputs and puts",
      "start": 1341.36,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "it into the sort of hidden space and",
      "start": 1343.44,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "then I'm going to gate that with X.V.",
      "start": 1345.28,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "And then, you know, I'm going to project",
      "start": 1347.76,
      "duration": 3.919,
      "language": "en"
    },
    {
      "text": "that back into sort of the the hidden",
      "start": 1349.44,
      "duration": 4.239,
      "language": "en"
    },
    {
      "text": "dimensionality using W2, right? So,",
      "start": 1351.679,
      "duration": 3.441,
      "language": "en"
    },
    {
      "text": "there's this gating operation that",
      "start": 1353.679,
      "duration": 3.681,
      "language": "en"
    },
    {
      "text": "happens entry-wise. And that's really,",
      "start": 1355.12,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "you know, the the basic thing that's",
      "start": 1357.36,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "happening here. And this is the the GLU",
      "start": 1358.64,
      "duration": 3.64,
      "language": "en"
    },
    {
      "text": "plus the",
      "start": 1361.12,
      "duration": 3.12,
      "language": "en"
    },
    {
      "text": "reloo. And then we have an extra",
      "start": 1362.28,
      "duration": 3.56,
      "language": "en"
    },
    {
      "text": "parameter that we've added here for the",
      "start": 1364.24,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "gating. This is V. Um, and so when",
      "start": 1365.84,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "someone says something like, oh, it's a",
      "start": 1369.2,
      "duration": 5.839,
      "language": "en"
    },
    {
      "text": "a giggloo uh fully, there's nothing to",
      "start": 1371.12,
      "duration": 5.6,
      "language": "en"
    },
    {
      "text": "laugh about that. there's the gigglu",
      "start": 1375.039,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "fully connected layer. Um what I've got",
      "start": 1376.72,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "here is you know I've got the the gel",
      "start": 1379.36,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "sort of for the nonlinearity and I've",
      "start": 1382.0,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "still got the exact same gating here of",
      "start": 1384.08,
      "duration": 3.2,
      "language": "en"
    },
    {
      "text": "x.v V right and this is the the",
      "start": 1385.76,
      "duration": 2.96,
      "language": "en"
    },
    {
      "text": "architecture that was used by many of",
      "start": 1387.28,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "the um uh Google models like T5V1.1",
      "start": 1388.72,
      "duration": 7.839,
      "language": "en"
    },
    {
      "text": "um gamma 2 gamma 3 um and then uh",
      "start": 1392.96,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "another variant there's a swigloo and",
      "start": 1396.559,
      "duration": 4.161,
      "language": "en"
    },
    {
      "text": "this has been very very popular uh swish",
      "start": 1398.64,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "is x times the sigmoid and this is the",
      "start": 1400.72,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "nonlinearity and you can kind of you",
      "start": 1403.2,
      "duration": 3.2,
      "language": "en"
    },
    {
      "text": "know a sigmoid is like this and x is",
      "start": 1404.72,
      "duration": 3.199,
      "language": "en"
    },
    {
      "text": "like this so it will look you know just",
      "start": 1406.4,
      "duration": 4.639,
      "language": "en"
    },
    {
      "text": "like the gausian error unit um and then",
      "start": 1407.919,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "you know you do the same thing here you",
      "start": 1411.039,
      "duration": 2.961,
      "language": "en"
    },
    {
      "text": "have a gating over the switch and then",
      "start": 1412.24,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "you get a fully connected layer here.",
      "start": 1414.0,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "Yes, I have a question. Below a certain",
      "start": 1416.24,
      "duration": 4.319,
      "language": "en"
    },
    {
      "text": "negative value, the switch function and",
      "start": 1418.64,
      "duration": 6.159,
      "language": "en"
    },
    {
      "text": "also the also the the G function it's",
      "start": 1420.559,
      "duration": 5.761,
      "language": "en"
    },
    {
      "text": "not monotonically increasing. In fact,",
      "start": 1424.799,
      "duration": 3.521,
      "language": "en"
    },
    {
      "text": "it's decreasing, right? And a lot of the",
      "start": 1426.32,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "argument about how gradient descent",
      "start": 1428.32,
      "duration": 3.04,
      "language": "en"
    },
    {
      "text": "works in like input machine learning is",
      "start": 1429.76,
      "duration": 3.2,
      "language": "en"
    },
    {
      "text": "that like okay, you want to do gradient",
      "start": 1431.36,
      "duration": 4.559,
      "language": "en"
    },
    {
      "text": "descent click but here it seems like it",
      "start": 1432.96,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "would go in the opposite direction if",
      "start": 1435.919,
      "duration": 4.481,
      "language": "en"
    },
    {
      "text": "you use gh or or switch or their gated",
      "start": 1437.28,
      "duration": 6.399,
      "language": "en"
    },
    {
      "text": "versions. So yeah, so the question was,",
      "start": 1440.4,
      "duration": 5.08,
      "language": "en"
    },
    {
      "text": "you know, this isn't monotonically",
      "start": 1443.679,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "decreasing. You know, there's a there's",
      "start": 1445.48,
      "duration": 3.72,
      "language": "en"
    },
    {
      "text": "a bit on the very left of this zero here",
      "start": 1447.28,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "that's kind of flipping in the in the",
      "start": 1449.2,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "derivative. Um, and isn't that going to",
      "start": 1450.72,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "be a problem? Um, I think intuitively",
      "start": 1453.2,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "you could you could have argued that",
      "start": 1455.6,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "this would be a problem. You might trap",
      "start": 1457.039,
      "duration": 4.481,
      "language": "en"
    },
    {
      "text": "a bunch of activations at zeros. Um, I",
      "start": 1458.48,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "think in practice, you know, if you look",
      "start": 1461.52,
      "duration": 3.2,
      "language": "en"
    },
    {
      "text": "at kind of like neural network",
      "start": 1463.36,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "optimization dynamics, what's actually",
      "start": 1464.72,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "happening is often you're throwing very",
      "start": 1467.12,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "high learning rates with momentum into",
      "start": 1469.36,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "the optimizer. And so you're not really",
      "start": 1471.2,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "going to converge to this zero point,",
      "start": 1473.84,
      "duration": 3.439,
      "language": "en"
    },
    {
      "text": "right? Like these activations are going",
      "start": 1475.76,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "to be all over the place. Um, and so in",
      "start": 1477.279,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "practice, I don't think this this little",
      "start": 1479.76,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "tiny negative piece is really an effect",
      "start": 1481.919,
      "duration": 4.561,
      "language": "en"
    },
    {
      "text": "that's going to be huge for the model,",
      "start": 1484.159,
      "duration": 5.081,
      "language": "en"
    },
    {
      "text": "if that makes sense.",
      "start": 1486.48,
      "duration": 5.6,
      "language": "en"
    },
    {
      "text": "Um, okay. And then going back to to",
      "start": 1489.24,
      "duration": 4.84,
      "language": "en"
    },
    {
      "text": "this, uh, the Swiggloo is is basically",
      "start": 1492.08,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "most models today, like the llama",
      "start": 1494.08,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "family, Palm, Elmo. Um, and I'll show",
      "start": 1496.08,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "you the the big table later. Um, but",
      "start": 1498.32,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "you'll see that the Swigloo is is very",
      "start": 1500.159,
      "duration": 3.921,
      "language": "en"
    },
    {
      "text": "very popular. And one thing to note, um,",
      "start": 1501.919,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "I'll talk about this again in the",
      "start": 1504.08,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "hyperparameters part is, you know, now",
      "start": 1505.279,
      "duration": 4.961,
      "language": "en"
    },
    {
      "text": "remember I've added this this V term,",
      "start": 1508.08,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "this extra parameter, right? And so I",
      "start": 1510.24,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "want to, you know, think about how to",
      "start": 1513.12,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "size this extra parameter. And what",
      "start": 1514.72,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "people do is gated models usually make",
      "start": 1517.12,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "this like hidden size, you know, the",
      "start": 1519.44,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "basically output dimensionality of W",
      "start": 1521.279,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "slightly smaller by a factor of 2/3 um",
      "start": 1523.44,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "in order to make sure that the total",
      "start": 1527.039,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "number of parameters of this whole thing",
      "start": 1528.72,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "remains the same as the non-gated",
      "start": 1531.279,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "counterparts. And that's a convention",
      "start": 1533.12,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "thing that most people do. Um if that",
      "start": 1534.559,
      "duration": 3.761,
      "language": "en"
    },
    {
      "text": "you don't quite understand what that is,",
      "start": 1536.72,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "I'll go back over that again later. But",
      "start": 1538.32,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "you can just kind of keep in mind that",
      "start": 1540.88,
      "duration": 2.799,
      "language": "en"
    },
    {
      "text": "basically for the gated linear units you",
      "start": 1542.159,
      "duration": 2.721,
      "language": "en"
    },
    {
      "text": "just make everything a little bit",
      "start": 1543.679,
      "duration": 3.441,
      "language": "en"
    },
    {
      "text": "smaller to make sure things are remain",
      "start": 1544.88,
      "duration": 4.679,
      "language": "en"
    },
    {
      "text": "parameter",
      "start": 1547.12,
      "duration": 6.799,
      "language": "en"
    },
    {
      "text": "matched. So oh yes question this may be",
      "start": 1549.559,
      "duration": 7.161,
      "language": "en"
    },
    {
      "text": "obvious in the past. Uh, one of the",
      "start": 1553.919,
      "duration": 5.841,
      "language": "en"
    },
    {
      "text": "benefits of relu is like it's very",
      "start": 1556.72,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "easily differentiable by the input. But",
      "start": 1559.76,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "if you know if you have the derivative",
      "start": 1562.48,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "of the cdf of the gausian, you have like",
      "start": 1563.84,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "a squared with x, does that not really",
      "start": 1566.64,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "slow things down? That's a that's a very",
      "start": 1569.36,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "good question. I'm not 100% sure what",
      "start": 1572.32,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "the internal like CUDA implementation of",
      "start": 1574.32,
      "duration": 5.92,
      "language": "en"
    },
    {
      "text": "the swigloo or the the galu gloo is. I",
      "start": 1576.88,
      "duration": 5.039,
      "language": "en"
    },
    {
      "text": "think it's entirely possible that like",
      "start": 1580.24,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "internally they might be implemented",
      "start": 1581.919,
      "duration": 3.12,
      "language": "en"
    },
    {
      "text": "with like lookup tables.",
      "start": 1583.12,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "Go ahead. I mean what really matters is",
      "start": 1585.039,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "the memory pressure here and like it",
      "start": 1586.96,
      "duration": 2.719,
      "language": "en"
    },
    {
      "text": "will be the exact same because you're",
      "start": 1588.559,
      "duration": 2.72,
      "language": "en"
    },
    {
      "text": "reading the same amount of elements",
      "start": 1589.679,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "performance. So the the extra comput is",
      "start": 1591.279,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "negligible on that's actually a yeah",
      "start": 1593.039,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "that's probably a better uh argument",
      "start": 1595.039,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "that like basically flops wise this is",
      "start": 1596.799,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "negligible anyway and that actually the",
      "start": 1598.64,
      "duration": 4.44,
      "language": "en"
    },
    {
      "text": "memory calculus is the same.",
      "start": 1600.4,
      "duration": 5.56,
      "language": "en"
    },
    {
      "text": "So okay",
      "start": 1603.08,
      "duration": 6.199,
      "language": "en"
    },
    {
      "text": "cool. All right so uh do gated linear",
      "start": 1605.96,
      "duration": 5.319,
      "language": "en"
    },
    {
      "text": "units work? Uh I will have more modern",
      "start": 1609.279,
      "duration": 3.681,
      "language": "en"
    },
    {
      "text": "evidence for this as well but I thought",
      "start": 1611.279,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "you know I should take you straight to",
      "start": 1612.96,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "the the horse's mouth uh Nom Shazir's",
      "start": 1614.159,
      "duration": 4.721,
      "language": "en"
    },
    {
      "text": "original paper um where he you know",
      "start": 1616.64,
      "duration": 5.6,
      "language": "en"
    },
    {
      "text": "evaluates all these GLU variants um and",
      "start": 1618.88,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "you know this is this is somewhat older",
      "start": 1622.24,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "stuff. So you're seeing cola and SST2",
      "start": 1623.76,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "performance um but you do see basically",
      "start": 1625.84,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "that the GLU variants consistently",
      "start": 1628.64,
      "duration": 6.12,
      "language": "en"
    },
    {
      "text": "perform better right glu is 84.2 84.12",
      "start": 1631.2,
      "duration": 7.52,
      "language": "en"
    },
    {
      "text": "84.36 84.67 67. Um, and you know, wow,",
      "start": 1634.76,
      "duration": 5.88,
      "language": "en"
    },
    {
      "text": "it's 2020s. They they even give you the",
      "start": 1638.72,
      "duration": 3.439,
      "language": "en"
    },
    {
      "text": "standard deviations so you can sort of",
      "start": 1640.64,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "figure out how significant um those",
      "start": 1642.159,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "results are. And they they in fact um",
      "start": 1644.0,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "are significant, right? Um and so this",
      "start": 1645.76,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "is some nice evidence to to see here. Um",
      "start": 1648.32,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "there was also you know the Nang at all",
      "start": 1651.44,
      "duration": 4.239,
      "language": "en"
    },
    {
      "text": "in 2020 paper which is a very nice paper",
      "start": 1653.36,
      "duration": 3.679,
      "language": "en"
    },
    {
      "text": "studying all sorts of architecture",
      "start": 1655.679,
      "duration": 3.921,
      "language": "en"
    },
    {
      "text": "variance um I think in the context of T5",
      "start": 1657.039,
      "duration": 5.201,
      "language": "en"
    },
    {
      "text": "style models. Um and once again you see",
      "start": 1659.6,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "that the the gated linear unit variants",
      "start": 1662.24,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "um consistently achieve kind of lower",
      "start": 1664.96,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "losses um than their counterparts,",
      "start": 1666.96,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "right? Like you see that the bolded",
      "start": 1669.039,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "lines are exactly at the GLU variance.",
      "start": 1670.24,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "Um and this uh pattern has basically",
      "start": 1673.039,
      "duration": 5.841,
      "language": "en"
    },
    {
      "text": "held up. Um so for gating and",
      "start": 1676.24,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "activations, you know, there are lots of",
      "start": 1678.88,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "lots of variance um across different",
      "start": 1681.12,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "models. Um but the gated linear unit has",
      "start": 1683.039,
      "duration": 4.801,
      "language": "en"
    },
    {
      "text": "become basically widespread and dominant",
      "start": 1685.76,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "and I think for good reason. Um, of",
      "start": 1687.84,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "course, um, the GLU isn't necessary for",
      "start": 1690.159,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "a good model. Like, it's important to",
      "start": 1693.36,
      "duration": 3.12,
      "language": "en"
    },
    {
      "text": "separate the two, right? Just because",
      "start": 1694.799,
      "duration": 3.921,
      "language": "en"
    },
    {
      "text": "it's probably the slightly better and",
      "start": 1696.48,
      "duration": 3.48,
      "language": "en"
    },
    {
      "text": "everyone does it doesn't mean it's",
      "start": 1698.72,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "necessary. Um, and you do see examples",
      "start": 1699.96,
      "duration": 4.52,
      "language": "en"
    },
    {
      "text": "of very high performance models not",
      "start": 1702.64,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "using a GLU. Like GPT3 uh is one",
      "start": 1704.48,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "example. Uh, more recent one, um,",
      "start": 1706.88,
      "duration": 5.519,
      "language": "en"
    },
    {
      "text": "Neotron 340B uses a squared relu, which",
      "start": 1709.44,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "I had not seen before, and Falcon 211B",
      "start": 1712.399,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "uses a RELU. uh both of those are",
      "start": 1715.12,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "relatively high performance models. So",
      "start": 1717.039,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "you can kind of see that it's not really",
      "start": 1718.72,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "necessary and so you know evidence does",
      "start": 1720.64,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "point towards consistent gains from uh",
      "start": 1723.12,
      "duration": 3.679,
      "language": "en"
    },
    {
      "text": "swiggloo and gaggloo and that's why we",
      "start": 1725.039,
      "duration": 4.76,
      "language": "en"
    },
    {
      "text": "ask you to implement exactly uh that",
      "start": 1726.799,
      "duration": 4.921,
      "language": "en"
    },
    {
      "text": "variant.",
      "start": 1729.799,
      "duration": 6.201,
      "language": "en"
    },
    {
      "text": "Cool. Okay. Um the final thing that I",
      "start": 1731.72,
      "duration": 6.04,
      "language": "en"
    },
    {
      "text": "want to talk about for architectures and",
      "start": 1736.0,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "this is one kind of final major I want",
      "start": 1737.76,
      "duration": 5.039,
      "language": "en"
    },
    {
      "text": "to say variation that we've seen. Um",
      "start": 1740.24,
      "duration": 4.799,
      "language": "en"
    },
    {
      "text": "normally uh the transformer block is",
      "start": 1742.799,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "serial right in the sense that you know",
      "start": 1745.039,
      "duration": 5.841,
      "language": "en"
    },
    {
      "text": "uh your your for each block the uh",
      "start": 1747.679,
      "duration": 5.041,
      "language": "en"
    },
    {
      "text": "outputs come in from the bottom and then",
      "start": 1750.88,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "you do your attention and then you pass",
      "start": 1752.72,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "the result of that computation forward",
      "start": 1755.12,
      "duration": 3.679,
      "language": "en"
    },
    {
      "text": "and then you do your MLP and then you",
      "start": 1756.96,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "pass that computation forward, right? Um",
      "start": 1758.799,
      "duration": 3.921,
      "language": "en"
    },
    {
      "text": "and so this is inherently serial. You do",
      "start": 1761.039,
      "duration": 4.161,
      "language": "en"
    },
    {
      "text": "attention and then MLP. But of course",
      "start": 1762.72,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "this might have certain like parallelism",
      "start": 1765.2,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "constraints. So if you want to paralyze",
      "start": 1767.52,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "this over gigantic you know sets of GPUs",
      "start": 1768.96,
      "duration": 5.599,
      "language": "en"
    },
    {
      "text": "it might be harder to do so um if you",
      "start": 1771.919,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "have this serial connection you know the",
      "start": 1774.559,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "systems concerns might also be more",
      "start": 1776.399,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "difficult right you might get lower",
      "start": 1777.919,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "utilization from your GPUs and so a few",
      "start": 1779.279,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "models have done this thing uh that I'll",
      "start": 1782.799,
      "duration": 5.841,
      "language": "en"
    },
    {
      "text": "call parallel layers um where basically",
      "start": 1784.799,
      "duration": 5.841,
      "language": "en"
    },
    {
      "text": "instead of having serial computation of",
      "start": 1788.64,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "attention and then MLP they will do them",
      "start": 1790.64,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "both at the same time right so you will",
      "start": 1792.96,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "get your X you know from your previous",
      "start": 1794.96,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "layer you will comput both the MLP and",
      "start": 1796.799,
      "duration": 4.161,
      "language": "en"
    },
    {
      "text": "the attention side by side and then you",
      "start": 1798.96,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "will add them together into the residual",
      "start": 1800.96,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "stream and then that will be your output",
      "start": 1802.64,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "right um and this was pioneered uh by",
      "start": 1804.48,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "GPTJ which is kind of this open source",
      "start": 1807.039,
      "duration": 5.201,
      "language": "en"
    },
    {
      "text": "replication effort and uh the folks uh",
      "start": 1809.36,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "at Google doing palm were kind of bold",
      "start": 1812.24,
      "duration": 3.679,
      "language": "en"
    },
    {
      "text": "enough to do this at at the really big",
      "start": 1814.08,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "scale um and many others have kind of",
      "start": 1815.919,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "followed since um so if you're",
      "start": 1818.159,
      "duration": 3.921,
      "language": "en"
    },
    {
      "text": "implementing this right you can share a",
      "start": 1819.919,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "lot of stuff like the layer norms and",
      "start": 1822.08,
      "duration": 3.199,
      "language": "en"
    },
    {
      "text": "the matrix multiplies can get fused",
      "start": 1823.679,
      "duration": 3.281,
      "language": "en"
    },
    {
      "text": "together and you can get some systems",
      "start": 1825.279,
      "duration": 2.801,
      "language": "en"
    },
    {
      "text": "efficiency",
      "start": 1826.96,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "um out of that it hasn't been quite as",
      "start": 1828.08,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "popular since then at least in the last",
      "start": 1830.559,
      "duration": 3.041,
      "language": "en"
    },
    {
      "text": "year. I think most of the models that",
      "start": 1832.32,
      "duration": 3.04,
      "language": "en"
    },
    {
      "text": "we've seen have been serial layers",
      "start": 1833.6,
      "duration": 3.439,
      "language": "en"
    },
    {
      "text": "rather than parallel ones. Um I think",
      "start": 1835.36,
      "duration": 3.12,
      "language": "en"
    },
    {
      "text": "the only exceptions to this are like",
      "start": 1837.039,
      "duration": 3.841,
      "language": "en"
    },
    {
      "text": "coher command A, command R plus um and",
      "start": 1838.48,
      "duration": 4.84,
      "language": "en"
    },
    {
      "text": "Falcon Q",
      "start": 1840.88,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "11B. So now I think we have the ability",
      "start": 1843.32,
      "duration": 5.16,
      "language": "en"
    },
    {
      "text": "to kind of go back to you know this big,",
      "start": 1845.84,
      "duration": 4.719,
      "language": "en"
    },
    {
      "text": "you know, hard to see chart and then see",
      "start": 1848.48,
      "duration": 3.679,
      "language": "en"
    },
    {
      "text": "what what I was sort of pointing at at",
      "start": 1850.559,
      "duration": 4.401,
      "language": "en"
    },
    {
      "text": "the very beginning. So this column here,",
      "start": 1852.159,
      "duration": 3.841,
      "language": "en"
    },
    {
      "text": "you know, you don't really need to be",
      "start": 1854.96,
      "duration": 2.8,
      "language": "en"
    },
    {
      "text": "able to read any of the text because",
      "start": 1856.0,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "think the colors will tell you",
      "start": 1857.76,
      "duration": 2.56,
      "language": "en"
    },
    {
      "text": "everything you need to see. This check",
      "start": 1858.88,
      "duration": 3.679,
      "language": "en"
    },
    {
      "text": "mark here, this is basically pre versus",
      "start": 1860.32,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "postnorm. The only two models I I really",
      "start": 1862.559,
      "duration": 4.641,
      "language": "en"
    },
    {
      "text": "know of in the the early days uh that",
      "start": 1864.72,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "did um postnorm, this is the original",
      "start": 1867.2,
      "duration": 4.479,
      "language": "en"
    },
    {
      "text": "transformer and GPT and BERT if you want",
      "start": 1869.6,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "to include that into this table. Um and",
      "start": 1871.679,
      "duration": 3.201,
      "language": "en"
    },
    {
      "text": "then almost everybody else, I think",
      "start": 1873.44,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "basically everyone else um has done uh",
      "start": 1874.88,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "porm. The only other non-checked boxes",
      "start": 1877.6,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "here are models that are proprietary and",
      "start": 1879.52,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "I don't have details for. Um, this",
      "start": 1881.2,
      "duration": 4.479,
      "language": "en"
    },
    {
      "text": "column here on the on the leftmost",
      "start": 1884.0,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "thing, this is RMS norm versus layer",
      "start": 1885.679,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "norm. The gray boxes are the layer norm.",
      "start": 1887.52,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "The blue ones are RMS norm. Basically,",
      "start": 1889.919,
      "duration": 4.081,
      "language": "en"
    },
    {
      "text": "most people have converted to RMS norm.",
      "start": 1892.24,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "As I said, um, this column next to it is",
      "start": 1894.0,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "serial and parallel layers. Once again,",
      "start": 1896.399,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "most people do serial, but you see other",
      "start": 1898.159,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "variants. Um, what I'm going to talk",
      "start": 1900.559,
      "duration": 2.96,
      "language": "en"
    },
    {
      "text": "about next is going to be position",
      "start": 1902.399,
      "duration": 2.801,
      "language": "en"
    },
    {
      "text": "embeddings, and that'll be kind of more",
      "start": 1903.519,
      "duration": 3.361,
      "language": "en"
    },
    {
      "text": "interesting in a moment here. Um any",
      "start": 1905.2,
      "duration": 3.12,
      "language": "en"
    },
    {
      "text": "questions about any of this architecture",
      "start": 1906.88,
      "duration": 3.2,
      "language": "en"
    },
    {
      "text": "stuff before I uh move on? Hopefully",
      "start": 1908.32,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "that gives you a bit of an overview of",
      "start": 1910.08,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "at least the major variations in",
      "start": 1912.0,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "architectures that that we see.",
      "start": 1913.44,
      "duration": 6.32,
      "language": "en"
    },
    {
      "text": "Yes. Serial layer or computation more",
      "start": 1916.96,
      "duration": 5.199,
      "language": "en"
    },
    {
      "text": "efficient than parallel. So uh the",
      "start": 1919.76,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "question was whether serial is more",
      "start": 1922.159,
      "duration": 3.441,
      "language": "en"
    },
    {
      "text": "efficient than parallel. Um it's it",
      "start": 1923.919,
      "duration": 3.441,
      "language": "en"
    },
    {
      "text": "should be the actually the reverse that",
      "start": 1925.6,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "parallel is more efficient than serial",
      "start": 1927.36,
      "duration": 3.679,
      "language": "en"
    },
    {
      "text": "and that's why you're kind of willing to",
      "start": 1929.36,
      "duration": 3.199,
      "language": "en"
    },
    {
      "text": "do this. So in some sense you might",
      "start": 1931.039,
      "duration": 3.841,
      "language": "en"
    },
    {
      "text": "expect serial to be more expressive",
      "start": 1932.559,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "because you're composing two",
      "start": 1934.88,
      "duration": 2.639,
      "language": "en"
    },
    {
      "text": "computations rather than just adding",
      "start": 1936.159,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "them together. Um but the benefit of",
      "start": 1937.519,
      "duration": 4.241,
      "language": "en"
    },
    {
      "text": "parallel in theory is that if you write",
      "start": 1939.519,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "kind of the right kinds of fused",
      "start": 1941.76,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "kernels, a lot of these operations can",
      "start": 1943.279,
      "duration": 3.441,
      "language": "en"
    },
    {
      "text": "be done in parallel or the computation",
      "start": 1945.2,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "is shared across the different um",
      "start": 1946.72,
      "duration": 4.04,
      "language": "en"
    },
    {
      "text": "parallel",
      "start": 1948.88,
      "duration": 6.96,
      "language": "en"
    },
    {
      "text": "parts. Okay. So cool. Um, so the last",
      "start": 1950.76,
      "duration": 6.84,
      "language": "en"
    },
    {
      "text": "thing uh I want to talk about in",
      "start": 1955.84,
      "duration": 3.199,
      "language": "en"
    },
    {
      "text": "architecture land, I think this is the",
      "start": 1957.6,
      "duration": 3.679,
      "language": "en"
    },
    {
      "text": "last thing is uh variations in position",
      "start": 1959.039,
      "duration": 4.401,
      "language": "en"
    },
    {
      "text": "embeddings. Um, and I think this one's",
      "start": 1961.279,
      "duration": 4.481,
      "language": "en"
    },
    {
      "text": "interesting because in the first few",
      "start": 1963.44,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "years of of sort of LM land, there were",
      "start": 1965.76,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "a lot of different things that people",
      "start": 1968.48,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "were trying. Um, sign embeddings were",
      "start": 1970.24,
      "duration": 4.559,
      "language": "en"
    },
    {
      "text": "from the original transformer. You know,",
      "start": 1973.12,
      "duration": 3.439,
      "language": "en"
    },
    {
      "text": "you should have learned this in 224n.",
      "start": 1974.799,
      "duration": 4.081,
      "language": "en"
    },
    {
      "text": "There's sign and cosine positions. Um",
      "start": 1976.559,
      "duration": 4.401,
      "language": "en"
    },
    {
      "text": "many others did absolute embeddings like",
      "start": 1978.88,
      "duration": 4.639,
      "language": "en"
    },
    {
      "text": "the GPTs and OPT all basically just",
      "start": 1980.96,
      "duration": 4.559,
      "language": "en"
    },
    {
      "text": "added a position learned position vector",
      "start": 1983.519,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "to the embedding. Um some others like T5",
      "start": 1985.519,
      "duration": 6.321,
      "language": "en"
    },
    {
      "text": "um and Gopher did uh various kinds of",
      "start": 1989.039,
      "duration": 4.721,
      "language": "en"
    },
    {
      "text": "relative embeddings that add vectors to",
      "start": 1991.84,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "the attention computation and then I",
      "start": 1993.76,
      "duration": 5.759,
      "language": "en"
    },
    {
      "text": "think most models have converged to rope",
      "start": 1996.96,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "um which is you know relative position",
      "start": 1999.519,
      "duration": 4.081,
      "language": "en"
    },
    {
      "text": "embeddings um and this I think actually",
      "start": 2001.2,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "started in GPTJ once again another open-",
      "start": 2003.6,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "source contribution um and has really",
      "start": 2006.0,
      "duration": 4.559,
      "language": "en"
    },
    {
      "text": "rapidly been picked up by most of the",
      "start": 2008.0,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "models um and so the highle thought",
      "start": 2010.559,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "process behind rope is that the thing",
      "start": 2013.12,
      "duration": 5.919,
      "language": "en"
    },
    {
      "text": "that matters is relative positions uh of",
      "start": 2015.279,
      "duration": 7.041,
      "language": "en"
    },
    {
      "text": "these vectors right and so um if I have",
      "start": 2019.039,
      "duration": 6.401,
      "language": "en"
    },
    {
      "text": "an embedding f of x of i where x is you",
      "start": 2022.32,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "know the word I'm trying to embed and i",
      "start": 2025.44,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "is my position then I should be able to",
      "start": 2027.44,
      "duration": 4.239,
      "language": "en"
    },
    {
      "text": "write things down in this way right so",
      "start": 2029.84,
      "duration": 4.319,
      "language": "en"
    },
    {
      "text": "there should exist a f such that f of x",
      "start": 2031.679,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "i and f of yj if I take the inner",
      "start": 2034.159,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "product of these embeddings then I can",
      "start": 2036.559,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "write this down as some different",
      "start": 2039.039,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "function g which is a function of the",
      "start": 2040.399,
      "duration": 4.561,
      "language": "en"
    },
    {
      "text": "two words and the difference in their",
      "start": 2042.399,
      "duration": 4.961,
      "language": "en"
    },
    {
      "text": "positions Right? So this is a a",
      "start": 2044.96,
      "duration": 6.639,
      "language": "en"
    },
    {
      "text": "definition that enforces um basically uh",
      "start": 2047.36,
      "duration": 5.999,
      "language": "en"
    },
    {
      "text": "position invariance or absolute position",
      "start": 2051.599,
      "duration": 3.441,
      "language": "en"
    },
    {
      "text": "invariance. So you only pay attention to",
      "start": 2053.359,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "the how far apart these two words are.",
      "start": 2055.04,
      "duration": 4.639,
      "language": "en"
    },
    {
      "text": "Um and so you can you know do a brief",
      "start": 2057.679,
      "duration": 3.521,
      "language": "en"
    },
    {
      "text": "check and see okay what happens with",
      "start": 2059.679,
      "duration": 3.281,
      "language": "en"
    },
    {
      "text": "signs? Well you get these cross terms",
      "start": 2061.2,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "that that are not relative. So you do",
      "start": 2062.96,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "still leak absolute position",
      "start": 2064.8,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "information. Um absolute positions like",
      "start": 2066.56,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "it's in the name you know it's not a",
      "start": 2068.879,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "relative uh position embedding. And um",
      "start": 2070.399,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "relative embeddings um well it is",
      "start": 2073.2,
      "duration": 4.479,
      "language": "en"
    },
    {
      "text": "relative but it's not an inner product.",
      "start": 2075.599,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "So it sort of violates this constraint.",
      "start": 2077.679,
      "duration": 5.4,
      "language": "en"
    },
    {
      "text": "Um and so rope is this kind of clever",
      "start": 2079.839,
      "duration": 6.161,
      "language": "en"
    },
    {
      "text": "observation that we do know one thing",
      "start": 2083.079,
      "duration": 6.121,
      "language": "en"
    },
    {
      "text": "that is you know invariant to um sort of",
      "start": 2086.0,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "absolute things which is rotations. And",
      "start": 2089.2,
      "duration": 3.679,
      "language": "en"
    },
    {
      "text": "so we're going to exploit that structure",
      "start": 2091.2,
      "duration": 3.679,
      "language": "en"
    },
    {
      "text": "to come up with our position embeddings.",
      "start": 2092.879,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "Um right we know that inner products are",
      "start": 2094.879,
      "duration": 4.881,
      "language": "en"
    },
    {
      "text": "invariant to arbitrary rotation. So",
      "start": 2097.599,
      "duration": 3.841,
      "language": "en"
    },
    {
      "text": "we're going to leverage that. So on the",
      "start": 2099.76,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "left, this is the starting point. Let's",
      "start": 2101.44,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "say my my embedding for the word we is",
      "start": 2103.359,
      "duration": 4.881,
      "language": "en"
    },
    {
      "text": "this arrow over here. And my embedding",
      "start": 2105.76,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "for the word no is this other arrow over",
      "start": 2108.24,
      "duration": 4.879,
      "language": "en"
    },
    {
      "text": "here. Now I want to embed uh this",
      "start": 2110.48,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "sequence. We know that. And I only, you",
      "start": 2113.119,
      "duration": 3.921,
      "language": "en"
    },
    {
      "text": "know, I look at the word we and no. So",
      "start": 2115.2,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "how do I do that? Well, we is in",
      "start": 2117.04,
      "duration": 3.12,
      "language": "en"
    },
    {
      "text": "position zero. So I'm not going to",
      "start": 2118.48,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "rotate that guy at all. Um no is in",
      "start": 2120.16,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "position one. So I'm going to rotate him",
      "start": 2122.88,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "uh by, you know, one, you know, unit of",
      "start": 2125.2,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "rotation. And so now I have this",
      "start": 2127.76,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "embedding for we know. And now let's say",
      "start": 2129.44,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "I want to embed this sequence. Of course",
      "start": 2132.56,
      "duration": 4.799,
      "language": "en"
    },
    {
      "text": "we know. Now we and no are have the same",
      "start": 2134.64,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "relative positioning to each other. And",
      "start": 2137.359,
      "duration": 4.081,
      "language": "en"
    },
    {
      "text": "so let's look at what happens. Wei gets",
      "start": 2139.2,
      "duration": 4.879,
      "language": "en"
    },
    {
      "text": "shifted by two positions. I rotate we by",
      "start": 2141.44,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "you know I start you know in this",
      "start": 2144.079,
      "duration": 3.04,
      "language": "en"
    },
    {
      "text": "vertical position and I rotate them",
      "start": 2145.76,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "twice. One and two. And then I rotate no",
      "start": 2147.119,
      "duration": 6.161,
      "language": "en"
    },
    {
      "text": "by three positions because it's 1 2 3 uh",
      "start": 2150.4,
      "duration": 6.16,
      "language": "en"
    },
    {
      "text": "sorry 0 1 2 3 position. Right? And so",
      "start": 2153.28,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "now if you look at these two arrows,",
      "start": 2156.56,
      "duration": 3.519,
      "language": "en"
    },
    {
      "text": "they have the same relative angle,",
      "start": 2158.64,
      "duration": 2.719,
      "language": "en"
    },
    {
      "text": "right? So their inner products are",
      "start": 2160.079,
      "duration": 2.961,
      "language": "en"
    },
    {
      "text": "preserved. And so this is kind of the",
      "start": 2161.359,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "the nice fun idea about rope. You just",
      "start": 2163.04,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "rotate the vectors and the rotation",
      "start": 2165.68,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "angle is determined by the position of",
      "start": 2168.0,
      "duration": 5.119,
      "language": "en"
    },
    {
      "text": "each word. And rotations, you know, the",
      "start": 2170.64,
      "duration": 5.0,
      "language": "en"
    },
    {
      "text": "inner products don't care about relative",
      "start": 2173.119,
      "duration": 5.361,
      "language": "en"
    },
    {
      "text": "rotations. And so these inner products",
      "start": 2175.64,
      "duration": 4.12,
      "language": "en"
    },
    {
      "text": "are only going to look at sort of the",
      "start": 2178.48,
      "duration": 2.68,
      "language": "en"
    },
    {
      "text": "the difference in",
      "start": 2179.76,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "distance. Now it's easy to think about",
      "start": 2181.16,
      "duration": 4.919,
      "language": "en"
    },
    {
      "text": "in 2D because rotations are kind of",
      "start": 2183.359,
      "duration": 4.641,
      "language": "en"
    },
    {
      "text": "obvious in 2D. There's only one way to",
      "start": 2186.079,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "rotate a vector. Um but in",
      "start": 2188.0,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "highdimensional spaces where we operate,",
      "start": 2189.839,
      "duration": 4.561,
      "language": "en"
    },
    {
      "text": "it's not obvious at all how we are going",
      "start": 2192.079,
      "duration": 4.961,
      "language": "en"
    },
    {
      "text": "to do this rotation. So the rope folks",
      "start": 2194.4,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "came up with you know in some ways the",
      "start": 2197.04,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "simplest but also effective way of doing",
      "start": 2198.8,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "this. And the way to do it is you take",
      "start": 2200.88,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "your highdimensional vector in this case",
      "start": 2203.2,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "D and I'm just going to cut it up into",
      "start": 2204.96,
      "duration": 4.879,
      "language": "en"
    },
    {
      "text": "blocks of two dimensions. And every two",
      "start": 2207.28,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "dimension is going to be rotated by some",
      "start": 2209.839,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "theta. So there's going to be a rotation",
      "start": 2212.4,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "speed. Um and I'm going to rotate the",
      "start": 2214.16,
      "duration": 6.24,
      "language": "en"
    },
    {
      "text": "pairs of dimensions. Um and so now every",
      "start": 2217.04,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "pair of dimensions is encoding, you",
      "start": 2220.4,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "know, all these relative positions. And",
      "start": 2222.24,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "much like in s and cosine embeddings,",
      "start": 2224.32,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "I'm going to pick some set of thetas",
      "start": 2226.4,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "such that some embeddings are rotated",
      "start": 2228.48,
      "duration": 4.639,
      "language": "en"
    },
    {
      "text": "quickly and others are rotated much more",
      "start": 2230.64,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "slowly. So they can capture both high",
      "start": 2233.119,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "frequency information or like close by",
      "start": 2234.8,
      "duration": 5.6,
      "language": "en"
    },
    {
      "text": "information and very far away uh sort of",
      "start": 2237.119,
      "duration": 5.601,
      "language": "en"
    },
    {
      "text": "lower frequency positioning information,",
      "start": 2240.4,
      "duration": 6.32,
      "language": "en"
    },
    {
      "text": "right? Um and the actual rope math here",
      "start": 2242.72,
      "duration": 5.359,
      "language": "en"
    },
    {
      "text": "is, you know, if you're going to think",
      "start": 2246.72,
      "duration": 3.04,
      "language": "en"
    },
    {
      "text": "about rotations, it's just going to be",
      "start": 2248.079,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "multiplying with various sign and cosine",
      "start": 2249.76,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "rotation matrices. Hopefully you",
      "start": 2251.839,
      "duration": 2.641,
      "language": "en"
    },
    {
      "text": "remember this kind of from linear",
      "start": 2253.28,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "algebra and trig. Um and so you can",
      "start": 2254.48,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "think about this as an operation where",
      "start": 2257.28,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "you multiply you know your embedding",
      "start": 2258.88,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "vectors with these you know block 2x two",
      "start": 2260.8,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "block matrices. Um and there's no sort",
      "start": 2264.16,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "of additive or cross terms that sort of",
      "start": 2266.56,
      "duration": 5.72,
      "language": "en"
    },
    {
      "text": "appear here. This is all purely uh",
      "start": 2268.88,
      "duration": 6.88,
      "language": "en"
    },
    {
      "text": "relative. Um one thing that is different",
      "start": 2272.28,
      "duration": 5.559,
      "language": "en"
    },
    {
      "text": "um if you're used to sort of absolute",
      "start": 2275.76,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "position embeddings or sign and cosine",
      "start": 2277.839,
      "duration": 5.041,
      "language": "en"
    },
    {
      "text": "embeddings here is that um the rope is",
      "start": 2279.68,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "going to operate at the actual attention",
      "start": 2282.88,
      "duration": 2.8,
      "language": "en"
    },
    {
      "text": "layer. Right? you're not going to add",
      "start": 2284.56,
      "duration": 2.96,
      "language": "en"
    },
    {
      "text": "position embeddings at the bottom.",
      "start": 2285.68,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "Whenever these attention computations",
      "start": 2287.52,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "are going to be done, you're going to",
      "start": 2289.04,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "intervene on that layer and then that's",
      "start": 2290.4,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "going to give you your position uh",
      "start": 2292.56,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "information. And so, you know, I pulled",
      "start": 2294.56,
      "duration": 2.799,
      "language": "en"
    },
    {
      "text": "this from, I think, the llama",
      "start": 2296.24,
      "duration": 2.8,
      "language": "en"
    },
    {
      "text": "implementation of rope. You know, you've",
      "start": 2297.359,
      "duration": 3.521,
      "language": "en"
    },
    {
      "text": "got the initial normal attention stuff",
      "start": 2299.04,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "at the very top like query keys and",
      "start": 2300.88,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "values. These are, you know, your normal",
      "start": 2302.64,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "linear projections. Um, and then, you",
      "start": 2304.24,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "know, you're going to come up with",
      "start": 2307.2,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "cosine and s angles. These are rotation",
      "start": 2308.48,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "angles telling you how much to rotate",
      "start": 2310.88,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "different blocks of um the uh query and",
      "start": 2313.2,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "key. And then so you take your query and",
      "start": 2316.72,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "your key and you're going to rotate them",
      "start": 2318.96,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "by the cosiness and signs. And now",
      "start": 2320.72,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "you've gotten rotated query and rotated",
      "start": 2322.56,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "key. And that's going to be what's going",
      "start": 2324.64,
      "duration": 3.12,
      "language": "en"
    },
    {
      "text": "to go into the rest of your attention",
      "start": 2326.24,
      "duration": 3.04,
      "language": "en"
    },
    {
      "text": "computation. Right? So you don't do this",
      "start": 2327.76,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "at the bottom, you do it whenever you",
      "start": 2329.28,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "generate your queries and keys.",
      "start": 2331.359,
      "duration": 3.281,
      "language": "en"
    },
    {
      "text": "Hopefully that's that's clear. um that's",
      "start": 2332.72,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "really critical to enforcing um kind of",
      "start": 2334.64,
      "duration": 5.32,
      "language": "en"
    },
    {
      "text": "this uh relative positioning only um",
      "start": 2337.04,
      "duration": 6.84,
      "language": "en"
    },
    {
      "text": "information. Okay,",
      "start": 2339.96,
      "duration": 6.84,
      "language": "en"
    },
    {
      "text": "good. So um one of the things I want to",
      "start": 2343.88,
      "duration": 5.16,
      "language": "en"
    },
    {
      "text": "highlight is that rope is actually one",
      "start": 2346.8,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "of the things that it seems like",
      "start": 2349.04,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "everyone has converged on. I I you know",
      "start": 2350.4,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "went through all 19 of those papers um",
      "start": 2352.4,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "over the weekend and basically all of",
      "start": 2354.8,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "them now use rope um for various",
      "start": 2356.88,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "different reasons. there's you know the",
      "start": 2359.04,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "reason that rope has now many different",
      "start": 2360.8,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "algorithms for extrapolating context",
      "start": 2362.8,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "length and that's an important part of",
      "start": 2364.72,
      "duration": 3.2,
      "language": "en"
    },
    {
      "text": "sort of the modern productionized",
      "start": 2366.16,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "language model um but also it seems to",
      "start": 2367.92,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "be empirically quite effective even at",
      "start": 2370.56,
      "duration": 3.519,
      "language": "en"
    },
    {
      "text": "fairly small scales in small context",
      "start": 2372.24,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "length so it's kind of won out on this",
      "start": 2374.079,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "um uh what's it called position",
      "start": 2376.079,
      "duration": 5.361,
      "language": "en"
    },
    {
      "text": "embedding battle okay um any questions",
      "start": 2378.079,
      "duration": 4.801,
      "language": "en"
    },
    {
      "text": "before I move on to to some of the",
      "start": 2381.44,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "hyperparameter stuff yes is the rate of",
      "start": 2382.88,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "rotation consistent across all these",
      "start": 2384.88,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "models um I don't think they're all the",
      "start": 2386.64,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "same there's some variation in the",
      "start": 2388.88,
      "duration": 4.44,
      "language": "en"
    },
    {
      "text": "thetas.",
      "start": 2390.32,
      "duration": 3.0,
      "language": "en"
    },
    {
      "text": "Oh yes. Are the the thetas like for each",
      "start": 2393.68,
      "duration": 6.32,
      "language": "en"
    },
    {
      "text": "pair um are those hyperparameters or are",
      "start": 2396.8,
      "duration": 5.039,
      "language": "en"
    },
    {
      "text": "they trained? They're not. It's the",
      "start": 2400.0,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "thetas that determine the rotation",
      "start": 2401.839,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "angles. They're not hyperparameters.",
      "start": 2403.68,
      "duration": 4.439,
      "language": "en"
    },
    {
      "text": "Much like in the in the signs and",
      "start": 2405.44,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "cosiness here there's kind of a schedule",
      "start": 2408.119,
      "duration": 4.761,
      "language": "en"
    },
    {
      "text": "to the rotation angles that are",
      "start": 2410.96,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "determined and it's in the same",
      "start": 2412.88,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "intuition in the signs and cosiness. You",
      "start": 2414.24,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "want to cover different frequency ranges",
      "start": 2416.24,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "um in order to get higher or lower uh",
      "start": 2418.24,
      "duration": 5.92,
      "language": "en"
    },
    {
      "text": "frequency um information.",
      "start": 2420.64,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "Yes. Oh, the rotations create any",
      "start": 2424.16,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "difficulty with like training. I wonder",
      "start": 2426.4,
      "duration": 5.199,
      "language": "en"
    },
    {
      "text": "like this like angular rotations. Um the",
      "start": 2428.48,
      "duration": 4.879,
      "language": "en"
    },
    {
      "text": "rotations themselves don't really create",
      "start": 2431.599,
      "duration": 3.201,
      "language": "en"
    },
    {
      "text": "any issues because one way of thinking",
      "start": 2433.359,
      "duration": 2.881,
      "language": "en"
    },
    {
      "text": "about a rotation is that it's just a",
      "start": 2434.8,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "matrix multiply, right? Since thetas are",
      "start": 2436.24,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "fixed, right, and the M's here are",
      "start": 2438.64,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "fixed, this is really just a fixed",
      "start": 2440.48,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "matrix that multiplies your vector. And",
      "start": 2442.32,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "so in that sense it's not really an",
      "start": 2444.48,
      "duration": 2.879,
      "language": "en"
    },
    {
      "text": "issue. If you were learning the thetas",
      "start": 2445.92,
      "duration": 2.56,
      "language": "en"
    },
    {
      "text": "then maybe you have issues because",
      "start": 2447.359,
      "duration": 2.321,
      "language": "en"
    },
    {
      "text": "you're you know maybe differentiating",
      "start": 2448.48,
      "duration": 2.8,
      "language": "en"
    },
    {
      "text": "through trig functions but you're not",
      "start": 2449.68,
      "duration": 3.24,
      "language": "en"
    },
    {
      "text": "doing that here.",
      "start": 2451.28,
      "duration": 4.36,
      "language": "en"
    },
    {
      "text": "So okay",
      "start": 2452.92,
      "duration": 5.64,
      "language": "en"
    },
    {
      "text": "cool. So now I think we go even one more",
      "start": 2455.64,
      "duration": 5.479,
      "language": "en"
    },
    {
      "text": "level uh into the details here. Uh and",
      "start": 2458.56,
      "duration": 3.559,
      "language": "en"
    },
    {
      "text": "we're going to talk about",
      "start": 2461.119,
      "duration": 3.681,
      "language": "en"
    },
    {
      "text": "hyperparameters. Um I feel like when you",
      "start": 2462.119,
      "duration": 4.281,
      "language": "en"
    },
    {
      "text": "have to you know you're dropped in and",
      "start": 2464.8,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "you're asked to train you know a new",
      "start": 2466.4,
      "duration": 3.679,
      "language": "en"
    },
    {
      "text": "language model there's a lot of",
      "start": 2468.48,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "questions you have about hyperparameters",
      "start": 2470.079,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "because there's quite a few of them. And",
      "start": 2471.76,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "one of the things that I've realized is",
      "start": 2473.68,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "that actually only a few of these really",
      "start": 2475.2,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "get changed um across different",
      "start": 2477.04,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "successful models. There's actually like",
      "start": 2479.04,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "fairly clear rules of thumb and fairly",
      "start": 2480.56,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "clear guidelines that people seem to be",
      "start": 2482.56,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "following. Um so you know there are some",
      "start": 2484.4,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "things like how much bigger should the",
      "start": 2487.2,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "feed forward size be or how many heads",
      "start": 2488.48,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "should I have or what should my vocab",
      "start": 2490.8,
      "duration": 4.559,
      "language": "en"
    },
    {
      "text": "size be? Um and so we'll talk about each",
      "start": 2492.56,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "of those things and we'll try to",
      "start": 2495.359,
      "duration": 3.521,
      "language": "en"
    },
    {
      "text": "constrain the space of hyperparameters",
      "start": 2496.56,
      "duration": 6.48,
      "language": "en"
    },
    {
      "text": "that people um have. So you know the",
      "start": 2498.88,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "starting point we're going to look at a",
      "start": 2503.04,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "simple feed forward layer you know just",
      "start": 2504.64,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "the you know with the bias let's say um",
      "start": 2506.56,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "this is a relu version of it and so",
      "start": 2508.96,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "there's two hyperparameters here there's",
      "start": 2511.2,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "d model which is the dimensionality of x",
      "start": 2513.2,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "right that's the input coming into your",
      "start": 2515.52,
      "duration": 4.559,
      "language": "en"
    },
    {
      "text": "your MLP um and then you've got dfff so",
      "start": 2517.04,
      "duration": 4.559,
      "language": "en"
    },
    {
      "text": "this is the feed forward dimension this",
      "start": 2520.079,
      "duration": 3.361,
      "language": "en"
    },
    {
      "text": "is kind of the the output hidden",
      "start": 2521.599,
      "duration": 4.081,
      "language": "en"
    },
    {
      "text": "dimension of your MLP and from there",
      "start": 2523.44,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "you're going to project back onto D",
      "start": 2525.68,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "model right so what should um DFF be um",
      "start": 2527.2,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "in general eneral um you know these",
      "start": 2531.04,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "these things are going to be up",
      "start": 2533.04,
      "duration": 2.799,
      "language": "en"
    },
    {
      "text": "projections right you're going to have",
      "start": 2534.4,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "more hidden units than there were inputs",
      "start": 2535.839,
      "duration": 4.881,
      "language": "en"
    },
    {
      "text": "um but how much bigger well there is",
      "start": 2538.56,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "actually just like a consensus um almost",
      "start": 2540.72,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "everybody that uses you know relu style",
      "start": 2543.52,
      "duration": 5.599,
      "language": "en"
    },
    {
      "text": "uh MLPS are going to pick DFF is equal",
      "start": 2546.24,
      "duration": 6.8,
      "language": "en"
    },
    {
      "text": "to four times um D model um this is I",
      "start": 2549.119,
      "duration": 5.921,
      "language": "en"
    },
    {
      "text": "will show you some empirical evidence",
      "start": 2553.04,
      "duration": 4.559,
      "language": "en"
    },
    {
      "text": "for why this is a sane number later um",
      "start": 2555.04,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "but as far as I can tell there's no like",
      "start": 2557.599,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "you know law of nature that says you",
      "start": 2560.48,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "have to pick four. This is a convention",
      "start": 2562.319,
      "duration": 5.201,
      "language": "en"
    },
    {
      "text": "that has really held up. Now there are a",
      "start": 2564.079,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "few exceptions to this rule. Um remember",
      "start": 2567.52,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "that the GLU variants are going to scale",
      "start": 2570.079,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "this down by a factor of 2/3, right? And",
      "start": 2572.8,
      "duration": 5.279,
      "language": "en"
    },
    {
      "text": "if you scale it down by a factor of 2/3",
      "start": 2575.359,
      "duration": 5.201,
      "language": "en"
    },
    {
      "text": "um you're going to have uh roughly the",
      "start": 2578.079,
      "duration": 4.561,
      "language": "en"
    },
    {
      "text": "same number of parameters. Um, you can",
      "start": 2580.56,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "do a little bit of math and if you scale",
      "start": 2582.64,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "the GLU variance down by a factor of",
      "start": 2584.64,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "2/3, you'll come to the conclusion that",
      "start": 2586.72,
      "duration": 4.639,
      "language": "en"
    },
    {
      "text": "the way to do that is to set DFF equal",
      "start": 2588.8,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "to 8 over3d model, right? That's going",
      "start": 2591.359,
      "duration": 3.681,
      "language": "en"
    },
    {
      "text": "to be the number that you end up at. And",
      "start": 2593.52,
      "duration": 2.799,
      "language": "en"
    },
    {
      "text": "you can sort of convince yourself that",
      "start": 2595.04,
      "duration": 2.48,
      "language": "en"
    },
    {
      "text": "that will give you the same number of",
      "start": 2596.319,
      "duration": 3.04,
      "language": "en"
    },
    {
      "text": "parameters and that's the ratio that you",
      "start": 2597.52,
      "duration": 4.2,
      "language": "en"
    },
    {
      "text": "would get if you started with a ratio of",
      "start": 2599.359,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "four. So if you look at many of the",
      "start": 2601.72,
      "duration": 4.04,
      "language": "en"
    },
    {
      "text": "models, they actually do follow this",
      "start": 2604.079,
      "duration": 4.721,
      "language": "en"
    },
    {
      "text": "rule of thumb. um palm for example uh",
      "start": 2605.76,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "you know are palm mistro and llama are",
      "start": 2608.8,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "slightly larger these are glu models but",
      "start": 2611.28,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "they don't follow this 2.6 rule but if",
      "start": 2613.2,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "you look at for example llama you know",
      "start": 2615.28,
      "duration": 4.559,
      "language": "en"
    },
    {
      "text": "one quen deepseek e and t5 they all",
      "start": 2617.04,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "roughly follow um this like kind of",
      "start": 2619.839,
      "duration": 5.361,
      "language": "en"
    },
    {
      "text": "2.6ish rule um and I can sort of put up",
      "start": 2622.16,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "the uh the big table of lms that I made",
      "start": 2625.2,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "later with hyperparameters many many",
      "start": 2627.52,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "many of them fall into this roughly 2.6",
      "start": 2629.359,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "six range and that's the standard",
      "start": 2631.68,
      "duration": 5.159,
      "language": "en"
    },
    {
      "text": "parameterization of a GLU uh",
      "start": 2633.599,
      "duration": 5.921,
      "language": "en"
    },
    {
      "text": "unit. Um I'll go through one other",
      "start": 2636.839,
      "duration": 5.0,
      "language": "en"
    },
    {
      "text": "exception. I really like this exception",
      "start": 2639.52,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "because I think in many ways, you know,",
      "start": 2641.839,
      "duration": 5.441,
      "language": "en"
    },
    {
      "text": "uh big large language model training is",
      "start": 2644.56,
      "duration": 4.799,
      "language": "en"
    },
    {
      "text": "a game of copying hyperparameters from",
      "start": 2647.28,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "other people and so we don't learn very",
      "start": 2649.359,
      "duration": 2.561,
      "language": "en"
    },
    {
      "text": "much, right? Like it's very",
      "start": 2650.96,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "conservative. Um but T5 I really like",
      "start": 2651.92,
      "duration": 5.679,
      "language": "en"
    },
    {
      "text": "because in some sense it's really bold.",
      "start": 2655.68,
      "duration": 3.439,
      "language": "en"
    },
    {
      "text": "Um and I think Google people actually do",
      "start": 2657.599,
      "duration": 4.401,
      "language": "en"
    },
    {
      "text": "some pretty bold stuff. Um, and so if",
      "start": 2659.119,
      "duration": 5.361,
      "language": "en"
    },
    {
      "text": "you look at the 11 billion parameter T5",
      "start": 2662.0,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "model, they have a pretty pretty",
      "start": 2664.48,
      "duration": 4.92,
      "language": "en"
    },
    {
      "text": "incredible setting. Their hidden dim is",
      "start": 2666.8,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "1024, but their DFF, you know, their up",
      "start": 2669.4,
      "duration": 6.36,
      "language": "en"
    },
    {
      "text": "projected dimension is 65,000, right?",
      "start": 2672.16,
      "duration": 5.919,
      "language": "en"
    },
    {
      "text": "Um, and so that's going to give you a 64",
      "start": 2675.76,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "times multiplier um on the ratio of DFF",
      "start": 2678.079,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "to to uh D model. And of course, you",
      "start": 2681.04,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "know, you compare to this where Palm is",
      "start": 2683.119,
      "duration": 3.521,
      "language": "en"
    },
    {
      "text": "like a factor four and everyone else is,",
      "start": 2685.119,
      "duration": 3.281,
      "language": "en"
    },
    {
      "text": "you know, much smaller. This is this is",
      "start": 2686.64,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "a very large difference. Um and there's",
      "start": 2688.4,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "some other recent examples of of using",
      "start": 2691.04,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "much bigger um you know multipliers like",
      "start": 2692.96,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "gamma 2 kind of follows in these",
      "start": 2695.52,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "footsteps um and does a factor of eight.",
      "start": 2696.96,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "And I'll talk a little bit about this",
      "start": 2699.359,
      "duration": 3.681,
      "language": "en"
    },
    {
      "text": "exception um later. Of course T5 was a",
      "start": 2700.64,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "totally fine model. So this should tell",
      "start": 2703.04,
      "duration": 4.799,
      "language": "en"
    },
    {
      "text": "you it is possible to train a model with",
      "start": 2704.56,
      "duration": 6.799,
      "language": "en"
    },
    {
      "text": "you know such a uh much larger ratio.",
      "start": 2707.839,
      "duration": 5.201,
      "language": "en"
    },
    {
      "text": "So one of the things that I think is you",
      "start": 2711.359,
      "duration": 3.201,
      "language": "en"
    },
    {
      "text": "know quantitative evidence you know I",
      "start": 2713.04,
      "duration": 4.559,
      "language": "en"
    },
    {
      "text": "saw that 4x multiplier and I thought is",
      "start": 2714.56,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "that really the right thing to do or is",
      "start": 2717.599,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "there some more quantitative experiment",
      "start": 2719.28,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "someone's done to convince me that that",
      "start": 2721.2,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "is a good idea. Um so one of the figures",
      "start": 2723.04,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "from Jared Kaplan's sort of scaling law",
      "start": 2726.48,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "paper and most people know this paper",
      "start": 2728.88,
      "duration": 3.439,
      "language": "en"
    },
    {
      "text": "for for the scaling law component but",
      "start": 2730.319,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "actually there's also some really useful",
      "start": 2732.319,
      "duration": 4.081,
      "language": "en"
    },
    {
      "text": "hyperparameter components to this paper.",
      "start": 2734.079,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "um you'll actually see that they do",
      "start": 2736.4,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "exactly this thing that I'm talking",
      "start": 2737.839,
      "duration": 4.561,
      "language": "en"
    },
    {
      "text": "about the DFF to D model ratio. Um and",
      "start": 2739.28,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "they plot essentially how much the loss",
      "start": 2742.4,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "increases as you vary this and um you",
      "start": 2744.48,
      "duration": 5.599,
      "language": "en"
    },
    {
      "text": "kind of see that there's kind of a sweet",
      "start": 2747.92,
      "duration": 5.439,
      "language": "en"
    },
    {
      "text": "spot. This is, you know, a ratio of 1 2",
      "start": 2750.079,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "3 4 and then up to like 10 or so here,",
      "start": 2753.359,
      "duration": 4.561,
      "language": "en"
    },
    {
      "text": "right? And so there's a pretty wide",
      "start": 2755.839,
      "duration": 4.961,
      "language": "en"
    },
    {
      "text": "basin here anywhere between 1 to maybe",
      "start": 2757.92,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "up to 10 where you know you can pick",
      "start": 2760.8,
      "duration": 5.039,
      "language": "en"
    },
    {
      "text": "whatever feed forward ratio you want and",
      "start": 2763.44,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "it'll be roughly optimal. Um, and four",
      "start": 2765.839,
      "duration": 5.361,
      "language": "en"
    },
    {
      "text": "is not too far off from your your",
      "start": 2768.88,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "optimal choices over here. It's like",
      "start": 2771.2,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "one, two, three, four. It's like right",
      "start": 2772.88,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "here or maybe right here, right? So",
      "start": 2775.28,
      "duration": 2.72,
      "language": "en"
    },
    {
      "text": "that's that's a pretty reasonable",
      "start": 2776.8,
      "duration": 4.039,
      "language": "en"
    },
    {
      "text": "choice.",
      "start": 2778.0,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "Um, so what can we learn from all this",
      "start": 2780.839,
      "duration": 4.76,
      "language": "en"
    },
    {
      "text": "hyperparameter stuff? I think a lot of",
      "start": 2783.76,
      "duration": 3.359,
      "language": "en"
    },
    {
      "text": "the evidence points towards, you know,",
      "start": 2785.599,
      "duration": 3.681,
      "language": "en"
    },
    {
      "text": "you can pick the same defaults of, you",
      "start": 2787.119,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "know, if you're not using a GLU, you can",
      "start": 2789.28,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "multiply by four. If you're using a GLU,",
      "start": 2791.119,
      "duration": 5.121,
      "language": "en"
    },
    {
      "text": "you can use roughly 2.66. Um, and they",
      "start": 2793.2,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "can work pretty well for mostly all the",
      "start": 2796.24,
      "duration": 5.839,
      "language": "en"
    },
    {
      "text": "modern LMS. Um, T5 once again does show",
      "start": 2798.64,
      "duration": 4.959,
      "language": "en"
    },
    {
      "text": "that you don't have to follow these",
      "start": 2802.079,
      "duration": 3.04,
      "language": "en"
    },
    {
      "text": "rules, right? You can be a rule breaker",
      "start": 2803.599,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "and do whatever you'd like. um there's",
      "start": 2805.119,
      "duration": 3.521,
      "language": "en"
    },
    {
      "text": "no hyperparameter choice written in",
      "start": 2807.2,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "stone. You can get reasonable LMS at",
      "start": 2808.64,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "many other hyperparameters. Um that",
      "start": 2810.72,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "said, I think the the really funny",
      "start": 2812.8,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "epilogue to this story, right, is that",
      "start": 2814.24,
      "duration": 5.599,
      "language": "en"
    },
    {
      "text": "P5 has a follow-up model called P5V1.1",
      "start": 2816.16,
      "duration": 6.08,
      "language": "en"
    },
    {
      "text": "um that's improved and it uses a much",
      "start": 2819.839,
      "duration": 4.801,
      "language": "en"
    },
    {
      "text": "more standard 2.5 multiplier on gaggloo,",
      "start": 2822.24,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "right? So, you know, you can read",
      "start": 2824.64,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "between the lines and say like maybe",
      "start": 2825.92,
      "duration": 2.96,
      "language": "en"
    },
    {
      "text": "they looked at, you know, the original",
      "start": 2827.52,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "T5 and said actually maybe we want to",
      "start": 2828.88,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "walk back that 64 times multiplier and",
      "start": 2830.88,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "pick a more standard one. and they did",
      "start": 2833.28,
      "duration": 7.4,
      "language": "en"
    },
    {
      "text": "end up with a better model. So cool.",
      "start": 2835.04,
      "duration": 5.64,
      "language": "en"
    },
    {
      "text": "Yeah. Okay. So So I think that's a",
      "start": 2845.76,
      "duration": 2.8,
      "language": "en"
    },
    {
      "text": "that's a good question. So the the",
      "start": 2847.28,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "question was what's the ratio or sorry",
      "start": 2848.56,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "what's the relationship between you know",
      "start": 2851.119,
      "duration": 3.441,
      "language": "en"
    },
    {
      "text": "this ratio that I'm talking about here",
      "start": 2852.56,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "and generally the impact on the model",
      "start": 2854.56,
      "duration": 5.48,
      "language": "en"
    },
    {
      "text": "right? Um and so if we go all the way",
      "start": 2856.64,
      "duration": 5.92,
      "language": "en"
    },
    {
      "text": "back here",
      "start": 2860.04,
      "duration": 4.68,
      "language": "en"
    },
    {
      "text": "uh here, you know, the ratio is",
      "start": 2862.56,
      "duration": 4.559,
      "language": "en"
    },
    {
      "text": "controlling essentially how wide, you",
      "start": 2864.72,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "know, the the hidden part of this this",
      "start": 2867.119,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "MLP is. And so the original",
      "start": 2868.88,
      "duration": 4.479,
      "language": "en"
    },
    {
      "text": "justification in the T5 paper for for",
      "start": 2871.119,
      "duration": 4.881,
      "language": "en"
    },
    {
      "text": "picking 64 was to say actually we can",
      "start": 2873.359,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "get bigger and fatter matrix multiplies",
      "start": 2876.0,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "if we make that dimension really really",
      "start": 2878.319,
      "duration": 3.441,
      "language": "en"
    },
    {
      "text": "large. And while that is a kind of a",
      "start": 2880.0,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "true statement, you know, the wider it",
      "start": 2881.76,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "is, you know, you're getting more",
      "start": 2883.68,
      "duration": 3.919,
      "language": "en"
    },
    {
      "text": "parallel computation, so to speak,",
      "start": 2886.0,
      "duration": 3.04,
      "language": "en"
    },
    {
      "text": "rather than serial computation. So",
      "start": 2887.599,
      "duration": 2.641,
      "language": "en"
    },
    {
      "text": "you're spending your flops and your",
      "start": 2889.04,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "parameters in a slightly different way",
      "start": 2890.24,
      "duration": 3.04,
      "language": "en"
    },
    {
      "text": "than if you made your hidden units",
      "start": 2891.92,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "bigger, which would let you pass more",
      "start": 2893.28,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "information or using more units, which",
      "start": 2894.8,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "would let give you sort of more serial",
      "start": 2896.8,
      "duration": 3.519,
      "language": "en"
    },
    {
      "text": "computation, right? So you're spending",
      "start": 2898.8,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "your parameters and your flops in a in a",
      "start": 2900.319,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "slightly sub-optimal way from expressive",
      "start": 2902.48,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "power, but you might get it um get",
      "start": 2904.319,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "systems gains if sort of your your",
      "start": 2906.559,
      "duration": 4.28,
      "language": "en"
    },
    {
      "text": "matrices are wide",
      "start": 2908.64,
      "duration": 3.959,
      "language": "en"
    },
    {
      "text": "enough.",
      "start": 2910.839,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "Okay.",
      "start": 2912.599,
      "duration": 5.321,
      "language": "en"
    },
    {
      "text": "Excellent. So another thing that is a is",
      "start": 2915.16,
      "duration": 5.159,
      "language": "en"
    },
    {
      "text": "a surprising or maybe not surprising um",
      "start": 2917.92,
      "duration": 6.08,
      "language": "en"
    },
    {
      "text": "consensus hyperparameter is the um ratio",
      "start": 2920.319,
      "duration": 5.841,
      "language": "en"
    },
    {
      "text": "between the model dimension um and the",
      "start": 2924.0,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "head dimension times the number of",
      "start": 2926.16,
      "duration": 3.679,
      "language": "en"
    },
    {
      "text": "heads. Um, so I I clipped this from",
      "start": 2927.599,
      "duration": 5.681,
      "language": "en"
    },
    {
      "text": "224N, right? But really, um, the",
      "start": 2929.839,
      "duration": 6.081,
      "language": "en"
    },
    {
      "text": "basically canonical choice is to pick",
      "start": 2933.28,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "things so that the dimension D, that's a",
      "start": 2935.92,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "hidden dimension. And if you have",
      "start": 2938.4,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "multiple heads, you're just going to",
      "start": 2940.24,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "split up the number of dimensions each",
      "start": 2941.92,
      "duration": 3.439,
      "language": "en"
    },
    {
      "text": "head gets, right? So you're going to",
      "start": 2944.079,
      "duration": 3.841,
      "language": "en"
    },
    {
      "text": "keep the dimensions fixed um, as you add",
      "start": 2945.359,
      "duration": 4.401,
      "language": "en"
    },
    {
      "text": "more heads. And you don't have to do",
      "start": 2947.92,
      "duration": 3.439,
      "language": "en"
    },
    {
      "text": "that, right? As you add more heads, you",
      "start": 2949.76,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "could just keep the same number of",
      "start": 2951.359,
      "duration": 2.881,
      "language": "en"
    },
    {
      "text": "dimensions per head, and you could just",
      "start": 2952.64,
      "duration": 3.439,
      "language": "en"
    },
    {
      "text": "let the attention part take more and",
      "start": 2954.24,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "more parameters, right? you could do",
      "start": 2956.079,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "that. That's an option that you have. Um",
      "start": 2957.76,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "but most models once again do follow",
      "start": 2959.68,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "this guideline. Um we see GPT3, T5,",
      "start": 2962.0,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "Lambda, POM, and llama 2. They all have",
      "start": 2965.2,
      "duration": 5.6,
      "language": "en"
    },
    {
      "text": "a ratio of one or almost exactly one. Um",
      "start": 2967.44,
      "duration": 5.6,
      "language": "en"
    },
    {
      "text": "T5 is the one exception uh that breaks",
      "start": 2970.8,
      "duration": 4.319,
      "language": "en"
    },
    {
      "text": "this rule. They tried the the big ratio",
      "start": 2973.04,
      "duration": 6.16,
      "language": "en"
    },
    {
      "text": "of 16. Um but otherwise it is all, you",
      "start": 2975.119,
      "duration": 6.081,
      "language": "en"
    },
    {
      "text": "know, fairly following this consensus.",
      "start": 2979.2,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "There's been a couple papers that have",
      "start": 2981.2,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "argued against this 1:1 ratio. Um, you",
      "start": 2983.04,
      "duration": 5.039,
      "language": "en"
    },
    {
      "text": "know, there's a a notable one by um I",
      "start": 2985.76,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "don't know how to pronounce this, Boja",
      "start": 2988.079,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "Panelli at all 2020 um who have argued",
      "start": 2989.359,
      "duration": 4.801,
      "language": "en"
    },
    {
      "text": "that, you know, if you have um more and",
      "start": 2991.839,
      "duration": 4.081,
      "language": "en"
    },
    {
      "text": "more heads, they're going to have lower",
      "start": 2994.16,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "and lower rank. Um, and if you have very",
      "start": 2995.92,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "few dimensions per head, that's going to",
      "start": 2998.4,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "start affecting the expressiveness of",
      "start": 3000.16,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "the attention operation. Um, but in",
      "start": 3002.0,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "practice, it doesn't really seem like we",
      "start": 3004.16,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "see too many significant low rank",
      "start": 3006.079,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "bottlenecks in practice. Um, and most of",
      "start": 3008.16,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "the models with this this ratio of one",
      "start": 3010.4,
      "duration": 3.679,
      "language": "en"
    },
    {
      "text": "seem to do just fine, right? This is",
      "start": 3012.48,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "really a parameter that's generally been",
      "start": 3014.079,
      "duration": 4.401,
      "language": "en"
    },
    {
      "text": "held constant by most of the models um",
      "start": 3016.16,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "that we've seen. If I have time, I'll",
      "start": 3018.48,
      "duration": 3.119,
      "language": "en"
    },
    {
      "text": "talk a little bit about different",
      "start": 3020.319,
      "duration": 3.201,
      "language": "en"
    },
    {
      "text": "optimizations that people have made on",
      "start": 3021.599,
      "duration": 4.401,
      "language": "en"
    },
    {
      "text": "this like multi head component. Um but",
      "start": 3023.52,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "hyperparameter rise things have have",
      "start": 3026.0,
      "duration": 3.96,
      "language": "en"
    },
    {
      "text": "stayed fairly um",
      "start": 3027.76,
      "duration": 5.839,
      "language": "en"
    },
    {
      "text": "similar. I think one of the the big ones",
      "start": 3029.96,
      "duration": 5.639,
      "language": "en"
    },
    {
      "text": "in terms of hyperparameters is the the",
      "start": 3033.599,
      "duration": 4.801,
      "language": "en"
    },
    {
      "text": "aspect ratio. Um so you know we can",
      "start": 3035.599,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "think about deep networks, right? We can",
      "start": 3038.4,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "have more and more layers or we can have",
      "start": 3040.079,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "wide networks. And generally if you want",
      "start": 3042.24,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "one knob to control the width that would",
      "start": 3044.319,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "be uh sort of the the hidden dimension",
      "start": 3046.0,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "of the residual street, right? That",
      "start": 3048.319,
      "duration": 2.481,
      "language": "en"
    },
    {
      "text": "would control essentially the width of",
      "start": 3049.68,
      "duration": 3.919,
      "language": "en"
    },
    {
      "text": "almost all the operations um at once.",
      "start": 3050.8,
      "duration": 5.6,
      "language": "en"
    },
    {
      "text": "And so this seems like a pretty critical",
      "start": 3053.599,
      "duration": 4.161,
      "language": "en"
    },
    {
      "text": "thing to tune. You might think that",
      "start": 3056.4,
      "duration": 3.04,
      "language": "en"
    },
    {
      "text": "deeper networks are smarter and more",
      "start": 3057.76,
      "duration": 3.359,
      "language": "en"
    },
    {
      "text": "expressive or wider networks are more",
      "start": 3059.44,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "efficient. Um there is generally a sweet",
      "start": 3061.119,
      "duration": 5.041,
      "language": "en"
    },
    {
      "text": "spot um of ratios that people have",
      "start": 3064.0,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "picked. Um there have been sort of",
      "start": 3066.16,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "outliers. Some of the early models used",
      "start": 3068.48,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "much smaller ratios here. So what that",
      "start": 3070.72,
      "duration": 6.639,
      "language": "en"
    },
    {
      "text": "means is that um they were much uh much",
      "start": 3073.28,
      "duration": 6.319,
      "language": "en"
    },
    {
      "text": "wider than they were deep. And then some",
      "start": 3077.359,
      "duration": 5.121,
      "language": "en"
    },
    {
      "text": "models have gone really deep um where",
      "start": 3079.599,
      "duration": 5.041,
      "language": "en"
    },
    {
      "text": "they had way more sort of D sorry the",
      "start": 3082.48,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "other way around really wide where they",
      "start": 3084.64,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "had way more D model than N layer. Um",
      "start": 3086.24,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "and there's been generally a sweet spot",
      "start": 3088.88,
      "duration": 4.479,
      "language": "en"
    },
    {
      "text": "of saying we want about 128 sort of",
      "start": 3090.72,
      "duration": 5.359,
      "language": "en"
    },
    {
      "text": "hidden dimensions per layer. Um, and",
      "start": 3093.359,
      "duration": 5.521,
      "language": "en"
    },
    {
      "text": "that has been generally stuck to by a",
      "start": 3096.079,
      "duration": 5.121,
      "language": "en"
    },
    {
      "text": "lot of the GPT3 and llama variant",
      "start": 3098.88,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "models. Um, and I'll talk a little bit",
      "start": 3101.2,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "about evidence for that in a second. Um,",
      "start": 3103.2,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "there's considerations about aspect",
      "start": 3107.04,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "ratio that are quite important. Um, they",
      "start": 3108.88,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "will control the amount of sort of",
      "start": 3111.44,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "parallelism um that we can do. So, if",
      "start": 3112.96,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "you're doing um something called uh",
      "start": 3115.68,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "pipeline parallel, what you're often",
      "start": 3118.16,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "going to do is you're going to take your",
      "start": 3120.24,
      "duration": 2.8,
      "language": "en"
    },
    {
      "text": "different layers and you're going to cut",
      "start": 3121.44,
      "duration": 3.04,
      "language": "en"
    },
    {
      "text": "them up and you're going to put them on",
      "start": 3123.04,
      "duration": 3.2,
      "language": "en"
    },
    {
      "text": "different devices or different blocks of",
      "start": 3124.48,
      "duration": 3.2,
      "language": "en"
    },
    {
      "text": "devices because you'll parallelize, you",
      "start": 3126.24,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "know, within each layer as well. Um and",
      "start": 3127.68,
      "duration": 5.679,
      "language": "en"
    },
    {
      "text": "so there's going to be certain kinds of",
      "start": 3130.079,
      "duration": 4.881,
      "language": "en"
    },
    {
      "text": "um constraints that you're going to put",
      "start": 3133.359,
      "duration": 3.681,
      "language": "en"
    },
    {
      "text": "on your model. And also, you know, if",
      "start": 3134.96,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "you have really wide models, then you",
      "start": 3137.04,
      "duration": 3.2,
      "language": "en"
    },
    {
      "text": "can do something called tensor parallel",
      "start": 3138.559,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "where you slice up the matrices and then",
      "start": 3140.24,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "you distribute those on GPUs. And one",
      "start": 3142.319,
      "duration": 4.081,
      "language": "en"
    },
    {
      "text": "thing that we'll learn in in I think uh",
      "start": 3144.48,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "1 2 3 four or five lectures is that",
      "start": 3146.4,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "these different parallelism paradigms",
      "start": 3149.2,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "are going to have different constraints",
      "start": 3151.28,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "right you need really fast networking uh",
      "start": 3152.88,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "for tensor parallel and you can sort of",
      "start": 3155.359,
      "duration": 4.641,
      "language": "en"
    },
    {
      "text": "maybe get away with slower networking or",
      "start": 3157.28,
      "duration": 4.559,
      "language": "en"
    },
    {
      "text": "higher latency networking for pipeline",
      "start": 3160.0,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "parallel and so your networking",
      "start": 3161.839,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "constraints might in turn drive some of",
      "start": 3163.839,
      "duration": 5.121,
      "language": "en"
    },
    {
      "text": "these like width depth considerations.",
      "start": 3166.559,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "But setting that aside, you might",
      "start": 3168.96,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "abstractly ask, you know, what is the",
      "start": 3170.319,
      "duration": 4.401,
      "language": "en"
    },
    {
      "text": "impact of aspect ratio model",
      "start": 3172.96,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "performance? And once again, uh, Kaplan,",
      "start": 3174.72,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "uh, at all have a really nice visual",
      "start": 3177.44,
      "duration": 5.119,
      "language": "en"
    },
    {
      "text": "sort of aid showing how aspect ratio",
      "start": 3179.92,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "impacts performance. And so this is",
      "start": 3182.559,
      "duration": 4.081,
      "language": "en"
    },
    {
      "text": "three different scales, 50 million, uh,",
      "start": 3184.319,
      "duration": 5.601,
      "language": "en"
    },
    {
      "text": "274 million and 1.5 billion parameters.",
      "start": 3186.64,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "And the x-axis is axis effect ratio,",
      "start": 3189.92,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "y-axis is sort of loss difference, um,",
      "start": 3192.48,
      "duration": 5.359,
      "language": "en"
    },
    {
      "text": "in percentage change. And you see that",
      "start": 3194.96,
      "duration": 5.119,
      "language": "en"
    },
    {
      "text": "you know around 100 right which is once",
      "start": 3197.839,
      "duration": 3.841,
      "language": "en"
    },
    {
      "text": "again I told you was the around the",
      "start": 3200.079,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "consensus choice of hyperparameters is",
      "start": 3201.68,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "the minimum across different scales",
      "start": 3203.68,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "right so this is kind of backed by some",
      "start": 3205.44,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "of this like uh large scale uh",
      "start": 3207.52,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "hyperparameter data that's been",
      "start": 3209.76,
      "duration": 3.2,
      "language": "en"
    },
    {
      "text": "published by Kaplan at all and it",
      "start": 3211.52,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "roughly matches that intuition and a",
      "start": 3212.96,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "really nice thing here is it seems to be",
      "start": 3214.8,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "the case that aspect ratio optima does",
      "start": 3216.64,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "not shift too much uh across uh several",
      "start": 3219.28,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "orders of magnitude here so if this",
      "start": 3222.4,
      "duration": 3.199,
      "language": "en"
    },
    {
      "text": "holds up even more that's that's very",
      "start": 3224.0,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "good news you can keep training um on uh",
      "start": 3225.599,
      "duration": 6.081,
      "language": "en"
    },
    {
      "text": "one fixed aspect ratio. Um one thing I",
      "start": 3228.16,
      "duration": 5.679,
      "language": "en"
    },
    {
      "text": "will note um that is quite an",
      "start": 3231.68,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "interesting result is um EK um and",
      "start": 3233.839,
      "duration": 4.801,
      "language": "en"
    },
    {
      "text": "others at Google had this very",
      "start": 3236.64,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "interesting paper sort of studying",
      "start": 3238.64,
      "duration": 4.719,
      "language": "en"
    },
    {
      "text": "impact of depth versus width um both",
      "start": 3240.48,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "upstream and downstream. And one of the",
      "start": 3243.359,
      "duration": 4.081,
      "language": "en"
    },
    {
      "text": "things that they found was that if",
      "start": 3245.28,
      "duration": 5.279,
      "language": "en"
    },
    {
      "text": "you're looking at um losses then it",
      "start": 3247.44,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "doesn't really matter. Parameter is the",
      "start": 3250.559,
      "duration": 3.441,
      "language": "en"
    },
    {
      "text": "only thing that matters. Deeper models",
      "start": 3252.4,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "don't help you. Um, but the story is",
      "start": 3254.0,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "less clear if you're looking at",
      "start": 3256.48,
      "duration": 3.04,
      "language": "en"
    },
    {
      "text": "downstream accuracy. At the time, they",
      "start": 3257.92,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "were looking at sort of fine-tuned",
      "start": 3259.52,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "superlue accuracy. They were arguing",
      "start": 3260.8,
      "duration": 3.519,
      "language": "en"
    },
    {
      "text": "that for the same amount of flops,",
      "start": 3262.8,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "deeper models might be better. So, I'll",
      "start": 3264.319,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "sort of just leave it at that. There's",
      "start": 3266.48,
      "duration": 3.359,
      "language": "en"
    },
    {
      "text": "not quite as much follow-up um to this",
      "start": 3267.92,
      "duration": 3.199,
      "language": "en"
    },
    {
      "text": "work, at least in the open, that I've",
      "start": 3269.839,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "seen, but downstream performance may",
      "start": 3271.119,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "actually be slightly different in terms",
      "start": 3273.119,
      "duration": 5.24,
      "language": "en"
    },
    {
      "text": "of the the aspect ratio considerations",
      "start": 3274.72,
      "duration": 6.44,
      "language": "en"
    },
    {
      "text": "here.",
      "start": 3278.359,
      "duration": 7.401,
      "language": "en"
    },
    {
      "text": "Okay, cool. Um the final thing um that I",
      "start": 3281.16,
      "duration": 6.12,
      "language": "en"
    },
    {
      "text": "want to talk about in this sort of very",
      "start": 3285.76,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "low-level hyperparameter world is what",
      "start": 3287.28,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "are kind of the vocabulary sizes that",
      "start": 3289.52,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "you might want to pick. Um and in",
      "start": 3291.44,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "general vocabulary sizes have been",
      "start": 3293.92,
      "duration": 4.639,
      "language": "en"
    },
    {
      "text": "trending upwards. Um and I think a big",
      "start": 3295.76,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "part of why is because you know LLMs are",
      "start": 3298.559,
      "duration": 4.481,
      "language": "en"
    },
    {
      "text": "being deployed out in the wild. They're",
      "start": 3301.44,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "becoming more useful services. And when",
      "start": 3303.04,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "that happens, you're going to interact",
      "start": 3305.04,
      "duration": 2.48,
      "language": "en"
    },
    {
      "text": "with people speaking different",
      "start": 3306.4,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "languages, um people using emojis, all",
      "start": 3307.52,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "sorts of other kinds of, you know,",
      "start": 3309.76,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "almost modalities or languages um than",
      "start": 3311.599,
      "duration": 5.441,
      "language": "en"
    },
    {
      "text": "what you might expect. And so I think",
      "start": 3314.24,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "some of the earlier models and",
      "start": 3317.04,
      "duration": 4.559,
      "language": "en"
    },
    {
      "text": "especially monolingual models um ranged",
      "start": 3318.88,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "around in the 30 to 50,000 uh token",
      "start": 3321.599,
      "duration": 5.041,
      "language": "en"
    },
    {
      "text": "vocabulary range. Um you can kind of see",
      "start": 3324.64,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "this in like GPTs, the early llamas. Um,",
      "start": 3326.64,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "but if you look at uh the multilingual",
      "start": 3329.44,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "or I would call like production systems",
      "start": 3331.44,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "um that have come out, they've all sort",
      "start": 3333.839,
      "duration": 3.681,
      "language": "en"
    },
    {
      "text": "of been shifting towards the 100 to",
      "start": 3335.68,
      "duration": 3.439,
      "language": "en"
    },
    {
      "text": "250,000",
      "start": 3337.52,
      "duration": 4.559,
      "language": "en"
    },
    {
      "text": "uh range for their vocabulary sizes. Um,",
      "start": 3339.119,
      "duration": 5.041,
      "language": "en"
    },
    {
      "text": "and you know, I I looked at Command A,",
      "start": 3342.079,
      "duration": 3.921,
      "language": "en"
    },
    {
      "text": "which is one of Coher's models. They're",
      "start": 3344.16,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "a company that emphasizes a lot of",
      "start": 3346.0,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "multilingual stuff. You know, you see",
      "start": 3347.44,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "very large uh vocab sizes from them. um",
      "start": 3349.44,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "even with GPT4 and and many others that",
      "start": 3352.48,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "have copied the GPD4 tokenizer are going",
      "start": 3354.96,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "to be around the 100k tokens, right? And",
      "start": 3357.2,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "so that's kind of the the the standard",
      "start": 3359.44,
      "duration": 3.919,
      "language": "en"
    },
    {
      "text": "that a lot of people are operating at",
      "start": 3361.76,
      "duration": 5.359,
      "language": "en"
    },
    {
      "text": "roughly at 100k to to 200k uh token",
      "start": 3363.359,
      "duration": 5.601,
      "language": "en"
    },
    {
      "text": "size. And I think there's been work",
      "start": 3367.119,
      "duration": 3.921,
      "language": "en"
    },
    {
      "text": "showing that as models get bigger um",
      "start": 3368.96,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "these models can in some sense handle",
      "start": 3371.04,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "more and more or make good use of more",
      "start": 3372.72,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "and more um uh vocab elements. And so",
      "start": 3374.48,
      "duration": 4.879,
      "language": "en"
    },
    {
      "text": "you might see, you know, increasing",
      "start": 3378.0,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "trends uh to token counts as models get",
      "start": 3379.359,
      "duration": 4.561,
      "language": "en"
    },
    {
      "text": "scaled up or more more data is used to",
      "start": 3382.0,
      "duration": 3.4,
      "language": "en"
    },
    {
      "text": "train",
      "start": 3383.92,
      "duration": 7.76,
      "language": "en"
    },
    {
      "text": "them. Cool. Okay. So the last thing um",
      "start": 3385.4,
      "duration": 8.64,
      "language": "en"
    },
    {
      "text": "this is no longer sort of specific",
      "start": 3391.68,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "hyperparameters but sort of two other",
      "start": 3394.04,
      "duration": 3.799,
      "language": "en"
    },
    {
      "text": "things that you might need to do before",
      "start": 3396.4,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "you sort of set your model to run. Um",
      "start": 3397.839,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "which is dropout and other kinds of",
      "start": 3400.24,
      "duration": 4.319,
      "language": "en"
    },
    {
      "text": "regularization, right? And I think this",
      "start": 3402.16,
      "duration": 3.919,
      "language": "en"
    },
    {
      "text": "one was really interesting to me when I",
      "start": 3404.559,
      "duration": 2.8,
      "language": "en"
    },
    {
      "text": "was originally doing kind of the",
      "start": 3406.079,
      "duration": 2.72,
      "language": "en"
    },
    {
      "text": "research for putting this lecture",
      "start": 3407.359,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "together. And if you sort of think about",
      "start": 3408.799,
      "duration": 4.161,
      "language": "en"
    },
    {
      "text": "pre-training, pre-training is about the",
      "start": 3410.96,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "furthest place that you might think of",
      "start": 3412.96,
      "duration": 4.639,
      "language": "en"
    },
    {
      "text": "from regularization, right? Because",
      "start": 3414.72,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "pre-training you do usually like one",
      "start": 3417.599,
      "duration": 3.361,
      "language": "en"
    },
    {
      "text": "epoch, right? You you can't even go",
      "start": 3419.44,
      "duration": 2.56,
      "language": "en"
    },
    {
      "text": "through all of your data because you",
      "start": 3420.96,
      "duration": 2.399,
      "language": "en"
    },
    {
      "text": "have too much of it. So you're going to",
      "start": 3422.0,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "do one epoch training um and you're",
      "start": 3423.359,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "almost certainly not overfitting the",
      "start": 3426.0,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "data in that one pass that you're doing,",
      "start": 3427.599,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "right? And so you might think, all",
      "start": 3429.68,
      "duration": 2.96,
      "language": "en"
    },
    {
      "text": "right, we don't need regularization for",
      "start": 3431.2,
      "duration": 3.04,
      "language": "en"
    },
    {
      "text": "pre-training, right? Let's just set your",
      "start": 3432.64,
      "duration": 3.199,
      "language": "en"
    },
    {
      "text": "your optimizer loose. It's all about",
      "start": 3434.24,
      "duration": 5.839,
      "language": "en"
    },
    {
      "text": "minimizing loss. Um, and this is this is",
      "start": 3435.839,
      "duration": 6.641,
      "language": "en"
    },
    {
      "text": "really good arguments for for why you",
      "start": 3440.079,
      "duration": 4.561,
      "language": "en"
    },
    {
      "text": "shouldn't need to to regularize. But",
      "start": 3442.48,
      "duration": 5.359,
      "language": "en"
    },
    {
      "text": "then if you look at what people do, um,",
      "start": 3444.64,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "the the story is actually kind of mixed.",
      "start": 3447.839,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "Um, and this story actually is is maybe",
      "start": 3450.64,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "even more mixed than than what has uh",
      "start": 3453.359,
      "duration": 3.921,
      "language": "en"
    },
    {
      "text": "turned out to be, but you know, early",
      "start": 3455.92,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "days people did a lot of dropout. Um and",
      "start": 3457.28,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "then you know there's a lot of weight",
      "start": 3460.319,
      "duration": 4.401,
      "language": "en"
    },
    {
      "text": "decay that also seems to be happening.",
      "start": 3462.0,
      "duration": 4.799,
      "language": "en"
    },
    {
      "text": "Um and these days I think a lot of the",
      "start": 3464.72,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "the people have stopped publishing",
      "start": 3466.799,
      "duration": 3.401,
      "language": "en"
    },
    {
      "text": "details on precisely their training",
      "start": 3468.24,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "hyperparameters but dropout has sort of",
      "start": 3470.2,
      "duration": 5.159,
      "language": "en"
    },
    {
      "text": "gone out of fashion but weight decay has",
      "start": 3472.96,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "really been something that a lot of",
      "start": 3475.359,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "people continue to do. Um and why is",
      "start": 3477.119,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "that? That's like a a really odd thing",
      "start": 3480.079,
      "duration": 3.681,
      "language": "en"
    },
    {
      "text": "to be doing. Right? So I'll give you a",
      "start": 3482.079,
      "duration": 3.121,
      "language": "en"
    },
    {
      "text": "moment to just kind of think about this",
      "start": 3483.76,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "state of affairs. Right? If you're",
      "start": 3485.2,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "doing, you know, training a really large",
      "start": 3486.64,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "neural network for one pass on SGD on",
      "start": 3488.72,
      "duration": 5.359,
      "language": "en"
    },
    {
      "text": "vast amounts of data, why would you use",
      "start": 3491.52,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "weight decay when you're doing that,",
      "start": 3494.079,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "right? So maybe some of you know the",
      "start": 3496.4,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "answer, but I think that's a kind of",
      "start": 3498.559,
      "duration": 2.921,
      "language": "en"
    },
    {
      "text": "interesting thing to think about. It's",
      "start": 3500.16,
      "duration": 5.439,
      "language": "en"
    },
    {
      "text": "very intuition sort of violating uh at",
      "start": 3501.48,
      "duration": 6.16,
      "language": "en"
    },
    {
      "text": "least uh for me.",
      "start": 3505.599,
      "duration": 6.161,
      "language": "en"
    },
    {
      "text": "So okay so the reason is because um you",
      "start": 3507.64,
      "duration": 6.36,
      "language": "en"
    },
    {
      "text": "know it's not to control overfitting in",
      "start": 3511.76,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "the sense that if you look at weight",
      "start": 3514.0,
      "duration": 4.319,
      "language": "en"
    },
    {
      "text": "decay different amounts of weight decay",
      "start": 3516.0,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "don't really seem to change the ratio of",
      "start": 3518.319,
      "duration": 4.881,
      "language": "en"
    },
    {
      "text": "of training loss to to validation loss",
      "start": 3520.72,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "right so you can train with different",
      "start": 3523.2,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "amounts of weight decay if you train for",
      "start": 3524.4,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "long enough where you know you control",
      "start": 3526.48,
      "duration": 2.8,
      "language": "en"
    },
    {
      "text": "your hyperparameters appropriately",
      "start": 3527.68,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "you'll end up with the same train to val",
      "start": 3529.28,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "loss gap so overfitting nothing's",
      "start": 3531.44,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "happening here even with zero weight",
      "start": 3534.0,
      "duration": 4.319,
      "language": "en"
    },
    {
      "text": "decay but what is interesting is that",
      "start": 3535.28,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "the weight decay seems to be interacting",
      "start": 3538.319,
      "duration": 5.121,
      "language": "en"
    },
    {
      "text": "you know somewhat in a strange way with",
      "start": 3540.72,
      "duration": 4.76,
      "language": "en"
    },
    {
      "text": "the learning rate schedules of the",
      "start": 3543.44,
      "duration": 5.119,
      "language": "en"
    },
    {
      "text": "optimizers. Um and so what's happening",
      "start": 3545.48,
      "duration": 6.2,
      "language": "en"
    },
    {
      "text": "is that if you look at um sort of a a",
      "start": 3548.559,
      "duration": 5.681,
      "language": "en"
    },
    {
      "text": "constant uh learning rate so this is a a",
      "start": 3551.68,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "model trained on constant learning rate",
      "start": 3554.24,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "and then you know you suddenly decrease",
      "start": 3556.079,
      "duration": 3.921,
      "language": "en"
    },
    {
      "text": "the learning rate in 10 year zero. So",
      "start": 3558.4,
      "duration": 3.199,
      "language": "en"
    },
    {
      "text": "you see this drop off as you you know",
      "start": 3560.0,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "decrease the learning rate. Um and then",
      "start": 3561.599,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "let's look at um different kinds of",
      "start": 3564.16,
      "duration": 4.639,
      "language": "en"
    },
    {
      "text": "weight decay that you could do. And what",
      "start": 3566.799,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "happens is, you know, with weight decay,",
      "start": 3568.799,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "the model's not training very well at",
      "start": 3570.4,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "this high learning rate. And then when",
      "start": 3572.319,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "you decrease the learning rate, it'll",
      "start": 3573.92,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "very rapidly drop off. And when you look",
      "start": 3575.599,
      "duration": 5.041,
      "language": "en"
    },
    {
      "text": "at sort of cosine learning rate decay,",
      "start": 3578.079,
      "duration": 4.401,
      "language": "en"
    },
    {
      "text": "what happens is that you know the models",
      "start": 3580.64,
      "duration": 4.719,
      "language": "en"
    },
    {
      "text": "with high weight decay start out very",
      "start": 3582.48,
      "duration": 4.639,
      "language": "en"
    },
    {
      "text": "slow, but then as they cool down, that",
      "start": 3585.359,
      "duration": 3.521,
      "language": "en"
    },
    {
      "text": "is their their learning rate decreases,",
      "start": 3587.119,
      "duration": 4.801,
      "language": "en"
    },
    {
      "text": "they very rapidly optimize. And so",
      "start": 3588.88,
      "duration": 5.679,
      "language": "en"
    },
    {
      "text": "there's some very complex sort of",
      "start": 3591.92,
      "duration": 5.0,
      "language": "en"
    },
    {
      "text": "interaction happening here between the",
      "start": 3594.559,
      "duration": 5.121,
      "language": "en"
    },
    {
      "text": "optimizer and the weight decay and some",
      "start": 3596.92,
      "duration": 4.76,
      "language": "en"
    },
    {
      "text": "sort of implicit sort of acceleration",
      "start": 3599.68,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "that happens near the tail end of",
      "start": 3601.68,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "training that ends up giving you better",
      "start": 3603.2,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "models. And so the answer to the",
      "start": 3605.68,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "question I posed you is, you know, you",
      "start": 3607.599,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "don't weight decay because you want to",
      "start": 3609.44,
      "duration": 3.2,
      "language": "en"
    },
    {
      "text": "regularize the model, which is kind of",
      "start": 3611.119,
      "duration": 3.281,
      "language": "en"
    },
    {
      "text": "what it was designed for. You're weight",
      "start": 3612.64,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "decaying in order to get actually better",
      "start": 3614.4,
      "duration": 4.959,
      "language": "en"
    },
    {
      "text": "training losses. And you end up doing",
      "start": 3616.88,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "that because of the various learning",
      "start": 3619.359,
      "duration": 4.081,
      "language": "en"
    },
    {
      "text": "dynamics at the tail end of training as",
      "start": 3621.28,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "you decrease your learning rates to",
      "start": 3623.44,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "zero. It's a very sort of very",
      "start": 3624.88,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "interesting and complex and in some ways",
      "start": 3627.68,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "you know uh troubling you know thing to",
      "start": 3629.92,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "be doing uh with language models. But",
      "start": 3632.0,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "now you sort of see why you know if you",
      "start": 3634.079,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "look at the a lot of the reports you'll",
      "start": 3636.24,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "see we use weight decay. This is kind of",
      "start": 3638.079,
      "duration": 3.48,
      "language": "en"
    },
    {
      "text": "why that ends up",
      "start": 3639.76,
      "duration": 4.52,
      "language": "en"
    },
    {
      "text": "happening.",
      "start": 3641.559,
      "duration": 6.52,
      "language": "en"
    },
    {
      "text": "Cool. Okay. So, um, putting all that",
      "start": 3644.28,
      "duration": 5.96,
      "language": "en"
    },
    {
      "text": "together, so there are certain things",
      "start": 3648.079,
      "duration": 3.681,
      "language": "en"
    },
    {
      "text": "that I think are just kind of",
      "start": 3650.24,
      "duration": 2.879,
      "language": "en"
    },
    {
      "text": "no-brainers. So, if you're picking",
      "start": 3651.76,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "various hyperparameters for your model,",
      "start": 3653.119,
      "duration": 3.2,
      "language": "en"
    },
    {
      "text": "you don't really need to think too",
      "start": 3655.04,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "deeply about them, um, in the sense that",
      "start": 3656.319,
      "duration": 3.681,
      "language": "en"
    },
    {
      "text": "they've been validated and basically",
      "start": 3658.4,
      "duration": 2.959,
      "language": "en"
    },
    {
      "text": "everyone else does them. So, this is",
      "start": 3660.0,
      "duration": 3.119,
      "language": "en"
    },
    {
      "text": "things like, you know, the hidden size",
      "start": 3661.359,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "of a of a MLP, the head dimensions of",
      "start": 3663.119,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "your your multi head attention, um, your",
      "start": 3665.839,
      "duration": 5.041,
      "language": "en"
    },
    {
      "text": "aspect ratio, um, and your choice of",
      "start": 3668.079,
      "duration": 4.561,
      "language": "en"
    },
    {
      "text": "regularization through weight decay.",
      "start": 3670.88,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "like all of those there's fairly good I",
      "start": 3672.64,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "think uh consensus evidence of how to",
      "start": 3675.04,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "pick most of these hyperparameters and",
      "start": 3677.28,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "those defaults roughly give you the",
      "start": 3679.2,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "kinds of um things that we suggest in",
      "start": 3680.64,
      "duration": 3.199,
      "language": "en"
    },
    {
      "text": "the assignment so you can kind of follow",
      "start": 3682.64,
      "duration": 2.56,
      "language": "en"
    },
    {
      "text": "along and they'll roughly give you",
      "start": 3683.839,
      "duration": 4.801,
      "language": "en"
    },
    {
      "text": "something similar um to this. So okay",
      "start": 3685.2,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "any questions about uh the",
      "start": 3688.64,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "hyperparameter uh piece?",
      "start": 3689.92,
      "duration": 5.679,
      "language": "en"
    },
    {
      "text": "Yes. Yeah. Is there a reason why",
      "start": 3694.16,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "dropouts like gone out of pattern?",
      "start": 3695.599,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "That's a good question. Um I don't think",
      "start": 3698.319,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "I've seen uh the question was why did",
      "start": 3700.319,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "dropout go out of fashion? Um I haven't",
      "start": 3702.319,
      "duration": 4.961,
      "language": "en"
    },
    {
      "text": "quite seen a deep analysis of of why",
      "start": 3705.119,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "dropout is or isn't helpful. Like I",
      "start": 3707.28,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "haven't seen any uh result that for",
      "start": 3709.599,
      "duration": 3.121,
      "language": "en"
    },
    {
      "text": "example shows that it helps for training",
      "start": 3711.2,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "loss. And as sort of this you know what",
      "start": 3712.72,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "this paper argues and logic would",
      "start": 3715.28,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "dictate there's not really a training",
      "start": 3716.88,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "overfitting issue with these models that",
      "start": 3719.44,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "can't even do one epoch over their",
      "start": 3721.76,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "training data.",
      "start": 3723.44,
      "duration": 3.72,
      "language": "en"
    },
    {
      "text": "Yes.",
      "start": 3725.359,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "Um, do multilingual vocabularies",
      "start": 3727.16,
      "duration": 5.159,
      "language": "en"
    },
    {
      "text": "actually contribute to improved",
      "start": 3730.319,
      "duration": 6.8,
      "language": "en"
    },
    {
      "text": "performance in one language? So, I get",
      "start": 3732.319,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "Yeah. So, the question was, do",
      "start": 3737.119,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "multilingual vocabularies contribute to",
      "start": 3738.319,
      "duration": 3.841,
      "language": "en"
    },
    {
      "text": "improving uh performance in one",
      "start": 3740.799,
      "duration": 2.481,
      "language": "en"
    },
    {
      "text": "language? When you say one language, do",
      "start": 3742.16,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "you mean do mon multilingual or like you",
      "start": 3743.28,
      "duration": 3.519,
      "language": "en"
    },
    {
      "text": "know larger vocabularies help",
      "start": 3745.44,
      "duration": 2.96,
      "language": "en"
    },
    {
      "text": "performance in English? Is that the",
      "start": 3746.799,
      "duration": 4.161,
      "language": "en"
    },
    {
      "text": "right question? Yeah. So, I think in",
      "start": 3748.4,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "your high resource language, the impact",
      "start": 3750.96,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "is less, right? So, you know, if you're",
      "start": 3753.28,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "only thinking about, you know, English",
      "start": 3755.52,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "language language modeling, you can get",
      "start": 3757.28,
      "duration": 4.319,
      "language": "en"
    },
    {
      "text": "away with smaller vocabularies. This",
      "start": 3759.52,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "much is kind of, you know, true. Um, but",
      "start": 3761.599,
      "duration": 4.641,
      "language": "en"
    },
    {
      "text": "the place where larger vocabularies is",
      "start": 3764.16,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "really helpful is when you're starting",
      "start": 3766.24,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "to get at, I wouldn't say the tail of",
      "start": 3768.16,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "your distribution, but when you get to",
      "start": 3769.839,
      "duration": 3.121,
      "language": "en"
    },
    {
      "text": "languages that are sort of more",
      "start": 3771.68,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "minority. And one great example of this,",
      "start": 3772.96,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "um, if you look at any of the coher, uh,",
      "start": 3775.28,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "announcements about their models or",
      "start": 3777.44,
      "duration": 3.679,
      "language": "en"
    },
    {
      "text": "their tokenizers, um, they basically",
      "start": 3778.88,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "always argue that because of the way",
      "start": 3781.119,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "they have larger vocabularies and the",
      "start": 3782.88,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "way they train their tokenizer, um,",
      "start": 3784.799,
      "duration": 3.921,
      "language": "en"
    },
    {
      "text": "non-English and like low resources",
      "start": 3787.04,
      "duration": 3.519,
      "language": "en"
    },
    {
      "text": "languages, they they are packed into",
      "start": 3788.72,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "much fewer tokens and so people using",
      "start": 3790.559,
      "duration": 6.081,
      "language": "en"
    },
    {
      "text": "those pay much fewer, you know, uh, much",
      "start": 3793.2,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "lower cost at inference time, right?",
      "start": 3796.64,
      "duration": 3.64,
      "language": "en"
    },
    {
      "text": "Which is a which is a great",
      "start": 3798.4,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "benefit. Oh yes, question. these plots",
      "start": 3800.28,
      "duration": 5.24,
      "language": "en"
    },
    {
      "text": "um if weight t doesn't have a",
      "start": 3803.92,
      "duration": 3.119,
      "language": "en"
    },
    {
      "text": "significant impact on the valve loss",
      "start": 3805.52,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "like why why do we care about like the",
      "start": 3807.039,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "training dynamics or the favorable",
      "start": 3808.88,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "operation dynamics right okay so the",
      "start": 3810.64,
      "duration": 3.12,
      "language": "en"
    },
    {
      "text": "question was if it doesn't have an",
      "start": 3812.64,
      "duration": 2.719,
      "language": "en"
    },
    {
      "text": "impact on val loss why do we care about",
      "start": 3813.76,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "training dynamics um the goal is still I",
      "start": 3815.359,
      "duration": 5.521,
      "language": "en"
    },
    {
      "text": "want to get you know um good training",
      "start": 3818.48,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "loss right this is the game that we're",
      "start": 3820.88,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "playing and the surprising thing about",
      "start": 3822.559,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "weight decay is that somehow it gets us",
      "start": 3824.48,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "better training losses like I think the",
      "start": 3826.079,
      "duration": 3.921,
      "language": "en"
    },
    {
      "text": "intuitive thing that makes sense is you",
      "start": 3828.16,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "do weight decay it gives you better val",
      "start": 3830.0,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "losses But that's not what happens. What",
      "start": 3831.76,
      "duration": 3.2,
      "language": "en"
    },
    {
      "text": "what it's getting you is better training",
      "start": 3833.44,
      "duration": 6.04,
      "language": "en"
    },
    {
      "text": "losses which are also the same as vales.",
      "start": 3834.96,
      "duration": 4.52,
      "language": "en"
    },
    {
      "text": "Yes. Are there differences in the",
      "start": 3841.28,
      "duration": 5.039,
      "language": "en"
    },
    {
      "text": "architecture hyperparameter choices",
      "start": 3844.48,
      "duration": 3.72,
      "language": "en"
    },
    {
      "text": "people make as they move towards like",
      "start": 3846.319,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "multimodal architectures if they're",
      "start": 3848.2,
      "duration": 5.0,
      "language": "en"
    },
    {
      "text": "images text? Yes. So the question was",
      "start": 3850.64,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "about multimodal models. That is a great",
      "start": 3853.2,
      "duration": 4.879,
      "language": "en"
    },
    {
      "text": "question. My my survey of multimodal",
      "start": 3856.0,
      "duration": 5.359,
      "language": "en"
    },
    {
      "text": "models is very incomplete. Um what I can",
      "start": 3858.079,
      "duration": 5.681,
      "language": "en"
    },
    {
      "text": "say is a lot of the academic and open",
      "start": 3861.359,
      "duration": 4.881,
      "language": "en"
    },
    {
      "text": "work that I've seen um they do uh what",
      "start": 3863.76,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "you might call like shallow or like",
      "start": 3866.24,
      "duration": 4.799,
      "language": "en"
    },
    {
      "text": "later fusion um or early fusion of the",
      "start": 3868.0,
      "duration": 5.039,
      "language": "en"
    },
    {
      "text": "modalities and the way that works is you",
      "start": 3871.039,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "kind of bolt the vision modality onto a",
      "start": 3873.039,
      "duration": 3.841,
      "language": "en"
    },
    {
      "text": "existing language model. In those cases,",
      "start": 3875.119,
      "duration": 3.121,
      "language": "en"
    },
    {
      "text": "the hyperparameter and architecture",
      "start": 3876.88,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "choices are fixed. Right? Um, one thing",
      "start": 3878.24,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "I will note and I will talk about this",
      "start": 3880.48,
      "duration": 4.319,
      "language": "en"
    },
    {
      "text": "in just a few slides, um, is that the",
      "start": 3882.079,
      "duration": 5.121,
      "language": "en"
    },
    {
      "text": "multimodal models pioneered some pretty",
      "start": 3884.799,
      "duration": 4.961,
      "language": "en"
    },
    {
      "text": "interesting techniques in stabilizing",
      "start": 3887.2,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "language model training and that's been",
      "start": 3889.76,
      "duration": 3.279,
      "language": "en"
    },
    {
      "text": "a really big theme and I'll talk a",
      "start": 3891.44,
      "duration": 3.04,
      "language": "en"
    },
    {
      "text": "little bit about those. So what is",
      "start": 3893.039,
      "duration": 3.441,
      "language": "en"
    },
    {
      "text": "different is often when you like bolt on",
      "start": 3894.48,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "this new kind of vision piece and you",
      "start": 3896.48,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "like retrain with that that's a big",
      "start": 3898.559,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "shock to the model and so you have to",
      "start": 3900.24,
      "duration": 2.96,
      "language": "en"
    },
    {
      "text": "think carefully about how to stabilize",
      "start": 3901.839,
      "duration": 2.641,
      "language": "en"
    },
    {
      "text": "that training process and those",
      "start": 3903.2,
      "duration": 2.8,
      "language": "en"
    },
    {
      "text": "innovations have actually seeped back",
      "start": 3904.48,
      "duration": 5.319,
      "language": "en"
    },
    {
      "text": "into like pure text language model",
      "start": 3906.0,
      "duration": 9.16,
      "language": "en"
    },
    {
      "text": "training. Okay, cool.",
      "start": 3909.799,
      "duration": 5.361,
      "language": "en"
    },
    {
      "text": "So um I went back through and you know I",
      "start": 3916.0,
      "duration": 4.799,
      "language": "en"
    },
    {
      "text": "looked through all these new papers and",
      "start": 3919.2,
      "duration": 2.8,
      "language": "en"
    },
    {
      "text": "as I was trying to think about okay",
      "start": 3920.799,
      "duration": 3.361,
      "language": "en"
    },
    {
      "text": "what's been new in the last year and",
      "start": 3922.0,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "sort of what new architecture and",
      "start": 3924.16,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "related things have happened actually",
      "start": 3925.76,
      "duration": 3.279,
      "language": "en"
    },
    {
      "text": "you know the the core architecture",
      "start": 3927.68,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "hasn't changed much but I think the one",
      "start": 3929.039,
      "duration": 4.241,
      "language": "en"
    },
    {
      "text": "thing that stood out as being very",
      "start": 3931.2,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "emphasized in a lot of the releases um",
      "start": 3933.28,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "has been what I would call stability",
      "start": 3936.0,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "tricks and so these are things where you",
      "start": 3938.24,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "would like to train your model in much",
      "start": 3940.96,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "more stable ways um and as you make",
      "start": 3942.96,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "bigger and bigger models or you train",
      "start": 3945.92,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "for longer and longer um these kinds of",
      "start": 3947.28,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "issues start to appear more and more. So",
      "start": 3949.68,
      "duration": 5.6,
      "language": "en"
    },
    {
      "text": "I've taken this um from the mode 2 paper",
      "start": 3951.44,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "um and actually that paper is a great",
      "start": 3955.28,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "sort of set of you know academic results",
      "start": 3957.44,
      "duration": 6.879,
      "language": "en"
    },
    {
      "text": "on LLM training stability and you know",
      "start": 3960.16,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "one thing they start with is is kind of",
      "start": 3964.319,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "this figure and you look at this blue",
      "start": 3965.92,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "curve over here and you look at this you",
      "start": 3967.599,
      "duration": 4.881,
      "language": "en"
    },
    {
      "text": "know L2 norm of the gradient graph and",
      "start": 3970.079,
      "duration": 4.801,
      "language": "en"
    },
    {
      "text": "this is terrifying graph to look look at",
      "start": 3972.48,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "right like you know your your loss curve",
      "start": 3974.88,
      "duration": 3.919,
      "language": "en"
    },
    {
      "text": "kind of seems to be behaving okay but",
      "start": 3976.88,
      "duration": 3.679,
      "language": "en"
    },
    {
      "text": "you've got some some bad spikes every",
      "start": 3978.799,
      "duration": 3.841,
      "language": "en"
    },
    {
      "text": "now and and you open up your gradient",
      "start": 3980.559,
      "duration": 4.401,
      "language": "en"
    },
    {
      "text": "norm and it's this horrible plot where",
      "start": 3982.64,
      "duration": 3.919,
      "language": "en"
    },
    {
      "text": "you've got spikes everywhere where your",
      "start": 3984.96,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "norms are completely blowing up. Um, and",
      "start": 3986.559,
      "duration": 4.721,
      "language": "en"
    },
    {
      "text": "you know, if you're training models like",
      "start": 3989.92,
      "duration": 2.48,
      "language": "en"
    },
    {
      "text": "this, you're going to have a really",
      "start": 3991.28,
      "duration": 3.039,
      "language": "en"
    },
    {
      "text": "tough time getting it to converge",
      "start": 3992.4,
      "duration": 3.679,
      "language": "en"
    },
    {
      "text": "reasonably. At some point, it's going",
      "start": 3994.319,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "to, you know, hit, you know, gradient",
      "start": 3996.079,
      "duration": 3.201,
      "language": "en"
    },
    {
      "text": "norm explodes and like you can't do",
      "start": 3997.599,
      "duration": 3.281,
      "language": "en"
    },
    {
      "text": "anything and your training is done,",
      "start": 3999.28,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "right? So, you can't train any further.",
      "start": 4000.88,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "And so, there's been a lot of emphasis",
      "start": 4003.2,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "basically trying to turn this blue curve",
      "start": 4005.28,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "into something that looks a lot like the",
      "start": 4007.28,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "orange curve. Um, and of course this",
      "start": 4008.96,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "loss is higher, but ignore that fact",
      "start": 4010.64,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "because I think they just switch data",
      "start": 4012.24,
      "duration": 3.2,
      "language": "en"
    },
    {
      "text": "sets in between these two training runs.",
      "start": 4013.52,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "Um, but this orange curve, you know, has",
      "start": 4015.44,
      "duration": 4.879,
      "language": "en"
    },
    {
      "text": "nice low gradient norms throughout. And",
      "start": 4017.359,
      "duration": 4.401,
      "language": "en"
    },
    {
      "text": "that's really the kind of pod that you",
      "start": 4020.319,
      "duration": 3.0,
      "language": "en"
    },
    {
      "text": "would much rather",
      "start": 4021.76,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "see. And so you might ask, where do",
      "start": 4023.319,
      "duration": 5.081,
      "language": "en"
    },
    {
      "text": "stability issues arise in transformers?",
      "start": 4026.24,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "And of course, they can arise basically",
      "start": 4028.4,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "everywhere. Um but if you look at the",
      "start": 4030.4,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "kind of interventions that people are",
      "start": 4032.799,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "making um there's really one place that",
      "start": 4034.72,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "really stands out as the the kind of",
      "start": 4037.599,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "problem child and that's the soft maxes.",
      "start": 4039.28,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "Um and it can be a problem because",
      "start": 4042.079,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "you're going to be taking exponentials",
      "start": 4044.16,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "and those can be numerically you know",
      "start": 4045.68,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "badly behaved. Um you're also dividing",
      "start": 4047.68,
      "duration": 3.919,
      "language": "en"
    },
    {
      "text": "two numbers and so you might have a",
      "start": 4050.24,
      "duration": 2.799,
      "language": "en"
    },
    {
      "text": "division by zero, right? So for many",
      "start": 4051.599,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "different reasons, this softmax piece is",
      "start": 4053.039,
      "duration": 5.601,
      "language": "en"
    },
    {
      "text": "a is a part that you know um you might",
      "start": 4055.599,
      "duration": 5.841,
      "language": "en"
    },
    {
      "text": "have lots of issues with. And so",
      "start": 4058.64,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "actually one more thing I want to talk",
      "start": 4061.44,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "about. So where are the softmaxes in a",
      "start": 4063.039,
      "duration": 4.161,
      "language": "en"
    },
    {
      "text": "transformer? Well, there's one at the",
      "start": 4065.52,
      "duration": 3.12,
      "language": "en"
    },
    {
      "text": "very end. So you've got to be careful",
      "start": 4067.2,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "about that output softmax. And also",
      "start": 4068.64,
      "duration": 4.479,
      "language": "en"
    },
    {
      "text": "there's soft maxes in your self",
      "start": 4071.28,
      "duration": 3.519,
      "language": "en"
    },
    {
      "text": "attention, right? So there's two soft",
      "start": 4073.119,
      "duration": 3.041,
      "language": "en"
    },
    {
      "text": "maxes that that we're going to think a",
      "start": 4074.799,
      "duration": 3.201,
      "language": "en"
    },
    {
      "text": "little bit about. And for each one I'm",
      "start": 4076.16,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "going to mention a stability",
      "start": 4078.0,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "intervention that has you know generally",
      "start": 4079.92,
      "duration": 3.96,
      "language": "en"
    },
    {
      "text": "seemed to be",
      "start": 4082.0,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "effective. Okay. So the first one is",
      "start": 4083.88,
      "duration": 5.88,
      "language": "en"
    },
    {
      "text": "called the zloss. Um and in my desire to",
      "start": 4086.72,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "site a paper that's older I've gone back",
      "start": 4089.76,
      "duration": 6.24,
      "language": "en"
    },
    {
      "text": "to Devlin in 2014. Um where in a machine",
      "start": 4092.24,
      "duration": 6.64,
      "language": "en"
    },
    {
      "text": "translation paper um you know their goal",
      "start": 4096.0,
      "duration": 4.719,
      "language": "en"
    },
    {
      "text": "was to try to you know make sure that",
      "start": 4098.88,
      "duration": 5.439,
      "language": "en"
    },
    {
      "text": "this normalizer was near one. So if you",
      "start": 4100.719,
      "duration": 5.841,
      "language": "en"
    },
    {
      "text": "look at P of X that's the output softmax",
      "start": 4104.319,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "over here. Um the output softmax is two",
      "start": 4106.56,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "terms. You exponentiate your logets and",
      "start": 4109.359,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "then you divide by the normalizer Z",
      "start": 4111.44,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "right the Z is just summing up you know",
      "start": 4113.359,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "the the values across all the vocap. And",
      "start": 4115.52,
      "duration": 4.639,
      "language": "en"
    },
    {
      "text": "so if you want this Z of X you want to",
      "start": 4118.159,
      "duration": 4.721,
      "language": "en"
    },
    {
      "text": "train the network to have a Z of X close",
      "start": 4120.159,
      "duration": 4.721,
      "language": "en"
    },
    {
      "text": "to one. Well then you can you know",
      "start": 4122.88,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "rewrite your loss and you can add a",
      "start": 4124.88,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "little second term here to try to force",
      "start": 4126.719,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "log of Z of XI to be close to zero.",
      "start": 4128.96,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "Right? Okay, so you're going to end up",
      "start": 4132.159,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "with an auxiliary loss term that's alpha",
      "start": 4133.04,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "log 2 Z of XI, right? You can kind of",
      "start": 4135.679,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "see that derivation on the on the right",
      "start": 4137.92,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "here. Um, and this is, you know, in some",
      "start": 4139.359,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "sense what people often call the Z loss.",
      "start": 4141.759,
      "duration": 4.801,
      "language": "en"
    },
    {
      "text": "Um, I think, you know, Jacob Develin and",
      "start": 4144.319,
      "duration": 3.761,
      "language": "en"
    },
    {
      "text": "others did this for machine translation",
      "start": 4146.56,
      "duration": 3.119,
      "language": "en"
    },
    {
      "text": "for totally different reasons than what",
      "start": 4148.08,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "it's used for today. Um but this was I",
      "start": 4149.679,
      "duration": 4.801,
      "language": "en"
    },
    {
      "text": "think the first instance of this in in",
      "start": 4152.88,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "language modeling land was Palm who used",
      "start": 4154.48,
      "duration": 5.359,
      "language": "en"
    },
    {
      "text": "this as they called it auxiliary loss of",
      "start": 4156.96,
      "duration": 6.239,
      "language": "en"
    },
    {
      "text": "Z loss 10^4 log 2 Z to basically",
      "start": 4159.839,
      "duration": 5.121,
      "language": "en"
    },
    {
      "text": "encourage the softmax normalizer to",
      "start": 4163.199,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "behave nicely and you can kind of reason",
      "start": 4164.96,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "through the behavior of this",
      "start": 4168.239,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "regularizer. If it succeeds and it",
      "start": 4169.44,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "forces log of Z of X to always be zero,",
      "start": 4171.679,
      "duration": 5.201,
      "language": "en"
    },
    {
      "text": "then the log and the exponent the",
      "start": 4174.88,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "exponential cancels and you've basically",
      "start": 4176.88,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "just got U of R of X. And that's a good",
      "start": 4178.719,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "place to be, right? That's a nice",
      "start": 4181.04,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "numerically stable operation. So all of",
      "start": 4182.159,
      "duration": 3.841,
      "language": "en"
    },
    {
      "text": "these sort of problematic operations",
      "start": 4184.4,
      "duration": 3.359,
      "language": "en"
    },
    {
      "text": "kind of go away. And so you can think of",
      "start": 4186.0,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "the softmax as being well behaved when Z",
      "start": 4187.759,
      "duration": 5.6,
      "language": "en"
    },
    {
      "text": "of X um is close to to uh one or log of",
      "start": 4189.92,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "Z is close to zero, right?",
      "start": 4193.359,
      "duration": 4.721,
      "language": "en"
    },
    {
      "text": "And you know, palm in some sense is a is",
      "start": 4195.76,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "very much a pioneer because they did",
      "start": 4198.08,
      "duration": 4.639,
      "language": "en"
    },
    {
      "text": "this zlos trick. Um, and many others",
      "start": 4200.159,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "didn't really do it for a long time or",
      "start": 4202.719,
      "duration": 3.761,
      "language": "en"
    },
    {
      "text": "at least the ones that had open papers.",
      "start": 4204.48,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "Um, but then there was a kind of",
      "start": 4206.48,
      "duration": 3.12,
      "language": "en"
    },
    {
      "text": "sequence of papers um that have done",
      "start": 4207.84,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "this. Byron 2 is actually the the",
      "start": 4209.6,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "earliest follow-up that I know of. And",
      "start": 4211.76,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "then DCLM and Almo and now um several",
      "start": 4213.36,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "others have basically picked up on ZLOS",
      "start": 4215.84,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "as a very nice convenient intervention",
      "start": 4218.08,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "uh for for improving stability. Um and",
      "start": 4220.4,
      "duration": 6.319,
      "language": "en"
    },
    {
      "text": "then uh the other trick that we see so",
      "start": 4224.08,
      "duration": 6.079,
      "language": "en"
    },
    {
      "text": "that was um how to stabilize the uh the",
      "start": 4226.719,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "output softmax but we've got another",
      "start": 4230.159,
      "duration": 3.841,
      "language": "en"
    },
    {
      "text": "softmax we've got to deal with right the",
      "start": 4232.239,
      "duration": 4.241,
      "language": "en"
    },
    {
      "text": "other softmax we have to deal with is in",
      "start": 4234.0,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "the attention operation um and so you",
      "start": 4236.48,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "know this is um from a Nvidia paper I",
      "start": 4239.52,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "forgot to put the citation marker um but",
      "start": 4242.0,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "here you know this is a a block diagram",
      "start": 4244.64,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "of how attention works you know you've",
      "start": 4247.04,
      "duration": 3.119,
      "language": "en"
    },
    {
      "text": "got your your layer norm at the",
      "start": 4248.64,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "beginning um you got your 2KVS um ignore",
      "start": 4250.159,
      "duration": 4.721,
      "language": "en"
    },
    {
      "text": "this for the moment. Um you might",
      "start": 4253.04,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "multiply your your Q's and your K's.",
      "start": 4254.88,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "You'll softmax it. You multiply the V",
      "start": 4256.96,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "and then you'll project it. Um and then",
      "start": 4259.52,
      "duration": 3.12,
      "language": "en"
    },
    {
      "text": "that's going to give you your fully",
      "start": 4261.52,
      "duration": 2.639,
      "language": "en"
    },
    {
      "text": "connected and your output, right? So so",
      "start": 4262.64,
      "duration": 3.039,
      "language": "en"
    },
    {
      "text": "if you ignore this this little piece",
      "start": 4264.159,
      "duration": 3.841,
      "language": "en"
    },
    {
      "text": "over here, um you know, this looks just",
      "start": 4265.679,
      "duration": 4.841,
      "language": "en"
    },
    {
      "text": "like your normal multi head attention",
      "start": 4268.0,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "operation. So what's kind of the",
      "start": 4270.52,
      "duration": 6.12,
      "language": "en"
    },
    {
      "text": "difference here? Um so several folks uh",
      "start": 4272.96,
      "duration": 5.6,
      "language": "en"
    },
    {
      "text": "came up with this idea or this approach",
      "start": 4276.64,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "called the QK norm where you take the",
      "start": 4278.56,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "the queries and the keys and you pass",
      "start": 4280.96,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "them through a layer norm layer um",
      "start": 4282.8,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "before you you know take their inner",
      "start": 4284.88,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "product for the softmax operation. Um",
      "start": 4286.4,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "and this is a you know very different",
      "start": 4288.88,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "kind of approach to controlling the",
      "start": 4290.56,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "behavior um of the softmax. Here you're",
      "start": 4292.8,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "not controlling the the normalizer Z.",
      "start": 4295.52,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "Instead, you're controlling the inputs",
      "start": 4297.6,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "to the softmax to be kind of bounded in",
      "start": 4299.04,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "size, and that's going to naturally",
      "start": 4301.679,
      "duration": 4.281,
      "language": "en"
    },
    {
      "text": "control uh the the bad behaviors of the",
      "start": 4303.12,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "softmax. Um, and as I said before, this",
      "start": 4305.96,
      "duration": 4.84,
      "language": "en"
    },
    {
      "text": "is originally an innovation uh from the",
      "start": 4308.56,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "vision and sort of multimodal model",
      "start": 4310.8,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "community. Um, Deani in 2023, this was,",
      "start": 4312.64,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "you know, a paper on training very large",
      "start": 4315.6,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "vision transformers. Um and then",
      "start": 4317.44,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "Chameleon and uh Edith Feix from uh",
      "start": 4319.84,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "Hugging Face sort of used these tricks",
      "start": 4322.64,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "for their like multimodal training",
      "start": 4324.64,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "components. Um and then you know it got",
      "start": 4326.4,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "picked up by several others like GMAT 2,",
      "start": 4329.28,
      "duration": 6.64,
      "language": "en"
    },
    {
      "text": "DCLM, OMO2 um all basically uses this",
      "start": 4331.6,
      "duration": 6.84,
      "language": "en"
    },
    {
      "text": "kind of techniques um in order to",
      "start": 4335.92,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "stabilize um their training. And I think",
      "start": 4338.44,
      "duration": 6.84,
      "language": "en"
    },
    {
      "text": "I'm allowed to add one joke per lecture",
      "start": 4341.76,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "and so this is the one I'm going to go",
      "start": 4345.28,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "with here. Um I think one of the things",
      "start": 4346.56,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "that really has stood out in terms of",
      "start": 4348.64,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "stability interventions has been just",
      "start": 4350.88,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "how strikingly effective layer norms",
      "start": 4352.8,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "are. Right? So we've seen, you know,",
      "start": 4355.28,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "going from layer norms just in the the",
      "start": 4357.04,
      "duration": 5.92,
      "language": "en"
    },
    {
      "text": "pre part of the block to the both the",
      "start": 4359.6,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "beginning and the end of the",
      "start": 4362.96,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "non-residual component and now we've",
      "start": 4364.48,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "also thrown it into the Q and the K uh",
      "start": 4366.4,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "component. At least in terms of",
      "start": 4368.96,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "improving stability, um layer norms have",
      "start": 4370.4,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "been shockingly effective without",
      "start": 4372.8,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "affecting performance too much.",
      "start": 4374.96,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "Um the last trick that I'll note um I",
      "start": 4377.28,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "think this one has been sort of not",
      "start": 4380.08,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "quite as frequently used um which is to",
      "start": 4382.48,
      "duration": 7.12,
      "language": "en"
    },
    {
      "text": "soft cap um the uh the lojets that go",
      "start": 4385.6,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "into the soft. So the other approach",
      "start": 4389.6,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "that you can take so qk norm is in some",
      "start": 4391.44,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "sense a very heavy-handed intervention",
      "start": 4393.679,
      "duration": 3.761,
      "language": "en"
    },
    {
      "text": "because we're going to operate over the",
      "start": 4396.0,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "entire vector. Um but one thing you",
      "start": 4397.44,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "could do is after you take the inner",
      "start": 4400.0,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "products for self attention, you could",
      "start": 4401.52,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "pass them through kind of like a soft",
      "start": 4403.52,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "maximum operation. So you can pass them",
      "start": 4405.28,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "through this uh equation over here. So",
      "start": 4407.28,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "you have your low jets as your input",
      "start": 4409.36,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "divided by the soft cap multiply by the",
      "start": 4411.44,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "soft cap. What does that do? Well, if",
      "start": 4414.08,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "your lows start exceeding the soft cap",
      "start": 4416.08,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "by a lot, the tanh is going to clip them",
      "start": 4418.4,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "off to one. And so you're going to have",
      "start": 4420.56,
      "duration": 5.119,
      "language": "en"
    },
    {
      "text": "a maximum value of soft cap over here,",
      "start": 4422.4,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "right? So this is going to control in",
      "start": 4425.679,
      "duration": 4.961,
      "language": "en"
    },
    {
      "text": "some sense soft clipping of the logets",
      "start": 4427.52,
      "duration": 6.159,
      "language": "en"
    },
    {
      "text": "um and gamma 2 um and I think 2 also do",
      "start": 4430.64,
      "duration": 6.24,
      "language": "en"
    },
    {
      "text": "this um it hasn't been I think quite as",
      "start": 4433.679,
      "duration": 5.841,
      "language": "en"
    },
    {
      "text": "uh popular otherwise and I think the",
      "start": 4436.88,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "other sort of evidence against this um",
      "start": 4439.52,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "the the Nvidia folks that I mentioned",
      "start": 4441.84,
      "duration": 3.839,
      "language": "en"
    },
    {
      "text": "earlier did actually quite a few",
      "start": 4443.76,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "different sort of stability uh improving",
      "start": 4445.679,
      "duration": 4.081,
      "language": "en"
    },
    {
      "text": "interventions and what they find is you",
      "start": 4447.84,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "know you have your baseline model over",
      "start": 4449.76,
      "duration": 3.919,
      "language": "en"
    },
    {
      "text": "here this is the the perplexity of the",
      "start": 4451.44,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "baseline model 11.19",
      "start": 4453.679,
      "duration": 4.401,
      "language": "en"
    },
    {
      "text": "Soft capping makes it worse. QK norm",
      "start": 4455.6,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "actually makes it better because you can",
      "start": 4458.08,
      "duration": 2.8,
      "language": "en"
    },
    {
      "text": "use more aggressive learning rates and",
      "start": 4459.28,
      "duration": 5.399,
      "language": "en"
    },
    {
      "text": "sort of push the optimizer further.",
      "start": 4460.88,
      "duration": 7.44,
      "language": "en"
    },
    {
      "text": "Um, cool. Okay. So, so that's the um end",
      "start": 4464.679,
      "duration": 5.56,
      "language": "en"
    },
    {
      "text": "of sort of the stability improving",
      "start": 4468.32,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "intervention stuff. Um, does anyone have",
      "start": 4470.239,
      "duration": 4.561,
      "language": "en"
    },
    {
      "text": "any questions? Um, I think that's been",
      "start": 4472.48,
      "duration": 4.239,
      "language": "en"
    },
    {
      "text": "kind of the the new development over the",
      "start": 4474.8,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "last year. Yes. So, for the QKV norm um",
      "start": 4476.719,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "like understand that during training you",
      "start": 4479.44,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "will have the layer norm being applied",
      "start": 4481.199,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "at inference time. Is the layer norm",
      "start": 4482.96,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "still being kept? Yes. So the question",
      "start": 4484.8,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "was at inference time do you still use",
      "start": 4487.28,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "the norm? And the answer is yes because",
      "start": 4488.8,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "the layer norm has kind of learned",
      "start": 4491.04,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "parameters like the whole you know",
      "start": 4493.12,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "action of the layer norm is it takes an",
      "start": 4494.88,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "activation normalizes it to unit and",
      "start": 4496.48,
      "duration": 3.759,
      "language": "en"
    },
    {
      "text": "then scales them to some size. If you",
      "start": 4498.56,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "take that out that's a huge change to",
      "start": 4500.239,
      "duration": 3.121,
      "language": "en"
    },
    {
      "text": "the model. It will have no idea what to",
      "start": 4501.84,
      "duration": 4.879,
      "language": "en"
    },
    {
      "text": "do with those unnormalized",
      "start": 4503.36,
      "duration": 3.359,
      "language": "en"
    },
    {
      "text": "activations.",
      "start": 4507.08,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "Okay",
      "start": 4508.76,
      "duration": 5.959,
      "language": "en"
    },
    {
      "text": "cool. All right. So um I have this last",
      "start": 4510.92,
      "duration": 6.36,
      "language": "en"
    },
    {
      "text": "bit last few slides um that I want to",
      "start": 4514.719,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "end with. Um if we if we go over then we",
      "start": 4517.28,
      "duration": 4.879,
      "language": "en"
    },
    {
      "text": "can always push this into the thee",
      "start": 4520.159,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "lecture but I think we also have a lot",
      "start": 4522.159,
      "duration": 3.201,
      "language": "en"
    },
    {
      "text": "of content next time because I have to",
      "start": 4524.159,
      "duration": 4.961,
      "language": "en"
    },
    {
      "text": "cover um deepseat v3. Um so the last",
      "start": 4525.36,
      "duration": 5.359,
      "language": "en"
    },
    {
      "text": "thing I want to cover is variations on",
      "start": 4529.12,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "the attention heads. Um so attention",
      "start": 4530.719,
      "duration": 5.281,
      "language": "en"
    },
    {
      "text": "heads I think haven't had as much you",
      "start": 4533.44,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "know work done to them. Um but there",
      "start": 4536.0,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "have been a few I think important",
      "start": 4538.8,
      "duration": 3.439,
      "language": "en"
    },
    {
      "text": "changes that you need to know about in",
      "start": 4540.48,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "order to understand uh the models that",
      "start": 4542.239,
      "duration": 3.681,
      "language": "en"
    },
    {
      "text": "are being trained. So the one thing I'll",
      "start": 4544.4,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "talk about the first thing I'll talk",
      "start": 4545.92,
      "duration": 4.239,
      "language": "en"
    },
    {
      "text": "about is GQA and MQA. Um and these",
      "start": 4547.28,
      "duration": 4.879,
      "language": "en"
    },
    {
      "text": "aren't really critical to kind of the",
      "start": 4550.159,
      "duration": 3.761,
      "language": "en"
    },
    {
      "text": "training time behavior of the models but",
      "start": 4552.159,
      "duration": 3.921,
      "language": "en"
    },
    {
      "text": "they're very important in understanding",
      "start": 4553.92,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "the inference cost and inference",
      "start": 4556.08,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "behavior of the models. And because this",
      "start": 4557.76,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "is a important architecture change I'll",
      "start": 4559.36,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "mention them here um in addition to",
      "start": 4561.36,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "probably being mentioned by Percy um in",
      "start": 4563.36,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "some of the inference lectures. The",
      "start": 4565.12,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "other thing that's a kind of new",
      "start": 4567.28,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "development I'll mention is how the most",
      "start": 4568.4,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "recent models like Llama 4, if you've",
      "start": 4571.04,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "heard of it, supports supposedly 10",
      "start": 4573.12,
      "duration": 3.68,
      "language": "en"
    },
    {
      "text": "million tokens of context. How does it",
      "start": 4575.199,
      "duration": 3.761,
      "language": "en"
    },
    {
      "text": "do that? Well, it does so by sort of",
      "start": 4576.8,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "messing with the attention pattern in",
      "start": 4578.96,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "very structured ways. Um, and so I'll",
      "start": 4580.64,
      "duration": 6.4,
      "language": "en"
    },
    {
      "text": "talk about um that as well.",
      "start": 4582.56,
      "duration": 7.76,
      "language": "en"
    },
    {
      "text": "So, GQA, MQA. Um, if you've looked at",
      "start": 4587.04,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "like some of the larger models like the",
      "start": 4590.32,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "big llama models or or others, you'll",
      "start": 4591.92,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "have heard or seen this term GQA or MQA.",
      "start": 4593.84,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "Um, and I'll talk through what that sort",
      "start": 4597.04,
      "duration": 5.199,
      "language": "en"
    },
    {
      "text": "of means. So, to set the stage, let's",
      "start": 4599.52,
      "duration": 4.719,
      "language": "en"
    },
    {
      "text": "think about the compute that you need to",
      "start": 4602.239,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "do attention, right? So, this is once",
      "start": 4604.239,
      "duration": 4.801,
      "language": "en"
    },
    {
      "text": "again 224n slides here. um you're going",
      "start": 4606.239,
      "duration": 5.281,
      "language": "en"
    },
    {
      "text": "to take your you know XQ your your query",
      "start": 4609.04,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "and your X K and then you're going to",
      "start": 4611.52,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "form your big uh sort of quadratic",
      "start": 4612.96,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "attention matrix and you can sort of",
      "start": 4615.04,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "walk through each of these matrix",
      "start": 4617.12,
      "duration": 3.039,
      "language": "en"
    },
    {
      "text": "multiplies and you can convince yourself",
      "start": 4618.48,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "that the total number of arithmetic",
      "start": 4620.159,
      "duration": 6.401,
      "language": "en"
    },
    {
      "text": "operations is going to be B * N * D ^ 2.",
      "start": 4622.4,
      "duration": 6.319,
      "language": "en"
    },
    {
      "text": "So that's going to be um B is the batch",
      "start": 4626.56,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "dimension um N is the sequence length",
      "start": 4628.719,
      "duration": 6.561,
      "language": "en"
    },
    {
      "text": "and D ^ 2 is going to be the hidden uh",
      "start": 4631.92,
      "duration": 4.68,
      "language": "en"
    },
    {
      "text": "dimension",
      "start": 4635.28,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "squared and you can ask about the total",
      "start": 4636.6,
      "duration": 4.52,
      "language": "en"
    },
    {
      "text": "memory accesses and this is going to be",
      "start": 4639.28,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "B * N * D and this is going to be for",
      "start": 4641.12,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "example accessing just this matrix here",
      "start": 4643.44,
      "duration": 5.2,
      "language": "en"
    },
    {
      "text": "this XQ is going to be that size and",
      "start": 4645.92,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "then the softmax is going to be B * H *",
      "start": 4648.64,
      "duration": 3.519,
      "language": "en"
    },
    {
      "text": "N^ 2 and you can kind of convince",
      "start": 4650.8,
      "duration": 2.96,
      "language": "en"
    },
    {
      "text": "yourself of that by just thinking about",
      "start": 4652.159,
      "duration": 4.161,
      "language": "en"
    },
    {
      "text": "the size of the softmax matrix.",
      "start": 4653.76,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "which is going to be batch time number",
      "start": 4656.32,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "of heads times all the different softmax",
      "start": 4657.92,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "activations that you have. So that's n^",
      "start": 4660.96,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "squ of them, right? Um and you've got a",
      "start": 4662.48,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "projection and you've got d squ",
      "start": 4664.56,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "projection operations at the very end",
      "start": 4667.04,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "over here. And so we can take the ratio",
      "start": 4668.88,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "of uh total memory accesses and",
      "start": 4672.48,
      "duration": 5.199,
      "language": "en"
    },
    {
      "text": "arithmetic operations. Um and this is",
      "start": 4674.88,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "going to be something that will be very",
      "start": 4677.679,
      "duration": 3.601,
      "language": "en"
    },
    {
      "text": "important in a couple lectures. Um this",
      "start": 4679.44,
      "duration": 4.239,
      "language": "en"
    },
    {
      "text": "idea called um arithmetic intensity,",
      "start": 4681.28,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "right? So we want our arithmetic",
      "start": 4683.679,
      "duration": 3.921,
      "language": "en"
    },
    {
      "text": "intensity to be high. What that means is",
      "start": 4685.28,
      "duration": 4.959,
      "language": "en"
    },
    {
      "text": "we want to be doing a lot of compute for",
      "start": 4687.6,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "every single memory access that we do.",
      "start": 4690.239,
      "duration": 3.841,
      "language": "en"
    },
    {
      "text": "And this is going to be because memory",
      "start": 4692.56,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "accesses are very expensive on a GPU",
      "start": 4694.08,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "relatively speaking. And compute is",
      "start": 4696.32,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "relatively cheap. Um and so in this you",
      "start": 4698.159,
      "duration": 4.721,
      "language": "en"
    },
    {
      "text": "know batch computation that I'm showing",
      "start": 4701.12,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "you here you know the arithmetic",
      "start": 4702.88,
      "duration": 3.279,
      "language": "en"
    },
    {
      "text": "intensity if you take the ratio of those",
      "start": 4704.56,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "two things is going to be 1 over um k +",
      "start": 4706.159,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "1 over um bn inverse. Um and so this is",
      "start": 4708.719,
      "duration": 5.121,
      "language": "en"
    },
    {
      "text": "going to mean that we can kind of keep",
      "start": 4712.159,
      "duration": 4.121,
      "language": "en"
    },
    {
      "text": "our GPUs um",
      "start": 4713.84,
      "duration": 5.76,
      "language": "en"
    },
    {
      "text": "running um because if we have sort of",
      "start": 4716.28,
      "duration": 5.48,
      "language": "en"
    },
    {
      "text": "large number of heads and we have large",
      "start": 4719.6,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "um batch size and large sequence length,",
      "start": 4721.76,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "you know, those are all going to be sort",
      "start": 4723.84,
      "duration": 4.879,
      "language": "en"
    },
    {
      "text": "of good large numbers. Of course, this",
      "start": 4725.84,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "is what happens at training time, right?",
      "start": 4728.719,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "So the issue is that inference time we",
      "start": 4731.12,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "do not have these big chunky matrices to",
      "start": 4733.04,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "multiply together. And so that's going",
      "start": 4735.199,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "to really change the the nature of the",
      "start": 4737.28,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "behavior of of our algorithms. So when",
      "start": 4739.52,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "we're generating text, right, remember",
      "start": 4742.8,
      "duration": 3.439,
      "language": "en"
    },
    {
      "text": "that we have to generate a token and",
      "start": 4744.48,
      "duration": 3.12,
      "language": "en"
    },
    {
      "text": "then the the transformer has to read",
      "start": 4746.239,
      "duration": 3.361,
      "language": "en"
    },
    {
      "text": "that token and then it has to process",
      "start": 4747.6,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "it. And now we can get the next token",
      "start": 4749.6,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "distribution and then we do the things",
      "start": 4751.679,
      "duration": 3.361,
      "language": "en"
    },
    {
      "text": "auto reggressively one token at a time,",
      "start": 4753.12,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "right? And by doing this, we can't",
      "start": 4755.04,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "parallelize this generation process. We",
      "start": 4757.12,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "need to go step by step for every single",
      "start": 4759.04,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "new token. Um, and when we do this,",
      "start": 4760.719,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "we're going to need to incrementally",
      "start": 4763.36,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "compute attention. an idea um that",
      "start": 4765.04,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "people call the KV cache. Um and so what",
      "start": 4767.12,
      "duration": 5.68,
      "language": "en"
    },
    {
      "text": "do you do? This is a lovely animation um",
      "start": 4769.76,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "of a KV cache um that's been explained.",
      "start": 4772.8,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "So if you can um sort of look at this",
      "start": 4775.12,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "this uh figure, what you're doing is you",
      "start": 4777.12,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "know you've got a query token, right? A",
      "start": 4780.48,
      "duration": 4.719,
      "language": "en"
    },
    {
      "text": "query token here is you've generated a",
      "start": 4783.12,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "new token. You're conditioning on it and",
      "start": 4785.199,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "now you want to ask what sort of",
      "start": 4787.199,
      "duration": 3.841,
      "language": "en"
    },
    {
      "text": "information should I look up in the past",
      "start": 4788.719,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "that query token, right? and your query",
      "start": 4791.04,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "tokens are shifting from one through n",
      "start": 4792.719,
      "duration": 3.681,
      "language": "en"
    },
    {
      "text": "because you're generating new tokens one",
      "start": 4794.88,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "at a time. You're building up this sort",
      "start": 4796.4,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "of key cache over here where basically",
      "start": 4799.12,
      "duration": 5.119,
      "language": "en"
    },
    {
      "text": "I'm building up all of the past tokens",
      "start": 4801.84,
      "duration": 4.319,
      "language": "en"
    },
    {
      "text": "keys, right? And the past tokens keys",
      "start": 4804.239,
      "duration": 3.841,
      "language": "en"
    },
    {
      "text": "don't change because they only depend on",
      "start": 4806.159,
      "duration": 3.921,
      "language": "en"
    },
    {
      "text": "things in the past. And so I'm",
      "start": 4808.08,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "incrementally as I generate tokens",
      "start": 4810.08,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "building up all of these past keys. And",
      "start": 4812.159,
      "duration": 4.881,
      "language": "en"
    },
    {
      "text": "each time I can compute one new element",
      "start": 4814.8,
      "duration": 4.879,
      "language": "en"
    },
    {
      "text": "of Q.K. Right? So the big attention",
      "start": 4817.04,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "matrix is going to be this lower",
      "start": 4819.679,
      "duration": 4.401,
      "language": "en"
    },
    {
      "text": "triangular matrix. I'm computing one row",
      "start": 4821.52,
      "duration": 4.639,
      "language": "en"
    },
    {
      "text": "at a time and that row is exactly what's",
      "start": 4824.08,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "necessary to generate the next token.",
      "start": 4826.159,
      "duration": 4.641,
      "language": "en"
    },
    {
      "text": "Right? So so this KV cache idea if",
      "start": 4828.32,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "you've not seen this before is this idea",
      "start": 4830.8,
      "duration": 3.919,
      "language": "en"
    },
    {
      "text": "of saying I'm going to generate the K's",
      "start": 4832.719,
      "duration": 4.801,
      "language": "en"
    },
    {
      "text": "and the V's um incrementally as I go as",
      "start": 4834.719,
      "duration": 4.801,
      "language": "en"
    },
    {
      "text": "I generate each token and I'm only going",
      "start": 4837.52,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "to compute Q um that's absolutely",
      "start": 4839.52,
      "duration": 5.56,
      "language": "en"
    },
    {
      "text": "necessary to do my",
      "start": 4842.88,
      "duration": 4.799,
      "language": "en"
    },
    {
      "text": "operations. And so once again you can go",
      "start": 4845.08,
      "duration": 4.92,
      "language": "en"
    },
    {
      "text": "through and do um sort of the the",
      "start": 4847.679,
      "duration": 4.801,
      "language": "en"
    },
    {
      "text": "various arithmetic uh components of you",
      "start": 4850.0,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "know how many uh flops do we do what's",
      "start": 4852.48,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "the total number of memory accesses and",
      "start": 4854.96,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "if you think about the KV cache right",
      "start": 4857.28,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "I'm only multiplying the absolute",
      "start": 4859.36,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "necessary keys and values right since",
      "start": 4861.44,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "I'm saving all of the intermediate",
      "start": 4863.36,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "computations um I'm not wasting any sort",
      "start": 4865.28,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "of matrix or vector vector multiply the",
      "start": 4868.4,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "total number of arithmetic operations",
      "start": 4870.8,
      "duration": 5.359,
      "language": "en"
    },
    {
      "text": "remains exactly the same B and D",
      "start": 4872.88,
      "duration": 6.0,
      "language": "en"
    },
    {
      "text": "But the memory access patterns are now",
      "start": 4876.159,
      "duration": 5.281,
      "language": "en"
    },
    {
      "text": "different. Why is that? Because you know",
      "start": 4878.88,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "when I do this KV caching thing, I'm",
      "start": 4881.44,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "going to have to move various kinds of",
      "start": 4883.44,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "uh parameters in and out of memory",
      "start": 4885.44,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "repeatedly. Whenever I multiply with a",
      "start": 4887.28,
      "duration": 4.72,
      "language": "en"
    },
    {
      "text": "key sort of K matrix, I'm going to have",
      "start": 4889.6,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "to put that into memory, right? And then",
      "start": 4892.0,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "multiply by K. And then I need to, you",
      "start": 4893.84,
      "duration": 2.96,
      "language": "en"
    },
    {
      "text": "know, put that away and I need to",
      "start": 4895.44,
      "duration": 2.799,
      "language": "en"
    },
    {
      "text": "compute some activations. And so I'm",
      "start": 4896.8,
      "duration": 3.359,
      "language": "en"
    },
    {
      "text": "repeatedly loading in um different",
      "start": 4898.239,
      "duration": 3.361,
      "language": "en"
    },
    {
      "text": "matrices. And that's going to give me a",
      "start": 4900.159,
      "duration": 3.841,
      "language": "en"
    },
    {
      "text": "much higher total memory access of b ^2",
      "start": 4901.6,
      "duration": 5.599,
      "language": "en"
    },
    {
      "text": "d plus n d^2. And so when you take this",
      "start": 4904.0,
      "duration": 6.08,
      "language": "en"
    },
    {
      "text": "ratio now the arithmetic intensity is",
      "start": 4907.199,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "not so good. You're going to get n /d +",
      "start": 4910.08,
      "duration": 6.159,
      "language": "en"
    },
    {
      "text": "1 over b inverse. Um and so if we sort",
      "start": 4912.719,
      "duration": 5.681,
      "language": "en"
    },
    {
      "text": "of reason through this. Okay. So if I",
      "start": 4916.239,
      "duration": 4.4,
      "language": "en"
    },
    {
      "text": "want arithmetic intensity to be high, I",
      "start": 4918.4,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "want this thing inside to be very small.",
      "start": 4920.639,
      "duration": 4.961,
      "language": "en"
    },
    {
      "text": "So I need really large batches and I",
      "start": 4922.48,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "need n / d to be small. What does that",
      "start": 4925.6,
      "duration": 4.079,
      "language": "en"
    },
    {
      "text": "mean? I need really short sequence",
      "start": 4927.92,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "lengths or really big model dimensions",
      "start": 4929.679,
      "duration": 6.081,
      "language": "en"
    },
    {
      "text": "and this n /d is really unfavorable",
      "start": 4932.56,
      "duration": 5.04,
      "language": "en"
    },
    {
      "text": "because I don't want a bigger model and",
      "start": 4935.76,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "I don't want a shorter sequence length",
      "start": 4937.6,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "right and so this is the core in some",
      "start": 4939.52,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "sense inference cost trade-off that",
      "start": 4941.52,
      "duration": 4.639,
      "language": "en"
    },
    {
      "text": "people face right you have this very bad",
      "start": 4943.679,
      "duration": 4.641,
      "language": "en"
    },
    {
      "text": "memory access pattern where you have",
      "start": 4946.159,
      "duration": 4.641,
      "language": "en"
    },
    {
      "text": "this one term n /d that's kind of really",
      "start": 4948.32,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "killing you in terms of you know the",
      "start": 4950.8,
      "duration": 3.48,
      "language": "en"
    },
    {
      "text": "throughput of your",
      "start": 4952.88,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "system and so this motivates this thing",
      "start": 4954.28,
      "duration": 5.64,
      "language": "en"
    },
    {
      "text": "called mqa Okay. Um, and the key idea",
      "start": 4957.12,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "here, right, hopefully, you know, you",
      "start": 4959.92,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "kind of see from this figure back here",
      "start": 4962.0,
      "duration": 5.28,
      "language": "en"
    },
    {
      "text": "that really the part that's really bad",
      "start": 4964.8,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "is the keys and the values. They have",
      "start": 4967.28,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "this KV cache thing being built up and",
      "start": 4969.36,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "there's memory moving in and out. So,",
      "start": 4971.52,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "what you do is you can have multiple",
      "start": 4973.52,
      "duration": 4.719,
      "language": "en"
    },
    {
      "text": "heads for the query, multiple query",
      "start": 4976.0,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "heads, but only one dimension or one",
      "start": 4978.239,
      "duration": 4.641,
      "language": "en"
    },
    {
      "text": "head for the keys and values. This",
      "start": 4980.56,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "immensely simplifies things. Once you do",
      "start": 4982.88,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "this, now you're moving much less",
      "start": 4985.04,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "information for the K's and the V's. And",
      "start": 4987.04,
      "duration": 5.199,
      "language": "en"
    },
    {
      "text": "so, you know, KMV is shared. Um, but",
      "start": 4989.04,
      "duration": 5.599,
      "language": "en"
    },
    {
      "text": "query has many heads. And so, you still",
      "start": 4992.239,
      "duration": 5.041,
      "language": "en"
    },
    {
      "text": "have multi head attention um or multiple",
      "start": 4994.639,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "queries um but only single K's and V.",
      "start": 4997.28,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "So, that's why it's called multi-query",
      "start": 5000.159,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "attention. And now when you do the same",
      "start": 5001.92,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "kind of arithmetic, we have fewer memory",
      "start": 5004.239,
      "duration": 3.361,
      "language": "en"
    },
    {
      "text": "accesses because we've shared the K's",
      "start": 5006.0,
      "duration": 3.12,
      "language": "en"
    },
    {
      "text": "and the V's. And the arithmetic",
      "start": 5007.6,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "intensity is much much better behaved,",
      "start": 5009.12,
      "duration": 3.84,
      "language": "en"
    },
    {
      "text": "right? And so we can increase things",
      "start": 5011.44,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "like you know we have uh we've decreased",
      "start": 5012.96,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "the first term by a factor of n so",
      "start": 5015.44,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "longer sequence lengths are now viable",
      "start": 5017.04,
      "duration": 4.08,
      "language": "en"
    },
    {
      "text": "and the second term is now divided by",
      "start": 5019.04,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "the number of heads. So this term is",
      "start": 5021.12,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "also not so terrible right so all the",
      "start": 5022.8,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "different terms are controlled now and",
      "start": 5024.48,
      "duration": 4.6,
      "language": "en"
    },
    {
      "text": "MQA can give you much better um",
      "start": 5026.4,
      "duration": 6.08,
      "language": "en"
    },
    {
      "text": "behaviors. um GQA or group query",
      "start": 5029.08,
      "duration": 5.4,
      "language": "en"
    },
    {
      "text": "attention basically changes this",
      "start": 5032.48,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "slightly. Um instead of having you know",
      "start": 5034.48,
      "duration": 5.759,
      "language": "en"
    },
    {
      "text": "single uh query or sorry um multiple",
      "start": 5037.04,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "query and single key you can reduce the",
      "start": 5040.239,
      "duration": 4.721,
      "language": "en"
    },
    {
      "text": "number of keys by some multiple and so",
      "start": 5042.88,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "this will let you trade off between kind",
      "start": 5044.96,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "of the the inference time behaviors and",
      "start": 5047.04,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "the expressiveness of the model because",
      "start": 5049.12,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "maybe going from multi head all the way",
      "start": 5050.639,
      "duration": 5.241,
      "language": "en"
    },
    {
      "text": "to multi-query is a little bit um too",
      "start": 5052.719,
      "duration": 5.761,
      "language": "en"
    },
    {
      "text": "aggressive. Um, you know, some works",
      "start": 5055.88,
      "duration": 6.52,
      "language": "en"
    },
    {
      "text": "show that uh GQA uh doesn't hurt, but",
      "start": 5058.48,
      "duration": 5.36,
      "language": "en"
    },
    {
      "text": "multi head attention hurts. I'm not",
      "start": 5062.4,
      "duration": 3.279,
      "language": "en"
    },
    {
      "text": "going to get into that. I'm just going",
      "start": 5063.84,
      "duration": 3.76,
      "language": "en"
    },
    {
      "text": "to close off with this this very last",
      "start": 5065.679,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "thing, which I think is a really",
      "start": 5067.6,
      "duration": 3.599,
      "language": "en"
    },
    {
      "text": "interesting development in the last few",
      "start": 5069.199,
      "duration": 6.081,
      "language": "en"
    },
    {
      "text": "months. Um, so back in 2019, OpenAI had",
      "start": 5071.199,
      "duration": 6.561,
      "language": "en"
    },
    {
      "text": "this kind of cool paper um basically",
      "start": 5075.28,
      "duration": 5.12,
      "language": "en"
    },
    {
      "text": "arguing how to build longer attention uh",
      "start": 5077.76,
      "duration": 4.479,
      "language": "en"
    },
    {
      "text": "models. And they were basically arguing",
      "start": 5080.4,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "well one way to do that is to come up",
      "start": 5082.239,
      "duration": 4.801,
      "language": "en"
    },
    {
      "text": "with sort of sparse attention patterns",
      "start": 5085.04,
      "duration": 3.44,
      "language": "en"
    },
    {
      "text": "right so instead of paying attention to",
      "start": 5087.04,
      "duration": 3.119,
      "language": "en"
    },
    {
      "text": "all of the sequence I'm going to pay",
      "start": 5088.48,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "attention to let's say a local window at",
      "start": 5090.159,
      "duration": 4.321,
      "language": "en"
    },
    {
      "text": "each sort of chunk and then I can have",
      "start": 5092.08,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "sort of um other sort of attention",
      "start": 5094.48,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "patterns that are like diagonals that",
      "start": 5096.56,
      "duration": 4.0,
      "language": "en"
    },
    {
      "text": "help propagate information across. So",
      "start": 5098.48,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "you can build sparse or structured",
      "start": 5100.56,
      "duration": 3.04,
      "language": "en"
    },
    {
      "text": "attention that trades off you know",
      "start": 5102.0,
      "duration": 3.28,
      "language": "en"
    },
    {
      "text": "various kinds of expressiveness versus",
      "start": 5103.6,
      "duration": 4.639,
      "language": "en"
    },
    {
      "text": "runtime. GPT3 uses exactly these kinds",
      "start": 5105.28,
      "duration": 4.399,
      "language": "en"
    },
    {
      "text": "of tricks when they originally released",
      "start": 5108.239,
      "duration": 4.801,
      "language": "en"
    },
    {
      "text": "it um to get larger um attention",
      "start": 5109.679,
      "duration": 5.441,
      "language": "en"
    },
    {
      "text": "windows. Sliding window attention is",
      "start": 5113.04,
      "duration": 4.639,
      "language": "en"
    },
    {
      "text": "another variant of this idea where you",
      "start": 5115.12,
      "duration": 4.32,
      "language": "en"
    },
    {
      "text": "know at each layer you only pay",
      "start": 5117.679,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "attention to a small region around your",
      "start": 5119.44,
      "duration": 5.52,
      "language": "en"
    },
    {
      "text": "current position. Um and this also is",
      "start": 5122.159,
      "duration": 4.881,
      "language": "en"
    },
    {
      "text": "going to control the total amount of",
      "start": 5124.96,
      "duration": 5.44,
      "language": "en"
    },
    {
      "text": "sort of resources that you need. Um the",
      "start": 5127.04,
      "duration": 4.8,
      "language": "en"
    },
    {
      "text": "total amount of resources you need in",
      "start": 5130.4,
      "duration": 2.88,
      "language": "en"
    },
    {
      "text": "order to do longer contact. So your",
      "start": 5131.84,
      "duration": 3.359,
      "language": "en"
    },
    {
      "text": "effective receptive field is now the",
      "start": 5133.28,
      "duration": 5.64,
      "language": "en"
    },
    {
      "text": "local one times kind of the the",
      "start": 5135.199,
      "duration": 6.721,
      "language": "en"
    },
    {
      "text": "layers. The final trick. So those were",
      "start": 5138.92,
      "duration": 5.08,
      "language": "en"
    },
    {
      "text": "kind of the older ideas. But the way",
      "start": 5141.92,
      "duration": 3.64,
      "language": "en"
    },
    {
      "text": "that this has kind of been modern",
      "start": 5144.0,
      "duration": 3.679,
      "language": "en"
    },
    {
      "text": "instantiation is some of the recent",
      "start": 5145.56,
      "duration": 6.2,
      "language": "en"
    },
    {
      "text": "papers like llama 4 um and gamma and",
      "start": 5147.679,
      "duration": 6.241,
      "language": "en"
    },
    {
      "text": "cohhere command a have now come up with",
      "start": 5151.76,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "this very clever trick of basically",
      "start": 5153.92,
      "duration": 5.84,
      "language": "en"
    },
    {
      "text": "having um transformer blocks where in",
      "start": 5156.4,
      "duration": 5.839,
      "language": "en"
    },
    {
      "text": "this case you have a block a set of four",
      "start": 5159.76,
      "duration": 4.959,
      "language": "en"
    },
    {
      "text": "transformer blocks. The very bottom one",
      "start": 5162.239,
      "duration": 4.801,
      "language": "en"
    },
    {
      "text": "uses full self attention with no",
      "start": 5164.719,
      "duration": 3.92,
      "language": "en"
    },
    {
      "text": "position embedding. So there's no rope",
      "start": 5167.04,
      "duration": 2.96,
      "language": "en"
    },
    {
      "text": "no nothing. It doesn't know about",
      "start": 5168.639,
      "duration": 2.641,
      "language": "en"
    },
    {
      "text": "position at all, but it's full self",
      "start": 5170.0,
      "duration": 3.36,
      "language": "en"
    },
    {
      "text": "attention and it only happens once every",
      "start": 5171.28,
      "duration": 4.64,
      "language": "en"
    },
    {
      "text": "four blocks. And then the three blocks",
      "start": 5173.36,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "above it use sliding window attention",
      "start": 5175.92,
      "duration": 4.56,
      "language": "en"
    },
    {
      "text": "with rope, right? And so this is",
      "start": 5178.32,
      "duration": 4.16,
      "language": "en"
    },
    {
      "text": "actually a really clever trick to both",
      "start": 5180.48,
      "duration": 3.679,
      "language": "en"
    },
    {
      "text": "control the systems aspect of things",
      "start": 5182.48,
      "duration": 3.6,
      "language": "en"
    },
    {
      "text": "because the full tension only happens",
      "start": 5184.159,
      "duration": 4.401,
      "language": "en"
    },
    {
      "text": "every, you know, every now and then and",
      "start": 5186.08,
      "duration": 4.48,
      "language": "en"
    },
    {
      "text": "also the length extrapolation aspect",
      "start": 5188.56,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "because rope only deals with local",
      "start": 5190.56,
      "duration": 4.159,
      "language": "en"
    },
    {
      "text": "context windows and anything that's",
      "start": 5192.8,
      "duration": 4.24,
      "language": "en"
    },
    {
      "text": "really really long range has no position",
      "start": 5194.719,
      "duration": 4.081,
      "language": "en"
    },
    {
      "text": "embeddings at all. So it could, you",
      "start": 5197.04,
      "duration": 3.199,
      "language": "en"
    },
    {
      "text": "know, extrapolate very very",
      "start": 5198.8,
      "duration": 2.56,
      "language": "en"
    },
    {
      "text": "aggressively, right? Because you don't",
      "start": 5200.239,
      "duration": 2.801,
      "language": "en"
    },
    {
      "text": "have to do this position um",
      "start": 5201.36,
      "duration": 3.52,
      "language": "en"
    },
    {
      "text": "extrapolation that you do uh with",
      "start": 5203.04,
      "duration": 3.199,
      "language": "en"
    },
    {
      "text": "something like rope. So that's a really",
      "start": 5204.88,
      "duration": 3.12,
      "language": "en"
    },
    {
      "text": "cool development that we've seen in the",
      "start": 5206.239,
      "duration": 4.96,
      "language": "en"
    },
    {
      "text": "last um couple months. So all right, I",
      "start": 5208.0,
      "duration": 4.88,
      "language": "en"
    },
    {
      "text": "think we're coming up on time. Uh feel",
      "start": 5211.199,
      "duration": 3.04,
      "language": "en"
    },
    {
      "text": "free to to ask any questions about",
      "start": 5212.88,
      "duration": 3.2,
      "language": "en"
    },
    {
      "text": "architecture or hyperparameters. Um I'll",
      "start": 5214.239,
      "duration": 5.721,
      "language": "en"
    },
    {
      "text": "be happy to answer questions after.",
      "start": 5216.08,
      "duration": 3.88,
      "language": "en"
    }
  ]
}