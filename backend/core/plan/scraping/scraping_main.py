"""Main application module for web scraping learning resources.

This is completely separate from the knowledge graph generation workflow
and focuses specifically on discovering, crawling, and organizing learning content.
"""

import asyncio
import logging
from typing import Optional, Dict, Any, List, Tuple
from pathlib import Path
import argparse
from datetime import datetime
import time
import json
import os
import re
from bs4 import BeautifulSoup
import requests
from urllib.parse import urljoin, urlparse, unquote
from tqdm import tqdm
import yt_dlp

import sys
from pathlib import Path

# Add parent directory to Python path for imports
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from firecrawl import FirecrawlApp, ScrapeOptions, JsonConfig
from scraping.config.firecrawl_config import FirecrawlConfig, ScrapingConfig
from scraping.models.learning_resource import ParsedCommand, CommandIntent
from utils.logging_config import setup_logging
from config.settings import Settings


logger = logging.getLogger(__name__)


class ScrapingApplication:
    """Main application class for web scraping operations."""
    
    def __init__(
        self,
        firecrawl_config: Optional[FirecrawlConfig] = None,
        scraping_config: Optional[ScrapingConfig] = None,
        settings: Optional[Settings] = None
    ):
        """Initialize the scraping application."""
        self._firecrawl_config = firecrawl_config or FirecrawlConfig.from_environment()
        self._scraping_config = scraping_config or ScrapingConfig.from_environment()
        self._settings = settings or Settings.get_default()
        
        # Initialize Firecrawl client
        self._firecrawl_app = FirecrawlApp(api_key=self._firecrawl_config.api_key)
        
        # Setup logging
        setup_logging(settings=self._settings)
        
        # Validate Firecrawl configuration
        if not self._firecrawl_config.api_key:
            raise ValueError("Firecrawl API key is required")
        if not self._firecrawl_config.api_key.startswith("fc-"):
            raise ValueError("Invalid Firecrawl API key format. Must start with 'fc-'")
        
        logger.info("ScrapingApplication initialized")
    
    async def search_learning_resources(self, topic: str) -> None:
        """Search for learning resources on a specific topic."""
        logger.info(f"Searching for learning resources on topic: {topic}")
        
        try:
            # Use Firecrawl to search and scrape content
            search_url = f"https://www.google.com/search?q={topic}+tutorial"
            
            # Use actions to interact with search results
            scrape_result = self._firecrawl_app.scrape_url(
                search_url,
                formats=['markdown', 'html'],
                actions=[
                    {"type": "wait", "milliseconds": 2000},
                    {"type": "scrape"}
                ]
            )
            
            # ç›´æ¥è®¿é—®å±æ€§è€Œä¸æ˜¯ä½¿ç”¨ get æ–¹æ³•
            print(f"ğŸ” Found learning resources for: {topic}")
            
            # Save results
            output_dir = Path(self._scraping_config.download_directory)
            output_dir.mkdir(parents=True, exist_ok=True)
            
            output_file = output_dir / f"search_results_{topic.replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            # æ„å»ºè¦ä¿å­˜çš„æ•°æ®
            result_data = {
                "markdown": scrape_result.markdown,
                "html": scrape_result.html,
                "metadata": scrape_result.metadata
            }
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ“Š Results saved to: {output_file}")
            print(f"ğŸ“ Download directory: {self._scraping_config.download_directory}")
            print(f"ğŸ¯ Content types: {', '.join(self._scraping_config.content_types)}")
            
            # æ‰“å°é¢„è§ˆ
            print("\nğŸ“„ Content Preview:")
            print(f"Title: {scrape_result.metadata.get('title', 'No title')}")
            print(f"Description: {scrape_result.metadata.get('description', 'No description')}")
            print(f"\nMarkdown Preview:\n{scrape_result.markdown[:500]}...")
                
        except Exception as e:
            logger.error(f"Error searching learning resources: {e}")
            print(f"âŒ Error: {e}")
    
    def _is_course_content_link(self, url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºè¯¾ç¨‹å†…å®¹é“¾æ¥"""
        content_patterns = [
            r'\.pdf$',
            r'\.ppt$',
            r'\.pptx$',
            r'youtube\.com/watch',
            r'youtu\.be/',
            r'drive\.google\.com',
            r'docs\.google\.com'
        ]
        return any(re.search(pattern, url, re.IGNORECASE) for pattern in content_patterns)

    def _extract_course_content(self, html: str, base_url: str) -> List[Dict[str, str]]:
        """æå–è¯¾ç¨‹å†…å®¹é“¾æ¥"""
        soup = BeautifulSoup(html, 'html.parser')
        content_links = []
        
        # æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['td', 'th'])
                if not cells:
                    continue
                    
                content = {
                    'topic': '',
                    'pdf': '',
                    'ppt': '',
                    'video': '',
                    'homework': ''
                }
                
                # æå–æ¯ä¸ªå•å…ƒæ ¼çš„å†…å®¹
                for i, cell in enumerate(cells):
                    links = cell.find_all('a')
                    for link in links:
                        href = link.get('href', '')
                        if not href:
                            continue
                            
                        # å¤„ç†ç›¸å¯¹URL
                        if not href.startswith(('http://', 'https://')):
                            href = urljoin(base_url, href)
                            
                        text = link.get_text().strip().lower()
                        
                        # æ ¹æ®é“¾æ¥æ–‡æœ¬å’ŒURLç‰¹å¾åˆ†ç±»å†…å®¹
                        if i == 0:  # ç¬¬ä¸€åˆ—é€šå¸¸æ˜¯ä¸»é¢˜
                            content['topic'] = link.get_text().strip()
                        elif 'pdf' in text or href.lower().endswith('.pdf'):
                            content['pdf'] = href
                        elif 'ppt' in text or any(href.lower().endswith(ext) for ext in ['.ppt', '.pptx']):
                            content['ppt'] = href
                        elif 'view' in text or 'video' in text or 'youtube' in href or 'youtu.be' in href:
                            content['video'] = href
                        elif 'hw' in text or 'homework' in text:
                            content['homework'] = href
                
                if any(content.values()):  # å¦‚æœæ‰¾åˆ°ä»»ä½•å†…å®¹
                    content_links.append(content)
        
        return content_links

    async def crawl_url(self, url: str) -> None:
        """Crawl a specific URL for learning content."""
        logger.info(f"Crawling URL for learning content: {url}")
        print(f"ğŸ•·ï¸ å¼€å§‹çˆ¬å–URL: {url}")
        
        try:
            # åˆ›å»ºä¸‹è½½ç›®å½•
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            download_dir = os.path.join(self._scraping_config.download_directory, f'crawl_results_{timestamp}')
            os.makedirs(download_dir, exist_ok=True)
            
            # å‘é€è¯·æ±‚è·å–é¡µé¢å†…å®¹
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            # è§£æé¡µé¢å†…å®¹
            content_links = self._extract_course_content(response.text, url)
            
            print(f"\nâœ… é¡µé¢è§£æå®Œæˆ!")
            print(f"ğŸ” å‘ç° {len(content_links)} ä¸ªè¯¾ç¨‹å†…å®¹é¡¹")
            
            # ä¿å­˜è¯¾ç¨‹å†…å®¹ç»“æ„
            structure_file = os.path.join(download_dir, 'course_structure.json')
            with open(structure_file, 'w', encoding='utf-8') as f:
                json.dump(content_links, f, indent=2, ensure_ascii=False)
            
            print("\nğŸ“¥ å¼€å§‹ä¸‹è½½è¯¾ç¨‹èµ„æº...")
            
            video_count = 0
            pdf_count = 0
            ppt_count = 0
            
            # ä¸‹è½½æ¯ä¸ªå†…å®¹é¡¹çš„èµ„æº
            for content in content_links:
                print(f"\nğŸ” å¤„ç†ä¸»é¢˜: {content['topic']}")
                
                # ä¸‹è½½PDF
                if content['pdf']:
                    print(f"ğŸ“„ ä¸‹è½½PDF: {content['pdf']}")
                    try:
                        self._download_file(content['pdf'], download_dir)
                        pdf_count += 1
                    except Exception as e:
                        print(f"âŒ PDFä¸‹è½½å¤±è´¥: {str(e)}")
                
                # ä¸‹è½½PPT
                if content['ppt']:
                    print(f"ğŸ“Š ä¸‹è½½PPT: {content['ppt']}")
                    try:
                        self._download_file(content['ppt'], download_dir)
                        ppt_count += 1
                    except Exception as e:
                        print(f"âŒ PPTä¸‹è½½å¤±è´¥: {str(e)}")
                
                # ä¸‹è½½è§†é¢‘
                if content['video']:
                    print(f"ğŸ“º ä¸‹è½½è§†é¢‘: {content['video']}")
                    try:
                        await self._download_video_with_transcript(content['video'], download_dir)
                        video_count += 1
                    except Exception as e:
                        print(f"âŒ è§†é¢‘ä¸‹è½½å¤±è´¥: {str(e)}")
            
            print(f"\nğŸ“Š ä¸‹è½½ç»Ÿè®¡:")
            print(f"ğŸ“º è§†é¢‘: {video_count} ä¸ª")
            print(f"ğŸ“„ PDF: {pdf_count} ä¸ª")
            print(f"ğŸ“Š PPT: {ppt_count} ä¸ª")
            
            print(f"\nğŸ“Š çˆ¬å–æ€»ç»“:")
            print(f"ç›®æ ‡URL: {url}")
            print(f"è¯¾ç¨‹ç»“æ„ä¿å­˜åœ¨: {structure_file}")
            print(f"ä¸‹è½½ç›®å½•: {download_dir}")
            
        except Exception as e:
            logger.error(f"Error crawling URL: {e}")
            print(f"âŒ é”™è¯¯: {e}")

    async def _download_video_with_transcript(self, url: str, output_dir: str) -> None:
        """ä¸‹è½½è§†é¢‘å’Œå­—å¹•"""
        try:
            ydl_opts = {
                'format': 'best',  # æœ€ä½³è´¨é‡
                'outtmpl': os.path.join(output_dir, '%(title)s.%(ext)s'),
                'writesubtitles': True,  # ä¸‹è½½å­—å¹•
                'writeautomaticsub': True,  # ä¸‹è½½è‡ªåŠ¨ç”Ÿæˆçš„å­—å¹•
                'subtitleslangs': ['en', 'zh-Hans', 'zh-Hant', 'zh-TW'],  # ä¸‹è½½è‹±æ–‡å’Œä¸­æ–‡å­—å¹•
                'writedescription': True,  # ä¸‹è½½è§†é¢‘æè¿°
                'writethumbnail': True,  # ä¸‹è½½ç¼©ç•¥å›¾
                'quiet': True,
                'no_warnings': True,
                'progress_hooks': [lambda d: print(
                    f"â³ ä¸‹è½½ä¸­: {d['filename']}" if d['status'] == 'downloading' 
                    else f"âœ… å®Œæˆ: {d['filename']}" if d['status'] == 'finished' 
                    else '')],
            }
            
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                info = ydl.extract_info(url, download=True)
                video_title = info.get('title', 'video')
                
                # ä¿å­˜è§†é¢‘ä¿¡æ¯
                info_file = os.path.join(output_dir, f'{video_title}_info.json')
                with open(info_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        'title': info.get('title'),
                        'description': info.get('description'),
                        'duration': info.get('duration'),
                        'view_count': info.get('view_count'),
                        'like_count': info.get('like_count'),
                        'upload_date': info.get('upload_date'),
                        'uploader': info.get('uploader'),
                        'channel_url': info.get('channel_url'),
                        'video_url': url,  # æ–°å¢ï¼šåŸå§‹è§†é¢‘é“¾æ¥
                    }, f, indent=2, ensure_ascii=False)
                
                print(f"âœ… è§†é¢‘ä¿¡æ¯å·²ä¿å­˜: {info_file}")
                
        except Exception as e:
            print(f"âŒ è§†é¢‘ä¸‹è½½å¤±è´¥ {url}: {str(e)}")

    def _is_video_link(self, url: str) -> bool:
        """åˆ¤æ–­URLæ˜¯å¦æ˜¯è§†é¢‘é“¾æ¥ã€‚"""
        # è§†é¢‘æ–‡ä»¶æ‰©å±•å
        video_extensions = ['.mp4', '.mov', '.avi', '.wmv', '.flv', '.webm', '.mkv']
        # è§†é¢‘å¹³å°åŸŸåå’Œè·¯å¾„ç‰¹å¾
        video_patterns = [
            'youtube.com/watch',
            'youtu.be/',  # çŸ­é“¾æ¥
            'vimeo.com/',
            'bilibili.com/video',
            'b23.tv/'
        ]
        
        url_lower = url.lower()
        return (
            any(url_lower.endswith(ext) for ext in video_extensions) or
            any(pattern in url_lower for pattern in video_patterns) or
            'youtube' in url_lower  # æ›´å®½æ¾çš„ YouTube é“¾æ¥åŒ¹é…
        )

    def _is_downloadable_file(self, url: str) -> bool:
        """åˆ¤æ–­URLæ˜¯å¦æ˜¯å¯ä¸‹è½½çš„æ–‡ä»¶ã€‚"""
        # æ‰©å±•å¯ä¸‹è½½æ–‡ä»¶ç±»å‹ï¼ŒåŒ…å« PPT
        downloadable_extensions = ['.pdf', '.ppt', '.pptx', '.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar', '.txt']
        return any(url.lower().endswith(ext) for ext in downloadable_extensions)

    def _extract_links_from_markdown(self, markdown: str, html: str, base_url: str) -> List[Tuple[str, str]]:
        """ä»Markdownå’ŒHTMLå†…å®¹ä¸­æå–é“¾æ¥ã€‚"""
        from bs4 import BeautifulSoup
        links = []
        
        # 1. ä» HTML æå–é“¾æ¥ï¼ˆä¼˜å…ˆï¼Œå› ä¸ºæ›´å‡†ç¡®ï¼‰
        soup = BeautifulSoup(html, 'html.parser')
        for a in soup.find_all('a'):
            href = a.get('href')
            if href:
                text = a.get_text().strip()
                # å¤„ç†ç›¸å¯¹è·¯å¾„
                if not href.startswith(('http://', 'https://')):
                    if href.startswith('//'):
                        href = 'https:' + href
                    elif href.startswith('/'):
                        parsed_base = urlparse(base_url)
                        href = f"{parsed_base.scheme}://{parsed_base.netloc}{href}"
                    else:
                        href = urljoin(base_url, href)
                links.append((text, href))
        
        # 2. ä» Markdown æå–é“¾æ¥ï¼ˆè¡¥å……ï¼‰
        markdown_pattern = re.compile(r'\[([^\]]*)\]\(([^)]+)\)')
        for match in markdown_pattern.finditer(markdown):
            text = match.group(1).strip()
            url = match.group(2).strip()
            if not url.startswith(('http://', 'https://')):
                url = urljoin(base_url, url)
            links.append((text, url))
        
        # 3. æå–ç›´æ¥çš„ URLï¼ˆè¡¥å……ï¼‰
        url_pattern = re.compile(r'(?<![\(\[])(https?://[^\s\)\]]+)')
        for match in url_pattern.finditer(markdown):
            url = match.group(0)
            links.append(('', url))
        
        # 4. å»é‡ï¼Œä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„é“¾æ¥
        seen = set()
        unique_links = []
        for text, url in links:
            if url not in seen:
                seen.add(url)
                unique_links.append((text, url))
        
        return unique_links

    def _download_file(self, url: str, output_dir: str) -> None:
        """ä¸‹è½½æ–‡ä»¶ã€‚"""
        try:
            
            # æ›´å¥½çš„æ–‡ä»¶åæå–
            parsed_url = urlparse(url)
            file_name = os.path.basename(unquote(parsed_url.path))
            
            # å¦‚æœæ²¡æœ‰æ–‡ä»¶åï¼Œå°è¯•ä»URLä¸­æå–
            if not file_name or '.' not in file_name:
                file_name = url.split('/')[-1]
                if not file_name or '.' not in file_name:
                    file_name = f"downloaded_file_{int(time.time())}"
            
            file_path = os.path.join(output_dir, file_name)
            
            # å‘é€è¯·æ±‚
            response = requests.get(url, stream=True, timeout=30, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            response.raise_for_status()
            
            # è·å–æ–‡ä»¶å¤§å°
            total_size = int(response.headers.get('content-length', 0))
            
            # ä¸‹è½½æ–‡ä»¶å¹¶æ˜¾ç¤ºè¿›åº¦
            with open(file_path, 'wb') as f, tqdm(
                desc=file_name,
                total=total_size,
                unit='iB',
                unit_scale=True,
                unit_divisor=1024,
            ) as pbar:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        size = f.write(chunk)
                        pbar.update(size)
            
            print(f"âœ… æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {file_path}")
            
        except Exception as e:
            print(f"âŒ æ–‡ä»¶ä¸‹è½½å¤±è´¥ {url}: {str(e)}")
            # å°è¯•é‡è¯•ä¸€æ¬¡
            try:
                print(f"ğŸ”„ é‡è¯•ä¸‹è½½: {url}")
                response = requests.get(url, timeout=60)
                response.raise_for_status()
                
                file_name = url.split('/')[-1] if '/' in url else 'downloaded_file'
                file_path = os.path.join(output_dir, file_name)
                
                with open(file_path, 'wb') as f:
                    f.write(response.content)
                print(f"âœ… é‡è¯•æˆåŠŸ: {file_path}")
            except Exception as retry_error:
                print(f"âŒ é‡è¯•ä¹Ÿå¤±è´¥: {str(retry_error)}")
    
    async def process_natural_language_command(self, command: str) -> None:
        """Process a natural language command for scraping operations."""
        logger.info(f"Processing command: {command}")
        
        try:
            if "search" in command.lower() or "find" in command.lower():
                topic = command.lower().replace("search", "").replace("find", "").strip()
                if topic:
                    await self.search_learning_resources(topic)
                else:
                    print("â“ Please specify a topic to search for")
            elif "crawl" in command.lower() or "http" in command.lower():
                words = command.split()
                url = next((word for word in words if word.startswith("http")), None)
                if url:
                    await self.crawl_url(url)
                else:
                    print("â“ Please provide a valid URL to crawl")
            else:
                print("â“ Command not recognized. Try 'search [topic]' or 'crawl [url]'")
                
        except Exception as e:
            logger.error(f"Error processing command: {e}")
            print(f"âŒ Error: {e}")
    
    @property
    def firecrawl_config(self) -> FirecrawlConfig:
        """Get the Firecrawl configuration."""
        return self._firecrawl_config
    
    @property
    def scraping_config(self) -> ScrapingConfig:
        """Get the scraping configuration."""
        return self._scraping_config


def create_scraping_application(
    firecrawl_config: Optional[FirecrawlConfig] = None,
    scraping_config: Optional[ScrapingConfig] = None,
    settings: Optional[Settings] = None
) -> ScrapingApplication:
    """Factory function to create a configured scraping application."""
    return ScrapingApplication(
        firecrawl_config=firecrawl_config,
        scraping_config=scraping_config,
        settings=settings
    )


async def main() -> None:
    """Main entry point for the scraping application."""
    try:
        parser = argparse.ArgumentParser(description="Learning Resource Scraper")
        parser.add_argument('--command', type=str, help='Natural language command for scraping')
        parser.add_argument('--topic', type=str, help='Topic to search for learning resources')
        parser.add_argument('--url', type=str, help='URL to crawl for content')
        args = parser.parse_args()
        
        # Create the scraping application
        app = create_scraping_application()
        
        print("=== Learning Resource Scraper ===")
        print(f"ğŸ“ Download directory: {app.scraping_config.download_directory}")
        print(f"ğŸ¯ Content types: {', '.join(app.scraping_config.content_types)}")
        print(f"ğŸ”§ Firecrawl server: {app.firecrawl_config.mcp_server_url}")
        print()
        
        if args.command:
            await app.process_natural_language_command(args.command)
        elif args.topic:
            await app.search_learning_resources(args.topic)
        elif args.url:
            await app.crawl_url(args.url)
        else:
            # Interactive mode
            print("ğŸ¤– Interactive mode - Enter commands:")
            print("   â€¢ search [topic] - Search for learning resources")
            print("   â€¢ crawl [url] - Crawl a specific URL")
            print("   â€¢ quit - Exit the application")
            print()
            
            while True:
                try:
                    command = input("scraper> ").strip()
                    if command.lower() in ['quit', 'exit', 'q']:
                        break
                    if command:
                        await app.process_natural_language_command(command)
                    print()
                except KeyboardInterrupt:
                    print("\nğŸ‘‹ Goodbye!")
                    break
        
        print("âœ… Scraping application completed successfully!")
        
    except Exception as e:
        logger.error(f"Scraping application failed: {e}")
        print(f"âŒ Error: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    # Run the scraping application
    exit_code = asyncio.run(main())
    exit(exit_code)